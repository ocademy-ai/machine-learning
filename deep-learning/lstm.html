
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>25. Long-short term memory &#8212; Ocademy Open Machine Learning Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/drawio.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/youtube.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "ocademy-ai/machine-learning-utterances");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"></script>
    <script>quizdown.init({"quizdown_js": "https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"});</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="26. Time series" href="time-series.html" />
    <link rel="prev" title="24. Autoencoder" href="autoencoder.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint">Learn AI together, for free! At <a color='lightblue' href='https://ocademy.cc'><u style='color:lightblue;'>Ocademy</u></a>.</div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo-long.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Ocademy Open Machine Learning Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PREREQUISITES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-introduction.html">
   1. Python programming introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-basics.html">
   2. Python programming basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-advanced.html">
   3. Python programming advanced
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DATA SCIENCE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/introduction/introduction.html">
   4. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/defining-data-science.html">
     4.1. Defining data science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/data-science-ethics.html">
     4.2. Data Science ethics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/defining-data.html">
     4.3. Defining data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/introduction-to-statistics-and-probability.html">
     4.4. Introduction to statistics and probability
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/working-with-data/working-with-data.html">
   5. Working with data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/relational-databases.html">
     5.1. Relational databases
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/non-relational-data.html">
     5.2. Non-relational data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/numpy.html">
     5.3. NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/pandas-md.html">
     5.4. Intro to pandas
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../data-science/working-with-data/pandas.html">
     5.5. Pandas
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/introduction-and-data-structures.html">
       5.5.1. Introduction and Data Structures
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/data-selection.html">
       5.5.2. Data Selection
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/advanced-pandas-techniques.html">
       5.5.3. Advanced Pandas Techniques
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/data-preparation.html">
     5.6. Data preparation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-visualization/data-visualization.html">
   6. Data visualization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-distributions.html">
     6.1. Visualizing distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-proportions.html">
     6.2. Visualizing proportions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-relationships.html">
     6.3. Visualizing relationships: all about honey üçØ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/meaningful-visualizations.html">
     6.4. Making meaningful visualizations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-science-lifecycle/data-science-lifecycle.html">
   7. Data Science lifecycle
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/introduction.html">
     7.1. Introduction to the Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/analyzing.html">
     7.2. Analyzing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/communication.html">
     7.3. Communication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-science-in-the-cloud/data-science-in-the-cloud.html">
   8. Data Science in the cloud
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/introduction.html">
     8.1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/the-low-code-no-code-way.html">
     8.2. The ‚Äúlow code/no code‚Äù way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/the-azure-ml-sdk-way.html">
     8.3. Data Science in the cloud: The ‚ÄúAzure ML SDK‚Äù way
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data-science/data-science-in-the-wild.html">
   9. Data Science in the real world
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING BASICS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-fundamentals/ml-overview.html">
   10. Machine Learning overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fundamentals/regression/regression-models-for-machine-learning.html">
   11. Regression models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/tools-of-the-trade.html">
     11.1. Tools of the trade
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/managing-data.html">
     11.2. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/linear-and-polynomial-regression.html">
     11.3. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/logistic-regression.html">
     11.4. Logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-fundamentals/build-a-web-app-to-use-a-machine-learning-model.html">
   12. Build a web app to use a Machine Learning model
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fundamentals/classification/getting-started-with-classification.html">
   13. Getting started with classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/introduction-to-classification.html">
     13.1. Introduction to classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/more-classifiers.html">
     13.2. More classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/yet-other-classifiers.html">
     13.3. Yet other classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/applied-ml-build-a-web-app.html">
     13.4. Applied Machine Learning : build a web app
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ADVANCED MACHINE LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-advanced/clustering/clustering-models-for-machine-learning.html">
   14. Clustering models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/clustering/introduction-to-clustering.html">
     14.1. Introduction to clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/clustering/k-means-clustering.html">
     14.2. K-Means clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-advanced/ensemble-learning/getting-started-with-ensemble-learning.html">
   15. Getting started with ensemble learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/ensemble-learning/bagging.html">
     15.1. Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/ensemble-learning/random-forest.html">
     15.2. Random forest
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/ensemble-learning/feature-importance.html">
     15.3. Feature importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-advanced/gradient-boosting/introduction-to-gradient-boosting.html">
   16. Introduction to Gradient Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/gradient-boosting/gradient-boosting.html">
     16.1. Gradient Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/gradient-boosting/gradient-boosting-example.html">
     16.2. Gradient boosting example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/gradient-boosting/xgboost.html">
     16.3. XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-advanced/gradient-boosting/xgboost-k-fold-cv-feature-importance.html">
     16.4. XGBoost + k-fold CV + Feature Importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-advanced/unsupervised-learning.html">
   17. Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-advanced/kernel-method.html">
   18. Kernel method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-advanced/model-selection.html">
   19. Model selection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dl-overview.html">
   20. Intro to Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnn.html">
   21. Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gan.html">
   22. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnn.html">
   23. Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="autoencoder.html">
   24. Autoencoder
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   25. Long-short term memory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="time-series.html">
   26. Time series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dqn.html">
   27. Deep Q-learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="image-classification.html">
   28. Image classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="image-segmentation.html">
   29. Image segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="difussion-model.html">
   30. Diffusion Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="object-detection.html">
   31. Object detection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING OPERATIONS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/overview.html">
   32. Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/problem-framing.html">
   33. Problem framing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/data-engineering.html">
   34. Data engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/model-training-and-evaluation.html">
   35. Model training &amp; evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/model-deployment.html">
   36. Model deployment
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  OTHERS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../assignments/README.html">
   37. Self-paced assignments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/set-up-env/first-assignment.html">
     37.3. First assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/set-up-env/second-assignment.html">
     37.4. Second assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/project-plan-template.html">
     37.5. Project Plan‚Äã Template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-introduction.html">
     37.6. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-basics.html">
     37.7. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-advanced.html">
     37.8. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-text-about-data-science.html">
     37.9. Analyzing text about Data Science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-scenarios.html">
     37.10. Data Science scenarios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/write-a-data-ethics-case-study.html">
     37.11. Write a data ethics case study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/lines-scatters-and-bars.html">
     37.12. Lines, scatters and bars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/apply-your-skills.html">
     37.13. Apply your skills
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/try-it-in-excel.html">
     37.14. Try it in Excel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/dive-into-the-beehive.html">
     37.15. Dive into the beehive
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/build-your-own-custom-vis.html">
     37.16. Build your own custom vis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/classifying-datasets.html">
     37.17. Classifying datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/small-diabetes-study.html">
     37.18. Small diabetes study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/introduction-to-statistics-and-probability.html">
     37.19. Introduction to probability and statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/displaying-airport-data.html">
     37.20. Displaying airport data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/soda-profits.html">
     37.21. Soda profits
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-COVID-19-papers.html">
     37.22. Analyzing COVID-19 papers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/estimation-of-COVID-19-pandemic.html">
     37.23. Estimation of COVID-19 pandemic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-processing-in-python.html">
     37.24. Data processing in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/evaluating-data-from-a-form.html">
     37.25. Evaluating data from a form
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-preparation.html">
     37.26. Data preparation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-data.html">
     37.27. Analyzing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/nyc-taxi-data-in-winter-and-summer.html">
     37.28. NYC taxi data in winter and summer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/matplotlib-applied.html">
     37.29. Matplotlib applied
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/tell-a-story.html">
     37.35. Tell a story
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/explore-a-planetary-computer-dataset.html">
     37.36. Explore a planetary computer dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/exploring-for-anwser.html">
     37.37. Exploring for answers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/market-research.html">
     37.38. Market research
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/low-code-no-code-data-science-project-on-azure-ml.html">
     37.39. Low code/no code Data Science project on Azure ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-project-using-azure-ml-sdk.html">
     37.40. Data Science project using Azure ML SDK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-in-the-cloud-the-azure-ml-sdk-way.html">
     37.41. Data Science in the cloud: The ‚ÄúAzure ML SDK‚Äù way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-overview-iris.html">
     37.42. Machine Learning overview - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-overview-mnist-digits.html">
     37.43. Machine Learning overview - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/regression-with-scikit-learn.html">
     37.44. Regression with Scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/california_housing.html">
     37.45. Linear regression - California Housing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/linear-regression-metrics.html">
     37.46. Linear Regression Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/loss-function.html">
     37.47. Loss Function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/gradient-descent.html">
     37.48. Gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/linear-regression-from-scratch.html">
     37.49. Linear Regression Implementation from Scratch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-logistic-regression-1.html">
     37.50. ML logistic regression - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-logistic-regression-2.html">
     37.51. ML logistic regression - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-neural-network-1.html">
     37.52. ML neural network - Assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/regression-tools.html">
     37.53. Regression tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/managing-data.html">
     37.54. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/exploring-visualizations.html">
     37.55. Exploring visualizations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/try-a-different-model.html">
     37.56. Try a different model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/create-a-regression-model.html">
     37.57. Create a regression model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-and-polynomial-regression.html">
     37.58. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/retrying-some-regression.html">
     37.59. Retrying some regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/pumpkin-varieties-and-color.html">
     37.60. Pumpkin varieties and color
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/delicious-asian-and-indian-cuisines.html">
     37.61. Delicious asian and indian cuisines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/explore-classification-methods.html">
     37.62. Explore classification methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/kernel-method-assignment-1.html">
     37.63. Kernel method assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/support_vector_machines_for_regression.html">
     37.64. Support Vector Machines (SVM) - Intro and SVM for Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/support_vector_machines_for_classification.html">
     37.65. Support Vector Machines (SVM) - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/decision_trees_for_regression.html">
     37.66. Decision Trees - Intro and Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/decision_trees_for_classification.html">
     37.67. Decision Trees - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/model-selection-assignment-1.html">
     37.68. Model selection assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/learning-curve-to-identify-overfit-underfit.html">
     37.69. Learning Curve To Identify Overfit &amp; Underfit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/dropout-and-batch-normalization.html">
     37.70. Dropout and Batch Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/lasso-and-ridge-regression.html">
     37.71. Lasso and Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/regularized-linear-models.html">
     37.72. Regularized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forests-intro-and-regression.html">
     37.73. Random forests intro and regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forests-for-classification.html">
     37.74. Random forests for classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/beyond-random-forests-more-ensemble-models.html">
     37.75. Beyond random forests: more ensemble models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/decision-trees.html">
     37.76. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/hyperparameter-tuning-gradient-boosting.html">
     37.77. Hyperparameter tuning gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/gradient-boosting-assignment.html">
     37.78. Gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/boosting-with-tuning.html">
     37.79. Boosting with tuning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forest-classifier-feature-importance.html">
     37.80. Random Forest Classifier with Feature Importance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/data-engineering.html">
     37.82. Data engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/counterintuitive-challenges-in-ml-debugging.html">
     37.83. Counterintuitive Challenges in ML Debugging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/debugging-in-classification.html">
     37.84. Case Study: Debugging in Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/debugging-in-regression.html">
     37.85. Case Study: Debugging in Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/study-the-solvers.html">
     37.86. Study the solvers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-classification-models.html">
     37.87. Build classification models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-classification-model.html">
     37.88. Build Classification Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/parameter-play.html">
     37.89. Parameter play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/how-to-choose-cnn-architecture-mnist.html">
     37.90. How to choose cnn architecture mnist
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/sign-language-digits-classification-with-cnn.html">
     37.92. Sign Language Digits Classification with CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/object-recognition-in-images-using-cnn.html">
     37.94. Object Recognition in Images using CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/tensorflow/intro_to_tensorflow_for_deeplearning.html">
     37.95. Intro to TensorFlow for Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/lstm/bitcoin-lstm-model-with-tweet-volume-and-sentiment.html">
     37.97. Bitcoin LSTM Model with Tweet Volume and Sentiment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/rnn/google-stock-price-prediction-rnn.html">
     37.99. Google Stock Price Prediction RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/autoencoder.html">
     37.101. Intro to Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/base-denoising-autoencoder-dimension-reduction.html">
     37.102. Base/Denoising Autoencoder &amp; Dimension Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/variational-autoencoder-and-faces-generation.html">
     37.103. Fun with Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/time-series-forecasting-assignment.html">
     37.104. Time Series Forecasting Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nn-for-classification-assignment.html">
     37.106. Neural Networks for Classification with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nn-classify-15-fruits-assignment.html">
     37.107. NN Classify 15 Fruits Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/dqn/dqn-on-foreign-exchange-market.html">
     37.112. DQN On Foreign Exchange Market
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/gan/art-by-gan.html">
     37.113. Art by gan
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/gan/gan-introduction.html">
     37.115. Generative Adversarial Networks (GANs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/overview/basic-classification-classify-images-of-clothing.html">
     37.116. Basic classification: Classify images of clothing
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../slides/introduction.html">
   38. Slides
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-introduction.html">
     38.1. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-basics.html">
     38.2. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-advanced.html">
     38.3. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-introduction.html">
     38.4. Data Science introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/relational-vs-non-relational-database.html">
     38.5. Relational vs. non-relational database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/numpy-and-pandas.html">
     38.6. NumPy and Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-visualization.html">
     38.7. Data visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-lifecycle.html">
     38.8. Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-in-the-cloud.html">
     38.9. Data Science in the cloud
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-in-real-world.html">
     38.10. Data Science in real world
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/ml-overview.html">
     38.11. Machine Learning overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/linear-regression.html">
     38.12. Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/logistic-regression.html">
     38.13. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/logistic-regression-condensed.html">
     38.14. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/neural-network.html">
     38.15. Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/build-an-ml-web-app.html">
     38.16. Build an machine learning web application
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/unsupervised-learning.html">
     38.17. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/kernel-method.html">
     38.18. Kernel method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/model-selection.html">
     38.19. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/deep-learning/cnn.html">
     38.20. Convolutional Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/deep-learning/gan.html">
     38.21. Generative Adversarial Network
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ocademy-ai/machine-learning/release?urlpath=lab/tree/open-machine-learning-jupyter-book/deep-learning/lstm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/ocademy-ai/machine-learning/blob/release/open-machine-learning-jupyter-book/deep-learning/lstm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning//issues/new?title=Issue%20on%20page%20%2Fdeep-learning/lstm.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/edit/release/open-machine-learning-jupyter-book/deep-learning/lstm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/deep-learning/lstm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   25.1. Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-gates">
   25.2. Special Gates
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code">
     25.2.1. Code
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   25.3. Your turn! üöÄ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   25.4. Acknowledgments
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Long-short term memory</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   25.1. Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#special-gates">
   25.2. Special Gates
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code">
     25.2.1. Code
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   25.3. Your turn! üöÄ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   25.4. Acknowledgments
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the necessary dependencies</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span> 
<span class="o">!{</span>sys.executable<span class="o">}</span><span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>pandas<span class="w"> </span>scikit-learn<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>jupyterlab_myst<span class="w"> </span>ipython
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="long-short-term-memory">
<h1><span class="section-number">25. </span>Long-short term memory<a class="headerlink" href="#long-short-term-memory" title="Permalink to this headline">#</a></h1>
<p>As we have learned the normal RNNs on the previous lessons, today we are going to use long-short term memory (LSTM) to fix the problems that are induced from the RNN architecture.</p>
<p>The vanilla LSTM is proposed in 2005, the paper is <a class="reference external" href="https://link.springer.com/chapter/10.1007/11550907_126">Bidirectional LSTM Networks for Improved Phoneme Classification and Recognition</a>, and after that, a lot of paper based on it appeared.</p>
<section id="overview">
<h2><span class="section-number">25.1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<p>The essential problems of RNN are vanishing/exploding gradient problems. The gradient vanishing/exploding problem is a common issue in training deep neural networks. This problem occurs when the gradient of the loss function with respect to the weights of the network becomes very small or very large.</p>
<p>There are several possibilities will bring on this kind of problem, for example:</p>
<ul class="simple">
<li><p>a poor choice of hyper-parameters</p></li>
<li><p>a poor architecture,</p></li>
<li><p>a bug in the code.</p></li>
</ul>
<p>Luckily, LSTM can use a memory cell for modeling long-range dependencies and avoid vanishing/exploding gradient problems.</p>
<p>First, let‚Äôs see the architecture of LSTM cell.</p>
<img src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/LSTM/LSTM_cell.png" width="90%" class="bg-white mb-1">
<p>It looks like the cell of RNN, but there are still some differences between them. The same part is the direction of data, one input and two outputs.
However, the LSTM cell adds a cell state <span class="math notranslate nohighlight">\(c^{&lt;t&gt;}\)</span>, which updates with the time. <span class="math notranslate nohighlight">\(c^{&lt;t-1&gt;}\)</span> means cell state at previous time step and <span class="math notranslate nohighlight">\(c^{&lt;t-1&gt;}\)</span> means cell state at current time step.
The <span class="math notranslate nohighlight">\(h^{t}\)</span> is still the activation.</p>
<img src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/LSTM/cell_state.png" width="90%" class="bg-white mb-1"></section>
<section id="special-gates">
<h2><span class="section-number">25.2. </span>Special Gates<a class="headerlink" href="#special-gates" title="Permalink to this headline">#</a></h2>
<p>Then, we take a look at the inside of cell. <span class="math notranslate nohighlight">\(\bigodot\)</span> means element-wise multiplication operator, <span class="math notranslate nohighlight">\(\bigoplus\)</span> means element-wise addition operator and <span class="math notranslate nohighlight">\(\sigma\)</span> means logistic sigmoid activation functions.</p>
<p>Now, we should pay attention to ‚ÄòGates‚Äô.</p>
<p>The first is ‚ÄòForget Gate‚Äô <span class="math notranslate nohighlight">\(f\)</span>, and it controls which information is remembered, and which is forgotten; can reset the cell state. This function can be written as <span class="math notranslate nohighlight">\(f_t = \sigma (W_{fx}x^{&lt;t&gt;} + W_{fh}h^{&lt;t-1&gt;} + b_f)\)</span></p>
<p>Next, ‚ÄòInput Gate‚Äô is <span class="math notranslate nohighlight">\(i_t = \sigma(W_{ix}x^{&lt;t&gt;} + W_{ih}h^{&lt;t-1&gt;} + b_i)\)</span> and ‚ÄòInput Node‚Äô is <span class="math notranslate nohighlight">\(g_t = tanh(W_{gt}x^{&lt;t&gt;} + W_{gh}x^{&lt;t-1&gt;} + b_g)\)</span>.</p>
<p>To brief summarize the previous gates in an expression: <span class="math notranslate nohighlight">\(C^{&lt;t&gt;} = (C^{&lt;t-1&gt;} \bigodot f_t) \bigoplus (i_t \bigodot g_t)\)</span>. Since <span class="math notranslate nohighlight">\(i_t\)</span> is ‚ÄòInput Gate‚Äô and <span class="math notranslate nohighlight">\(g_t\)</span> is ‚ÄòInput Gate‚Äô, <span class="math notranslate nohighlight">\((i_t \bigodot g_t)\)</span> is for updatting the cell state.</p>
<p>Finally, ‚ÄòOutput Gate‚Äô is for updating the values of hidden units: <span class="math notranslate nohighlight">\(o_t = \sigma(W_{ox}x^{&lt;t&gt;} + W_{oh}x^{&lt;t-1&gt;} + b_o)\)</span>. So the activateion of the current time is <span class="math notranslate nohighlight">\(h^{&lt;t&gt;} = o_t \bigodot tanh(C^{&lt;t&gt;})\)</span>.</p>
<section id="code">
<h3><span class="section-number">25.2.1. </span>Code<a class="headerlink" href="#code" title="Permalink to this headline">#</a></h3>
<p>Here we implement an LSTM model on all a data set of Shakespeare works.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="n">ops</span><span class="o">.</span><span class="n">reset_default_graph</span><span class="p">()</span>
<span class="n">tf</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">run_functions_eagerly</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Set RNN Parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">min_word_freq</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Trim the less frequent words off</span>
<span class="n">rnn_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># RNN Model size</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># Number of epochs to cycle through data</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># Train on this many examples at once</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># Learning rate</span>
<span class="n">training_seq_len</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># how long of a word group to consider</span>
<span class="n">embedding_size</span> <span class="o">=</span> <span class="n">rnn_size</span>  <span class="c1"># Word embedding size</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">500</span>  <span class="c1"># How often to save model checkpoints</span>
<span class="n">eval_every</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># How often to evaluate the test sentences</span>
<span class="n">prime_texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;thou art more&#39;</span><span class="p">,</span> <span class="s1">&#39;to be or not to&#39;</span><span class="p">,</span> <span class="s1">&#39;wherefore art thou&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Download/store Shakespeare data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;tmp&#39;</span>
<span class="n">data_file</span> <span class="o">=</span> <span class="s1">&#39;shakespeare.txt&#39;</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s1">&#39;shakespeare_model&#39;</span>
<span class="n">full_model_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">model_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Declare punctuation to remove, everything except hyphens and apostrophes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">punctuation</span> <span class="o">=</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span>
<span class="n">punctuation</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">punctuation</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>Make Model Directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">full_model_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">full_model_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Make data directory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">data_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading Shakespeare Data&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loading Shakespeare Data
</pre></div>
</div>
</div>
</div>
<p>Check if file is downloaded.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">data_file</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Not found, downloading Shakespeare texts from www.gutenberg.org&#39;</span><span class="p">)</span>
    <span class="n">shakespeare_url</span> <span class="o">=</span> <span class="s1">&#39;http://www.gutenberg.org/cache/epub/100/pg100.txt&#39;</span>
    <span class="c1"># Get Shakespeare text</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">shakespeare_url</span><span class="p">)</span>
    <span class="n">shakespeare_file</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>
    <span class="c1"># Decode binary into string</span>
    <span class="n">s_text</span> <span class="o">=</span> <span class="n">shakespeare_file</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>
    <span class="c1"># Drop first few descriptive paragraphs.</span>
    <span class="n">s_text</span> <span class="o">=</span> <span class="n">s_text</span><span class="p">[</span><span class="mi">7675</span><span class="p">:]</span>
    <span class="c1"># Remove newlines</span>
    <span class="n">s_text</span> <span class="o">=</span> <span class="n">s_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\r\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="n">s_text</span> <span class="o">=</span> <span class="n">s_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    
    <span class="c1"># Write to file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">data_file</span><span class="p">),</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">out_conn</span><span class="p">:</span>
        <span class="n">out_conn</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">s_text</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="c1"># If file has been saved, load from that file</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">data_file</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file_conn</span><span class="p">:</span>
        <span class="n">s_text</span> <span class="o">=</span> <span class="n">file_conn</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Clean text.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Cleaning Text&#39;</span><span class="p">)</span>
<span class="n">s_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[</span><span class="si">{}</span><span class="s1">]&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">punctuation</span><span class="p">),</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">s_text</span><span class="p">)</span>
<span class="n">s_text</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s1">&#39;\s+&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">s_text</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cleaning Text
</pre></div>
</div>
</div>
</div>
<p>Build word vocabulary function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_vocab</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">min_freq</span><span class="p">):</span>
    <span class="n">word_counts</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">Counter</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span>
    <span class="c1"># limit word counts to those more frequent than cutoff</span>
    <span class="n">word_counts</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">val</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">word_counts</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">val</span> <span class="o">&gt;</span> <span class="n">min_freq</span><span class="p">}</span>
    <span class="c1"># Create vocab --&gt; index mapping</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">word_counts</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
    <span class="n">vocab_to_ix_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">(</span><span class="n">i_x</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i_x</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">)}</span>
    <span class="c1"># Add unknown key --&gt; 0 index</span>
    <span class="n">vocab_to_ix_dict</span><span class="p">[</span><span class="s1">&#39;unknown&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># Create index --&gt; vocab mapping</span>
    <span class="n">ix_to_vocab_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">val</span><span class="p">:</span> <span class="n">key</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">vocab_to_ix_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    
    <span class="k">return</span> <span class="n">ix_to_vocab_dict</span><span class="p">,</span> <span class="n">vocab_to_ix_dict</span>
</pre></div>
</div>
</div>
</div>
<p>Build Shakespeare vocabulary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Building Shakespeare Vocab&#39;</span><span class="p">)</span>
<span class="n">ix2vocab</span><span class="p">,</span> <span class="n">vocab2ix</span> <span class="o">=</span> <span class="n">build_vocab</span><span class="p">(</span><span class="n">s_text</span><span class="p">,</span> <span class="n">min_word_freq</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ix2vocab</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vocabulary Length = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Building Shakespeare Vocab
Vocabulary Length = 8286
</pre></div>
</div>
</div>
</div>
<p>Sanity Check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ix2vocab</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab2ix</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Convert text to word vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s_text_words</span> <span class="o">=</span> <span class="n">s_text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">s_text_ix</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">s_text_words</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">s_text_ix</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vocab2ix</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="n">s_text_ix</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">s_text_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">s_text_ix</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Define LSTM RNN Model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LSTM_Model</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                 <span class="n">training_seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">infer_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LSTM_Model</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span> <span class="o">=</span> <span class="n">embedding_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span> <span class="o">=</span> <span class="n">rnn_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">infer_sample</span> <span class="o">=</span> <span class="n">infer_sample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        
        <span class="k">if</span> <span class="n">infer_sample</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span> <span class="o">=</span> <span class="n">training_seq_len</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">x_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        
        <span class="c1"># Softmax Output Weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
        
        <span class="c1"># Define Embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">]),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;embedding_mat&#39;</span><span class="p">)</span>
                                            
        <span class="n">embedding_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_data</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">)</span>
        
        <span class="c1"># Non inferred outputs</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">outputs</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">])</span>
        <span class="c1"># Logits and output</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logit_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logit_output</span><span class="p">)</span>
        
        <span class="n">loss_fun</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y_output</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">logit_output</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_state</span> <span class="o">=</span> <span class="n">last_state</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span>
        <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
                <span class="n">embedding_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                <span class="n">outputs</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">,</span><span class="n">initial_state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">outputs</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">])</span>
                <span class="n">logit_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
                <span class="n">cost</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">loss_fun</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">logit_output</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_seq_len</span><span class="p">)</span>

            <span class="n">trainable_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span>
            <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">trainable_variables</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">gradients</span><span class="p">,</span> <span class="n">trainable_variables</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">cost</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="n">train_step</span>
        
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">):</span>
        <span class="n">embedding_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">lstm_output</span><span class="p">,</span> <span class="n">final_state_h</span><span class="p">,</span> <span class="n">final_state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">initial_state</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">lstm_output</span><span class="p">),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">])</span>
        <span class="n">logit_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
        <span class="n">model_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logit_output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">model_output</span><span class="p">,</span> <span class="p">(</span><span class="n">final_state_h</span><span class="p">,</span> <span class="n">final_state_c</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">logits</span><span class="p">):</span>
        <span class="n">loss_fun</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_categorical_crossentropy</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fun</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_initial_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="o">.</span><span class="n">units</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
        
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">words</span><span class="o">=</span><span class="n">ix2vocab</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">vocab2ix</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">prime_text</span><span class="o">=</span><span class="s1">&#39;art&#39;</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">]),</span> <span class="n">tf</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_size</span><span class="p">])]</span>
        <span class="n">word_list</span> <span class="o">=</span> <span class="n">prime_text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_list</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="c1">#x = tf.expand_dims(vocab[word], axis=0)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">embedding_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="c1">#embedding_output = tf.expand_dims(embedding_output, axis=1)</span>
            <span class="n">prediction</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>

        <span class="n">out_sentence</span> <span class="o">=</span> <span class="n">prime_text</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">word_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
            <span class="c1">#x = tf.expand_dims(vocab[word], axis=0)</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">embedding_output</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_mat</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="n">prediction</span><span class="p">,</span> <span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embedding_output</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="n">state</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prediction</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span>
            <span class="n">sample</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">i</span><span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                     <span class="k">break</span>
                <span class="nb">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
                <span class="n">word</span> <span class="o">=</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">out_sentence</span> <span class="o">=</span> <span class="n">out_sentence</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">word</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state_h</span><span class="p">,</span> <span class="n">state_c</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">out_sentence</span>
</pre></div>
</div>
</div>
</div>
<p>Define LSTM Model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lstm_model</span> <span class="o">=</span> <span class="n">LSTM_Model</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                             <span class="n">training_seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">infer_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.compat.v1.nn.embedding_lookup), but are not present in its tracked objects:   &lt;tf.Variable &#39;embedding_mat:0&#39; shape=(8286, 128) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.linalg.matmul), but are not present in its tracked objects:   &lt;tf.Variable &#39;W:0&#39; shape=(128, 8286) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.__operators__.add), but are not present in its tracked objects:   &lt;tf.Variable &#39;b:0&#39; shape=(8286,) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
</pre></div>
</div>
</div>
</div>
<p>Train model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s_text_ix</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">training_seq_len</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">s_text_ix</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">training_seq_len</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">test_lstm_model</span> <span class="o">=</span> <span class="n">LSTM_Model</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                             <span class="n">training_seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">infer_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">num_batches</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s_text_ix</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">training_seq_len</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">s_text_ix</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">)</span>
<span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">training_seq_len</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">train_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">iteration_count</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">lstm_model</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="c1"># Shuffle word indices</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Starting Epoch #</span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">))</span>
    <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
    <span class="c1"># Create targets from shuffled batches</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">roll</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">]</span>
    <span class="c1"># Run a through one epoch</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Starting Epoch #</span><span class="si">{}</span><span class="s1"> of </span><span class="si">{}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">))</span>
    <span class="c1"># Reset initial LSTM state every epoch</span>
    <span class="c1">#state = lstm_model.get_initial_state(batch_size)</span>
    <span class="c1">#state = sess.run(lstm_model.initial_state)</span>
    <span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
        <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">lstm_model</span><span class="o">.</span><span class="n">train_op</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">targets</span><span class="p">[</span><span class="n">ix</span><span class="p">],</span><span class="n">state</span><span class="p">)</span>
        <span class="n">train_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch_loss</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loss = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">batch_loss</span><span class="p">))</span>

<span class="n">generated_text</span> <span class="o">=</span> <span class="n">lstm_model</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">words</span><span class="o">=</span><span class="n">ix2vocab</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">vocab2ix</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">prime_text</span><span class="o">=</span><span class="s1">&#39;oh my&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[array([[   0,    1,    2, ...,   39,   40,   41],
       [  42,   43,   44, ...,   66,   26,   67],
       [  68,   69,   70, ...,    7,    0,   88],
       ...,
       [  62,  932,  154, ...,  219,  895,  223],
       [  10,   51,  236, ..., 1129,    2,   12],
       [   0,   61,   81, ...,    1,    2,    3]]), array([[   0,   81,  228, ...,   29,  119, 1207],
       [  23,  227,    2, ...,  557,   50,  227],
       [  17,   79,   79, ...,   18, 1235,  107],
       ...,
       [ 688,   14,   20, ...,    7,  299,  510],
       [  12,  566,   29, ...,  536,  818,   47],
       [ 398,    0,    0, ...,   81,  228,  943]]), array([[  53,  431,   23, ...,    2,  141,   51],
       [1788,   58,  555, ...,  269,  295,  337],
       [ 214, 1644,  279, ...,   85,  735,  197],
       ...,
       [ 124,  322, 2209, ...,  121,   41, 1085],
       [ 376,   21,  517, ...,   21, 2217, 2218],
       [2219,  279,  545, ...,  431,   23,   51]]), array([[ 517, 1104, 1279, ...,  409,    7,  121],
       [ 369,  327,   23, ...,   62,   16,  107],
       [   7,   80,  151, ...,    0, 2235,   10],
       ...,
       [ 496, 1407,   50, ...,  270,   27, 1742],
       [ 596,    0,   20, ...,  228, 2389, 2674],
       [2385, 1411,  100, ..., 1104, 1279,   23]]), array([[1521, 1508,  162, ..., 1014,  842,   27],
       [1915,   26,  263, ...,   12,   82, 1925],
       [2333,   29,  870, ...,  335,  125,  689],
       ...,
       [  29, 1921,  863, ...,   12, 1685,  247],
       [  61,    0,   81, ...,  343,  136, 3019],
       [ 559,    0,  567, ..., 1508,  162, 1356]]), array([[ 578, 2524,   18, ...,  344,  470,  101],
       [ 479, 2379,  535, ...,  100,  207,  273],
       [  16, 2625,  100, ...,   12,    0,   12],
       ...,
       [ 227, 1972,  223, ...,  708,   51, 3145],
       [ 125,  227,  578, ...,  224,   81,    0],
       [  12,    0,   29, ..., 2524,   18, 1541]]), array([[   2,   51, 1391, ...,    0,   23,   10],
       [  47, 3327,  136, ...,  526,  478, 2378],
       [ 540,  349, 2524, ..., 2816,   23,   10],
       ...,
       [   0,  224,   14, ...,  224, 2381, 3161],
       [  18, 1470, 2664, ...,   67,  224, 2794],
       [  12,    0,  498, ...,   51, 1391,    0]]), array([[ 141, 3557, 1132, ..., 1096,  227,    0],
       [  61,    0, 3558, ...,  498, 2479,   26],
       [ 102,  227,   23, ...,  101, 2380, 2379],
       ...,
       [1566,   51,   19, ...,  100,  179, 2388],
       [  27,  101,  274, ..., 1030,  228, 3134],
       [ 136,   55, 1096, ..., 3557, 1132,   10]]), array([[1015,  474,  474, ...,   61,  100, 1590],
       [  16, 1308,   67, ...,    7,   21, 1733],
       [  23,  815,  769, ..., 1925,  186,  162],
       ...,
       [3967,  907,  535, ...,   23,   12, 1056],
       [  81,  509,  101, ...,   53,  223, 3929],
       [3338,   23,    0, ...,  474,  474, 1015]]), array([[1872,  150, 3974, ...,  136,   81,   82],
       [1984,   26,  100, ...,   51,    0,  408],
       [  12,  239,  909, ...,  100,  738,   88],
       ...,
       [1162,  136,   55, ...,    2, 3000,    0],
       [4162,  337,   41, ...,  325,   12,    0],
       [ 111,   12, 2568, ...,  150, 3974, 1703]]), array([[1893,  349,   29, ...,  100,  554,  118],
       [2670,   20,    2, ...,   21, 1006,  100],
       [ 174, 2977,   16, ...,   81, 1899,   61],
       ...,
       [ 941, 1075,   97, ..., 1645,   23, 3950],
       [4120,   23, 2465, ..., 1521,   71, 3781],
       [  36,  204, 3071, ...,  349,   29,  228]]), array([[3769,    0,  100, ...,   39,  761, 4334],
       [  23,    7,  165, ...,   12, 4336, 2664],
       [ 761,   13,  582, ..., 1000,   23, 1523],
       ...,
       [ 174,    0, 3788, ..., 1091,    0,   58],
       [ 112,  122,   23, ...,  201,   10,   12],
       [   0, 4539, 2241, ...,    0,  100, 2809]]), array([[ 946,  101, 1812, ...,  117,   12, 3822],
       [ 102,   11,   10, ...,   53,   51, 2716],
       [  12, 4544,   29, ...,   12, 1438,  100],
       ...,
       [ 226, 2443, 2386, ...,   12, 3510,   29],
       [   0,   53, 2416, ..., 1419,   50,  100],
       [ 262,   12, 2251, ...,  101, 1812,   53]]), array([[   0, 3781,   36, ..., 4699,  136, 1872],
       [4291, 4291,  274, ...,  534, 1068,   23],
       [ 524,  101,  239, ...,  219,   61, 2105],
       ...,
       [4764,  100,  382, ...,  105, 1364, 4786],
       [  97,   18,  274, ...,  265, 4764,   12],
       [2377,  101,  264, ..., 3781,   36, 2427]]), array([[ 101, 1023, 4786, ...,  227,   85,  228],
       [4842,  662,   23, ...,  894,  246,   85],
       [ 118,    6,   85, ...,  174,  141, 2426],
       ...,
       [  90,   12, 3306, ...,   23,    0,    0],
       [  85, 5014,   67, ...,    0,   23, 2995],
       [2053, 4975,  446, ..., 1023, 4786, 1162]]), array([[2377, 4730, 1276, ..., 1021,    7,   74],
       [  88,  818,  910, ...,   12, 2732,    0],
       [  21, 4590,   81, ...,  272,    0, 3425],
       ...,
       [  29,  223, 4928, ...,   29,  790,    0],
       [ 151,    2,   62, ...,  266,   88,    0],
       [1284,   12, 1466, ..., 4730, 1276,  251]]), array([[ 148,   88,   20, ..., 4896, 5155, 5156],
       [ 349,  100,  174, ...,  349,  638,   12],
       [ 864,  509,  421, ...,  534, 1128,  465],
       ...,
       [1064,  100, 1064, ..., 1671,  129,   12],
       [   0,   29,   55, ...,  250,   23,  141],
       [  12, 4265,   26, ...,   88,   20,   21]]), array([[ 232, 1683,  101, ...,    0,   29,   18],
       [3202, 4741, 2037, ..., 1093, 2513,    0],
       [  50,   12,  621, ..., 1973,    0, 1972],
       ...,
       [  88, 1346,  227, ..., 2243,  337,  272],
       [ 227,   31,   12, ...,  118,   23, 5378],
       [  88,  337,   81, ..., 1683,  101, 4764]]), array([[ 102,   20, 3612, ...,  105,  136,  337],
       [  81,   12,  424, ...,  842,   50,   10],
       [3316,  227,  124, ...,   29,    0, 1340],
       ...,
       [ 967,  204,  106, ..., 1321,  227,  174],
       [5465,   67,  509, ...,    0, 3385,  233],
       [ 101,  644, 1475, ...,   20, 3612, 2416]]), array([[  19,   29,  509, ...,  189,   12,  883],
       [1328,   21,  823, ...,   61, 5296,   29],
       [5295,   88, 2127, ...,  274,   10, 1032],
       ...,
       [1752, 2398, 1847, ...,   47,   14,  227],
       [5154,  118,    7, ..., 2498, 3249,   30],
       [   7,  922,   10, ...,   29,  509,  279]]), array([[5554,   23, 1593, ...,  328,   67,   16],
       [4087,  341,  710, ..., 5296,   29, 5293],
       [  23, 5299,   29, ...,  228, 1733,  263],
       ...,
       [ 729,   82, 2526, ...,  105,  219,    0],
       [   2,  228, 4029, ...,  295,  107,  701],
       [ 498, 5643, 1133, ...,   23, 1593,  540]]), array([[  12,  639,    6, ...,  101,  506,  100],
       [  14,    0,    0, ..., 1132,   17,   18],
       [5416,   55,  197, ..., 5008,    0, 5643],
       ...,
       [ 337, 1869,  102, ...,  335, 2398, 5699],
       [5640,   85,   51, ...,   47,  100,  219],
       [   0,  107,  227, ...,  639,    6,   12]]), array([[  10, 1180,  228, ...,  214,    0,  578],
       [ 212,  744, 5631, ...,  228, 1132, 2701],
       [5638,  100, 1590, ...,   23, 5761, 5631],
       ...,
       [ 100,  148,  214, ...,   12, 3238,   61],
       [ 232, 1683,   18, ...,   96, 5637,   27],
       [2404, 2465, 1093, ..., 1180,  228,    0]]), array([[3085, 5637, 3085, ...,  272,    0, 1287],
       [1807, 5643,   50, ..., 2671, 2971, 5643],
       [ 227,  272, 3117, ...,  355,   26,  100],
       ...,
       [  85,  967,  863, ...,    2,   69,    0],
       [  81,  569,   23, ..., 1798,  856,  125],
       [   7,  710,    0, ..., 5637, 3085, 2404]]), array([[5861,  932, 4007, ...,  588,    0,  115],
       [ 874,  898,  111, ...,  101,    0,    0],
       [ 207,    0,  101, ...,  555,  118,  349],
       ...,
       [ 208,  224, 1096, ..., 2404,  224, 2794],
       [ 107,   78,  729, ...,  127,  214,   14],
       [ 328,   29,  322, ...,  932, 4007,  107]]), array([[ 322, 1991, 1315, ...,   29, 3028, 1531],
       [  81,   18, 1521, ...,   29,    0,  498],
       [5954, 1004,   23, ...,   18,  604, 5777],
       ...,
       [  51, 2387,   51, ...,  184,   12, 3697],
       [ 174, 5451,   12, ...,  150,   55,  387],
       [   0,  376,  100, ..., 1991, 1315, 5954]]), array([[ 308,  505,   10, ...,  100,  534,  451],
       [   7,   30, 1207, ...,    0,  387, 2398],
       [  85,  967, 1869, ...,   29,   26, 5657],
       ...,
       [ 227,   53, 3191, ...,  343,    0,   29],
       [   0,   23, 3670, ...,  101, 3894,  842],
       [  88, 3471,  961, ...,  505,   10,    0]]), array([[6124,    2,  604, ..., 2340,  294, 6125],
       [  23,  399,    0, ..., 6088,  932,   61],
       [ 673,    0,  107, ...,  272,  818,  107],
       ...,
       [ 973, 6201, 2398, ..., 2450, 1004,   10],
       [  38,   42, 3080, ...,  227, 1029,  328],
       [ 417,  359,    0, ...,    2,  604,  125]]), array([[ 573,   81,    0, ...,  932, 1385, 6205],
       [6107,   10,  973, ...,  359,  452, 2404],
       [ 227,  555,  118, ...,   86,  227,  125],
       ...,
       [3444,  317, 1607, ...,  101,  482,  554],
       [  88,  588, 4400, ..., 6092, 1162,  100],
       [ 207,   11,   23, ...,   81,    0,  100]]), array([[   0,   29,   12, ..., 6088,   26,  850],
       [1207,  201,  101, ...,  540,   21,    0],
       [ 204,   20,    0, ..., 3208,   23,   10],
       ...,
       [   0, 2278,   27, ...,   53, 6053, 2447],
       [6075,   97,    7, ...,  819,    0,  115],
       [ 223, 4013,   10, ...,   29,   12, 6065]]), array([[ 111,   47,   51, ...,  240,   55, 1071],
       [3245, 6246,   81, ..., 2966,   81, 6081],
       [2127, 2398, 6078, ...,  996,   29,  138],
       ...,
       [ 236,  105,  911, ..., 4562, 4535, 6088],
       [1686, 1033,    7, ...,  272,  596, 1354],
       [  23,  291,  216, ...,   47,   51,  434]]), array([[ 911,    0,   13, ...,    0, 6387,  102],
       [  20,    0, 6388, ...,  885,  111,  216],
       [ 111, 2833,  571, ...,   12, 1541, 2404],
       ...,
       [ 359,  951,  588, ...,  100,  738,  100],
       [ 578,  295,  350, ...,   81, 6451,  465],
       [6451,   18, 3861, ...,    0,   13,   18]]), array([[ 967,  542,  100, ...,   26,  535,   61],
       [   0,  233,  911, ...,   55, 3650, 4148],
       [  23, 1364,   12, ..., 6451,   82,   17],
       ...,
       [  88,  101, 2380, ...,   55, 3877,  227],
       [  63,   12, 5266, ...,    0,  135,  162],
       [4012,  946,   34, ...,  542,  100,  272]]), array([[ 727,  725,  328, ..., 2241,   16, 2451],
       [5622,   23, 6442, ..., 2451, 2354, 2361],
       [  18,  162, 1050, ..., 5622, 5622, 1921],
       ...,
       [  39,   12,    0, ...,   48,  421,   29],
       [ 227,    6,  227, ...,  141,  101, 3836],
       [   0,  141,    0, ...,  725,  328,   23]]), array([[  10,  118,  105, ..., 3966,   29,    0],
       [  23,  270,   10, ..., 3234,   88,    0],
       [6538,   10,  738, ...,  118,   12,    0],
       ...,
       [ 101, 2387,  568, ...,  731,  329,   37],
       [ 960,   51,    4, ...,   12, 1765, 3791],
       [ 223,   23,    0, ...,  118,  105,   18]]), array([[   0, 2018,    2, ...,    0,   23, 5330],
       [3843,   50, 3342, ...,    0,  141, 6492],
       [  14,  214,  273, ...,  911, 3752,  232],
       ...,
       [1475,  224, 3497, ...,  208, 1703,  227],
       [  85,  238, 2692, ..., 6473,  729,   23],
       [ 150,  295,  262, ..., 2018,    2,   12]]), array([[1358,   81,  997, ...,  101, 2380,   85],
       [  12,  778,  585, ..., 1944,   10,   20],
       [   0,   18,    0, ...,   17,   48, 5202],
       ...,
       [  26, 3140,  148, ...,    0, 2397,  498],
       [2386,   81,  159, ..., 2156,    2,   42],
       [ 369,  220, 1315, ...,   81,  997,   29]]), array([[4350,    0,  159, ...,   50,    6,   12],
       [1063,  387,   10, ...,  136,  263,   88],
       [1311,   18, 2389, ...,    0,   23,    0],
       ...,
       [ 174,   16,  274, ...,   12, 2357,   10],
       [ 980, 6442,   12, ..., 1685, 2738, 6471],
       [ 224,   81, 3723, ...,    0,  159,    0]]), array([[1765, 1287,  227, ...,    7, 6754,  933],
       [ 118,   23,  101, ...,  521, 4300,  694],
       [   0,  102,  120, ...,   85, 1088,  387],
       ...,
       [  23, 3105,  107, ...,   10, 5067,   23],
       [ 150,  295, 6500, ...,   12, 3326,   29],
       [  12, 1234,  337, ..., 1287,  227,   26]]), array([[  12,   28,   29, ...,  221,   51, 1358],
       [ 224,  102,   88, ...,  291,  223,   33],
       [ 224,  389, 2326, ...,  540,   10, 6096],
       ...,
       [3596,  863, 4612, ..., 6893, 6786,  107],
       [ 907,  142,    0, ...,  100,  534,   18],
       [   0,   29,    0, ...,   28,   29,   47]]), array([[ 100,  272,  770, ...,   47, 1073,    7],
       [   0,   50,  908, ...,  465,  142,   50],
       [1073,  319,  349, ...,  417, 4381, 6784],
       ...,
       [2450, 6771,   88, ..., 3571,   53,   21],
       [   0,    6,    7, ...,  856,   10, 1749],
       [3109,  111,   20, ...,  272,  770,  908]]), array([[ 169,   23,  204, ...,   53,  907,  102],
       [3133,  272, 1187, ...,  371,   26,    7],
       [  74, 1132,    0, ..., 3349,   29,   21],
       ...,
       [  10,   12, 3113, ...,  240,  223,  204],
       [1425, 6783,   88, ...,   55,  328,   29],
       [ 554,   23,  139, ...,   23,  204,   10]]), array([[ 150,   55,   20, ...,  911, 5526, 1984],
       [   0,  101, 2559, ...,   12,    0, 1720],
       [   0,   23, 1048, ..., 1685,    6,  227],
       ...,
       [1117, 3248,  111, ...,  112,    0,   23],
       [2556,   18,  437, ...,   55, 6821,  100],
       [   0,  223, 1810, ...,   55,   20,  818]]), array([[  55,   85,   12, ...,  214,  266,   53],
       [2849,  224,   26, ..., 6758, 6767, 6768],
       [  23,   64,   85, ...,  911,  898,  550],
       ...,
       [ 209,  100, 1856, ..., 1695,  100,  272],
       [  12, 2528,   53, ..., 1163,   23,  987],
       [ 911,  346,   23, ...,   85,   12,    0]]), array([[1580,    0,   46, ...,   29, 1692, 2380],
       [6788,   12,  205, ..., 3748,  417,    0],
       [ 907,  263,   51, ...,   85,  643, 6962],
       ...,
       [  23,  148,  863, ...,   23,    0,   12],
       [1521,  160, 2522, ...,   81,  509, 3703],
       [5463,    0, 2404, ...,    0,   46,   85]]), array([[2840,  842,    0, ..., 5349,  227, 2056],
       [2056, 2411, 1096, ...,    0,    6,   42],
       [1572, 1749,  552, ..., 1137, 4448,  540],
       ...,
       [ 557, 2404,  100, ...,  398,    0, 1023],
       [ 350,    0,  159, ...,   18,  434,  815],
       [ 392,  246,   18, ...,  842,    0, 2329]]), array([[ 274, 2629, 5468, ..., 2639,  387, 2404],
       [3331,  125, 1073, ...,   29,  227, 2639],
       [2404, 3331, 2404, ...,   18,  491,    0],
       ...,
       [  88,   51, 2710, ...,   23,    7,  710],
       [1823,  223, 6770, ...,  274, 1425,   29],
       [  51,    0, 1430, ..., 2629, 5468, 1411]]), array([[   2,   12, 1229, ...,   85, 4818,  575],
       [ 136, 6770,   23, ...,    0,    0,  111],
       [3170,    0, 7009, ..., 4434, 1685,   82],
       ...,
       [   2, 5292,   33, ..., 4153,   29,    0],
       [  53,  227, 2639, ...,   48,  907,  102],
       [  20, 3086,  349, ...,   12, 1229,   29]]), array([[  88,  129, 5395, ...,   12, 6530, 2670],
       [4557,   18,    0, ..., 2639, 1162,  465],
       [2418,   18, 1685, ..., 6784,  588,   61],
       ...,
       [   2, 2063, 1071, ..., 1346,   64,   17],
       [4676, 2860, 4077, ...,  201,   12, 1394],
       [  12,  639,    0, ...,  129, 5395,   21]]), array([[2815,  387,   10, ...,   10, 2376,  101],
       [2976, 1087,  228, ...,  930,   29,    0],
       [  23, 2009, 1685, ...,  111,  465, 2954],
       ...,
       [   0,   29,    0, ...,  227,  204,    0],
       [1879,   23, 1376, ...,   17,   18, 3737],
       [  26,    6,    0, ...,  387,   10,  123]]), array([[ 778,  494,  127, ...,  907,   14,  305],
       [  29,  136, 2692, ...,  227,  112,  694],
       [1096,  101, 1685, ..., 1315, 5091,   29],
       ...,
       [  10,  228, 2978, ..., 2451, 2354, 2367],
       [  12, 2383, 2366, ...,   12,  160, 1191],
       [  29, 6212, 6123, ...,  494,  127,   12]]), array([[  88,   98,  101, ...,  742,   33,  224],
       [ 391,   55,   12, ...,  442,   23, 1063],
       [1093,  785,    0, ...,   17,   12, 2768],
       ...,
       [  55, 7185,  590, ...,   81, 2838,   12],
       [1112,   29,    0, ...,   23,  266,    0],
       [  81, 2250,    0, ...,   98,  101, 1191]]), array([[  51, 2904,  107, ...,   12,   13,   50],
       [ 907,    2,   55, ...,  214,  142,   88],
       [ 232,   23,  241, ..., 4071,    6,  911],
       ...,
       [5654,  854, 7343, ...,   62,   55,  279],
       [ 228, 1908,   23, ..., 7343,    7,    8],
       [  11, 7189,  227, ..., 2904,  107, 4427]]), array([[5281,  100, 2243, ...,   55,   81,  274],
       [  53,  228,  200, ...,  162, 3975,   10],
       [ 228, 7311,  465, ..., 2243,  227, 1345],
       ...,
       [   0, 2398,  116, ..., 7327,  842, 4588],
       [  12, 5278,  815, ...,   10,    0,   51],
       [ 317,  224, 2409, ...,  100, 2243,  227]]), array([[1250,   23, 5689, ..., 4219,  328, 5342],
       [  23,    0,  946, ..., 1720, 2089,   23],
       [   0,    0,  142, ..., 2202, 1364,   53],
       ...,
       [ 852,   12,  585, ...,  112,  113, 1317],
       [2763,  812, 3022, ..., 1836, 7054,   47],
       [  18, 5820,  224, ...,   23, 5689,    0]]), array([[4974, 6263,   67, ...,   23,   51,    0],
       [  33, 2613,    2, ..., 1745,  125,   29],
       [ 136, 4635,    0, ..., 4670,   42,  639],
       ...,
       [6312, 4758, 2250, ...,  125,  624,    0],
       [ 379,   26,   42, ..., 2713, 4758,   12],
       [   0,   29,  911, ..., 6263,   67,   12]]), array([[ 174, 5807,   67, ...,  615,   29, 2376],
       [2447,   48, 3412, ...,   69,   18, 3681],
       [2465,  997,   29, ...,  150,   27,  642],
       ...,
       [   0,   29,  339, ...,   67,  101, 3184],
       [1191,   23,  815, ...,   18, 1279,   29],
       [ 118,   12,   13, ..., 5807,   67,   12]]), array([[  10,   11, 3331, ...,  535, 1523, 3146],
       [ 954,  849,   12, ...,   23, 1200,  149],
       [ 232,   67,  101, ...,   50,   88,   10],
       ...,
       [ 694,   26,    2, ...,   23, 2933,    1],
       [   7,    8,   20, ...,    0, 4758,  588],
       [ 987, 1545,   26, ...,   11, 3331, 7327]]), array([[   0,  125,  100, ..., 3100,  124,  911],
       [7117,   88,   29, ...,   12,  297,   55],
       [  81,  228,    0, ..., 1132, 6095,  227],
       ...,
       [   0,  107,  159, ...,    0,    0,  159],
       [  14,  956,   10, ...,  907,  272, 1048],
       [2377, 7010,    2, ...,  125,  100,  272]]), array([[7363, 7495, 7184, ...,   20, 3005,   29],
       [  12,   52,  150, ...,  108,  535,   21],
       [4938, 7012, 7012, ..., 5180,  735,  535],
       ...,
       [ 118,  100, 2243, ...,   21,  479,   36],
       [7488, 4448,  100, ...,  112,   16,   85],
       [  18, 5142, 2069, ..., 7495, 7184, 6765]]), array([[   2,    0,  856, ...,  297,   23, 6969],
       [  29,  136,  639, ...,    0, 1275,   85],
       [  51, 7110,    0, ...,    0, 3904,  236],
       ...,
       [7329,   47,    0, ...,    0, 1749,  911],
       [1770,  343,   18, ...,    2,   12,    0],
       [   0,  148,   88, ...,    0,  856, 7111]]), array([[   2,   12, 4405, ...,  254,  101,  482],
       [  23,  319,   85, ...,   23, 3822,  118],
       [ 107,    0,    0, ..., 7012, 7184,   23],
       ...,
       [  30,    7,   18, ...,  223,   85,   51],
       [ 466,   23,    0, ...,   16, 1075,  224],
       [ 102,   20,    0, ...,   12, 4405,   23]]), array([[2710,   85, 3145, ..., 1795,  542, 1612],
       [ 111,   11, 7493, ...,    1,   18,    0],
       [  74,    7,   88, ..., 7493, 1004, 2278],
       ...,
       [  37,   11, 7495, ...,   68, 1107,  228],
       [2978, 1685, 6758, ..., 1153,  946,   12],
       [ 108,   29,   18, ...,   85, 3145,   23]]), array([[3623,  722,  273, ..., 7495,   61,  709],
       [  55,  227,  101, ...,   69,   18,  166],
       [  23, 1093,  120, ...,  223,   10,   51],
       ...,
       [   7,    0,   21, ...,  295,  613,   23],
       [ 613,  214,  102, ...,  359, 1545, 2427],
       [  16,  791, 1685, ...,  722,  273,  118]]), array([[  88, 1709,  694, ...,   23,   17,   48],
       [4400,    0,   67, ..., 2355, 6670,    0],
       [7153, 6348,   23, ..., 3894,  540, 4974],
       ...,
       [1133,   10,    0, ...,  227, 3730, 6946],
       [  10,  911, 1795, ...,   23,    2,   26],
       [3894,  510,   55, ..., 1709,  694, 2451]]), array([[ 809, 3168, 1880, ...,  280, 1685, 6758],
       [1162, 2529,  101, ..., 7598, 7599,    0],
       [7153,  349, 2118, ..., 7153,  100,  534],
       ...,
       [ 432,  322, 2401, ..., 5700, 6348,   23],
       [7104,  350, 2203, ..., 1685, 7153,  204],
       [2427,  228,   43, ..., 3168, 1880,  223]]), array([[ 118,    6,  100, ..., 2480, 2450, 2149],
       [1162,  204,  100, ...,  101, 1498, 1685],
       [7153,   48, 1896, ...,   81,  154,  159],
       ...,
       [1004,    2,  472, ...,   85, 2383, 2396],
       [7012,  561,  118, ...,  106, 7009, 1276],
       [1107, 7012,   23, ...,    6,  100,  240]]), array([[  20,  121,   23, ...,  107, 7645,   23],
       [1397,    0,    0, ..., 3319,  223,  227],
       [  26,   14, 2241, ...,   10, 1160,   12],
       ...,
       [  12, 1765,  124, ...,  118,   88,   53],
       [2710,    6,    7, ...,    0,  118,   10],
       [ 101, 2481, 2710, ...,  121,   23,  349]]), array([[  23,  221, 2489, ...,    0, 1685, 7153],
       [ 694,  673,  911, ...,  151, 2268,   26],
       [  12,    0, 2310, ..., 7422,   88,   18],
       ...,
       [7673,  247,  954, ...,   26,   12,    0],
       [   0,   23, 2376, ...,   50,   33,   12],
       [ 856,  842,    0, ...,  221, 2489,   53]]), array([[ 369, 1425, 7598, ...,  228, 1827, 5518],
       [6294,   55, 5518, ...,  534,    0,  179],
       [ 227, 4782,  124, ..., 3010,   97,  101],
       ...,
       [2377,   23, 3782, ...,  251,  986,  105],
       [ 136,  498, 3177, ...,  100,  534, 2855],
       [ 227,  102, 2404, ..., 1425, 7598,  100]]), array([[6026,  151,    0, ...,  274, 1765,   84],
       [ 223,   85,   18, ...,  136,   81, 1710],
       [1315, 3177,  100, ..., 2354, 2355,   48],
       ...,
       [1508,    0,   29, ...,  114,   61,   50],
       [ 540,   10,  115, ...,  377,    0,   53],
       [  69,  317,  111, ...,  151,    0, 1444]]), array([[1388,  228, 2021, ...,  945, 1490, 2672],
       [ 101,  729,  214, ..., 2404, 7688,  555],
       [ 228, 1334, 1163, ...,  356,   53,  885],
       ...,
       [5189,   10,  228, ...,    0,    0,  337],
       [  12, 4802, 1029, ...,  630,   10,    0],
       [ 337,  159, 1527, ...,  228, 2021,   23]]), array([[5981, 1839,    0, ...,   12, 5834,    0],
       [  85,   12, 1955, ...,  660,    0,    0],
       [   0,   18,  198, ...,   12, 4459, 3782],
       ...,
       [ 102,  262,   78, ...,  100,  179,  228],
       [  59,  588,  101, ...,  715,  993,  100],
       [ 382,  266,  162, ..., 1839,    0, 7340]]), array([[ 228,  274,  603, ...,  174,   88,  100],
       [ 262,  227,   53, ..., 2055,    4,  417],
       [7603,  136,   81, ..., 2357, 2692,    0],
       ...,
       [   0,   10,    0, ...,  369, 1026,  165],
       [4229,  509,  253, ...,   81,  791,  204],
       [ 274,  101, 2387, ...,  274,  603,  417]]), array([[ 842,    0,  115, ...,   88,  356,   18],
       [ 811,  264,  238, ...,  279,   38,   51],
       [4635,  482,  124, ...,   10, 1310,  387],
       ...,
       [1184, 6532,  136, ...,   50,   53, 1021],
       [ 224,   58,   88, ...,  266,  465,   81],
       [  27, 1359,   50, ...,    0,  115, 1254]]), array([[7728, 7073,   23, ...,  242,   55,  954],
       [  20,    7,  710, ...,    0,  118,   53],
       [ 100,  534, 1583, ...,   14, 6213,   23],
       ...,
       [2333,  475, 7247, ..., 2109,   12, 1991],
       [  29,    0,   98, ..., 6053,    6,   50],
       [  18, 3619,    0, ..., 7073,   23, 7184]]), array([[ 322, 2070,  349, ..., 6445, 2398, 7721],
       [  23,  908, 7730, ...,  643,  228, 2919],
       [  14,  246,  328, ..., 3596,  107,   37],
       ...,
       [ 101, 1239,   26, ...,   10,   12, 7386],
       [ 208,  100,   10, ...,   81,   50, 4762],
       [2398,   12, 1442, ..., 2070,  349,   26]]), array([[7034,  308,  385, ..., 4782,  149, 1685],
       [3331,   26, 4087, ...,    0,   12, 1071],
       [  29,   18, 1964, ...,    0,   17,   12],
       ...,
       [7788,   18,  610, ..., 1175, 3802,   10],
       [2333, 7784,   53, ..., 1285,   34, 3082],
       [  20,   24, 1531, ...,  308,  385,   29]]), array([[4119,   18, 3805, ...,   81,   18,    0],
       [ 150,  967,  973, ...,  150,  118,   88],
       [3333, 4120,  228, ..., 4119, 4120,   20],
       ...,
       [ 111,  911, 1163, ..., 2137,  223,  100],
       [ 738,  224,   14, ...,   51, 2832, 5456],
       [ 911,  705,  911, ...,   18, 3805, 1114]]), array([[  88, 2472, 7786, ...,  295,   37,   36],
       [2761,   61,    0, ..., 4771,   29, 3338],
       [ 150,  967,   20, ...,   23,  106,  729],
       ...,
       [   0, 1307,  118, ...,  111,  102,  907],
       [  67,   23,   88, ..., 4119,  111,  301],
       [ 142,  136,   18, ..., 2472, 7786,  102]]), array([[2040,   18,  772, ...,   26,  125,  224],
       [  14, 3715, 4119, ...,   23,  900, 7312],
       [  55,  102, 1425, ..., 3338,   23,  266],
       ...,
       [1068,  247,  359, ...,  240,  118,  228],
       [ 478, 4119,   23, ..., 3477,  738,  228],
       [2387, 4419,   23, ...,   18,  772,  863]]), array([[  50,  434,  102, ...,    0,  348, 4119],
       [2427,  227,  226, ..., 1285,   12,    0],
       [  10, 3261,  322, ..., 1713, 4120,   29],
       ...,
       [  26,    0,    2, ...,  136,  475, 5957],
       [2387,  382,  490, ...,   39, 3111,   29],
       [3141,  238,  147, ...,  434,  102,  332]]), array([[2713, 3177, 6775, ...,  958,  996,   23],
       [ 149,  224,  102, ..., 1440,  465,  262],
       [  26,  907,  272, ...,   18,  294,   14],
       ...,
       [  55, 7829,  272, ...,  908,    0,   10],
       [  18, 2054, 6866, ...,  359,   12, 7499],
       [  29,   51,    0, ..., 3177, 6775, 6775]]), array([[  52,  223,  232, ...,    0, 6866,  136],
       [  81,   88, 3471, ...,   23,  815,  240],
       [  16,    0, 7812, ...,  552,  189,   12],
       ...,
       [ 725,  228,  265, ..., 6522, 4270,  392],
       [ 595,   12, 1201, ..., 2392, 6866,   85],
       [  12, 3248, 2404, ...,  223,  232,   39]]), array([[ 205,    7,  979, ..., 2529, 5805,  150],
       [ 208,   21,  241, ...,   55,  209,   18],
       [1291,  439,   55, ...,   26, 2235,  149],
       ...,
       [ 929,   12, 2629, ..., 6076,  491, 2451],
       [2354, 2361,   18, ...,  842,   88, 3471],
       [ 228, 4775,    0, ...,    7,  979,   36]]), array([[  29, 2376,   97, ...,   21,  264,   81],
       [  26,  224,   37, ...,   16,   23,    7],
       [   8,  291,   18, ...,   14,   88, 1306],
       ...,
       [  97, 2446,  223, ...,  407,  111,  778],
       [2882,  227, 2404, ...,  710,  438,   30],
       [1040,  943,    0, ..., 2376,   97, 3697]]), array([[  12,    0,  739, ...,  201,   61,  540],
       [ 966, 1137,  227, ..., 4345,  224,  263],
       [  18,  622, 6230, ...,  860, 2922,   23],
       ...,
       [ 399,    2,  127, ...,   41, 6775,    7],
       [  30, 3128, 1091, ...,  272,  227,  758],
       [ 231,  540,  272, ...,    0,  739,   61]]), array([[ 963,   29,  434, ..., 6369,  223,  124],
       [1806, 1093, 4084, ..., 1532,  908, 4236],
       [  29,  481,  609, ..., 4071,   10, 4704],
       ...,
       [ 223,  201, 7868, ...,  272,  818,  238],
       [ 102,  179, 7868, ...,   20,  695, 2451],
       [7868,   23, 7852, ...,   29,  434,    0]]), array([[ 125,   81, 2293, ..., 5192,   23,  542],
       [ 417,  359, 4067, ...,    0,   81,    2],
       [  62,  978,  224, ...,   12, 4802,   29],
       ...,
       [2416, 7870,  227, ..., 1885,  107, 5405],
       [  12, 5465,   55, ...,  491, 7871,   48],
       [ 100,  954,  954, ...,   81, 2293,    0]]), array([[2450,  150,   12, ..., 3817,  228,  478],
       [  81,    2, 7852, ..., 2451, 7871,   23],
       [7870, 7852,   39, ...,   12,  123, 1098],
       ...,
       [   0,    0,    0, ...,    0,   10,  115],
       [5552, 5537, 7858, ..., 2404, 2045, 7868],
       [2203,   10, 7852, ...,  150,   12, 1468]]), array([[   0,    7,   74, ...,    0,   82, 6371],
       [5033,   85,   18, ..., 7752,  349,   39],
       [  12, 2719, 5753, ...,    7,    0,   17],
       ...,
       [  33, 2975, 1687, ...,   10, 7871,   23],
       [ 254,   88,    0, ...,  208,  227,  535],
       [6117,   11,   33, ...,    7,   74, 7749]]), array([[1276, 1475,  581, ...,   81,  958, 7852],
       [  88,   61, 2404, ..., 7860,   39, 3737],
       [ 100, 1544, 1106, ...,  534,  107,  214],
       ...,
       [  12, 7567,   29, ...,   29,  295, 3087],
       [ 535,  214, 6345, ...,   20,  748, 7909],
       [ 227,  102,   20, ..., 1475,  581,    7]]), array([[  12,    0,  621, ..., 2916,  107,    0],
       [3612,   85, 3612, ...,  986,  253,  224],
       [2824,  118,  124, ...,   50, 1123, 1040],
       ...,
       [  12, 1052,   29, ...,   12,  610,  478],
       [  29, 1293,  100, ...,  174, 3246,   88],
       [5370,   85,  295, ...,    0,  621,   51]]), array([[7906,  136, 6750, ..., 1184,  135,   12],
       [6445, 2398, 7912, ...,  731, 2318, 6999],
       [  51,  344,  311, ...,   67, 7030, 1438],
       ...,
       [ 389,   23, 1033, ...,   88, 2780, 6374],
       [  20,    0,  517, ...,    0,  274, 4914],
       [ 660, 1817, 1093, ...,  136, 6750, 7107]]), array([[ 141, 1164,   10, ...,   23,  764,   51],
       [ 482,  387,   17, ...,  239,    0,  463],
       [  81,   17,   12, ...,  873,   18,    0],
       ...,
       [6308,  997,   85, ..., 2759, 3889,    2],
       [ 136, 1712, 1192, ...,  204,   81, 2061],
       [  27,  162,   55, ..., 1164,   10,  262]]), array([[3208, 7908,   36, ...,  179,   55,  234],
       [ 100,  266,   18, ...,   12, 5514,   26],
       [ 389,   17,  297, ...,  761, 2831, 2664],
       ...,
       [2291, 4038,  272, ...,  907,  174,   88],
       [ 179,  907, 3067, ...,  224,  207,   11],
       [2398, 7936, 3288, ..., 7908,   36,  266]]), array([[7936, 3288,   36, ...,  149,    6,  735],
       [  20,  274, 3801, ..., 6982,  100,  174],
       [1645,  343, 3015, ...,   29,   23, 7293],
       ...,
       [ 214,  115, 4655, ..., 1290,  907,  535],
       [ 154,   10,   20, ...,    0, 2915,   67],
       [  12,    0, 3618, ..., 3288,   36,  290]]), array([[  88, 7203,    6, ...,   23,   82, 5610],
       [ 506, 1430, 1430, ...,  227,   23,  101],
       [  52, 2475,  761, ...,  214, 2175,   53],
       ...,
       [  53,  118,  232, ...,  100,   14,  129],
       [ 343,  227,  826, ...,   26,    0,   81],
       [  18,    0,    0, ..., 7203,    6,  227]]), array([[ 292,  291,  136, ..., 1132,   18,   93],
       [2475,  761, 1036, ...,  270,  154,  987],
       [  10,    0, 1541, ...,  174,   23,  272],
       ...,
       [ 107,   17,  107, ...,  224,  263,   61],
       [2727,  224,  148, ...,   23,   85,    0],
       [   0,   12,  275, ...,  291,  136,  856]]), array([[ 223,   61, 1356, ...,   18,    0, 7682],
       [ 100,  174,   88, ...,  136, 7682,   20],
       [1028, 7934,   50, ..., 1414, 5669,  228],
       ...,
       [  29,  101,    0, ...,  159,   81,  249],
       [  23,  103,  105, ...,   42,    0,    0],
       [  67,   42, 5919, ...,   61, 1356,  967]]), array([[   0,  100,   94, ...,   10, 7949,   10],
       [ 249, 7785,  208, ...,  308,  585,   81],
       [   0,   29,  136, ...,  542,   10,   20],
       ...,
       [ 735, 2222, 1882, ..., 7958,    6,    7],
       [  38,  851,  100, ...,  272,   88,  154],
       [ 274, 4238, 7954, ...,  100,   94,    0]]), array([[1731,   12, 2208, ..., 7968,   39,  228],
       [ 973, 2404, 7958, ...,   53,  136, 7762],
       [3022,  100,  534, ...,  249, 7971, 7958],
       ...,
       [ 326,  270,  343, ...,  516,  295,    0],
       [  26,  326,   82, ..., 3105, 1056,   12],
       [  91,    0,   48, ...,   12, 2208, 7968]]), array([[   7, 7558, 1009, ...,   97,   62,   20],
       [2433, 6026,   21, ...,  939, 6098,   67],
       [  12, 3081,   29, ...,   42,   41, 4795],
       ...,
       [   0, 1653,  136, ...,  153,   81,   12],
       [2155,  227, 2241, ..., 3502, 7963,   67],
       [  47, 3323,  207, ..., 7558, 1009,  125]]), array([[1764, 2477,  811, ...,    0,   10, 1293],
       [  68,   23,  394, ...,  879,   29, 2259],
       [ 100,  272, 2418, ...,   88, 1660,   10],
       ...,
       [  51,  434,   29, ...,   23,  907,   14],
       [1741,  141,  521, ...,    0,  100,  142],
       [   0,   85,   12, ..., 2477,  811,  105]]), array([[2354, 2360,   18, ...,    2, 7990,  250],
       [2354, 2360,   18, ...,    2,   12, 3054],
       [5328, 2354, 2365, ..., 1892,   10,    0],
       ...,
       [ 349,   18,  945, ...,  100, 3942, 3942],
       [ 204, 2278,  162, ...,   62,  118,   39],
       [ 118,  121,  369, ..., 2360,   18, 1254]]), array([[  18,  475, 3291, ...,  983,  118, 1162],
       [   0,   18, 5132, ..., 1892,   23,  561],
       [ 118,  100,  842, ...,   12, 1362,   97],
       ...,
       [ 227,   53, 7351, ...,  150,  967, 6646],
       [ 204, 4147,  387, ...,  866, 1419, 2488],
       [2404,   12,    0, ...,  475, 3291,   47]]), array([[  29,   45,  100, ...,    0,   29,    0],
       [  18,  622, 1750, ..., 7225, 7991, 1368],
       [1276,  742,   10, ..., 3055, 2398, 1892],
       ...,
       [  61, 2404,  197, ..., 5291,   23,  112],
       [  51, 3228,   18, ...,  227,  102,  578],
       [ 107,  274,  276, ...,   45,  100,    0]]), array([[   0, 2020, 6535, ...,    0,    2,   12],
       [4691,   18,    0, ...,  322, 4691,  100],
       [   0,    0,  925, ...,  498,   48,    0],
       ...,
       [2387,  319,  874, ...,   10,   12, 2807],
       [ 349,  694,   55, ...,  214,  207,  141],
       [  20,  846,   23, ..., 2020, 6535, 4681]]), array([[ 125,  346,  159, ..., 7131,  564, 3301],
       [  36, 1733,  228, ..., 6784, 3819,   27],
       [ 162,    0,  208, ..., 2427,  227,   18],
       ...,
       [1885,   62,   81, ..., 3146,  204,   10],
       [  12, 2103,   14, ..., 8012, 8013, 3326],
       [8014, 8015,   23, ...,  346,  159,   10]]), array([[2377,   23, 7486, ...,   18, 1521,  274],
       [3861,   29,  669, ..., 1075,   53, 8016],
       [3326,   47,   81, ...,    0, 1461,   10],
       ...,
       [ 228, 6581,   81, ..., 4448, 1570, 4448],
       [ 915, 2451,    0, ...,   17, 1191, 5516],
       [7844,  246, 1068, ...,   23, 7486,    0]]), array([[ 228,  369,  174, ...,  219, 3106, 2115],
       [ 201,   23, 1075, ...,  663, 3326,  742],
       [  12,    0,   12, ...,   61, 8020,  100],
       ...,
       [ 118,   23,   42, ...,   37,  141,   10],
       [3764,  552,  242, ..., 2328,  889,  189],
       [3159, 1393,   58, ...,  369,  174,  227]]), array([[  41,  174,    0, ...,  597, 8019, 2137],
       [1839,   24, 1839, ...,   29, 4947,  790],
       [   0,   85, 8005, ...,  911, 3146,  273],
       ...,
       [8039, 4833,   10, ..., 7952, 1683,  136],
       [6845, 3759, 2706, ...,    0,  746, 1346],
       [   0,  100,  291, ...,  174,    0,   51]]), array([[  29,   18, 3202, ...,   61,  359,   26],
       [ 609,  382,   88, ...,  105,   10,  609],
       [   0, 8039,  100, ..., 7980, 2706,   97],
       ...,
       [3555,   55,   81, ...,  582,   12,  557],
       [  29, 2450, 1595, ...,  223,   18, 4608],
       [   0, 2158,  111, ...,   18, 3202,  224]]), array([[  76,    7,  115, ...,  929,   55,  272],
       [3664,   51,    0, ..., 1084,   42,  159],
       [  81,  359, 3925, ...,  100,  842,   12],
       ...,
       [  64,  266,    7, ..., 2335,  101, 2380],
       [   6,    0,  118, ...,  660,   10,   12],
       [ 919,   29,   51, ...,    7,  115,   18]]), array([[8033, 6897,  100, ..., 4654, 3089, 8033],
       [ 100,  272,   12, ..., 2794,   55, 6691],
       [ 270,  266,  100, ...,  124,    0, 3379],
       ...,
       [  10,   42,    0, ...,   29,  526,  491],
       [   0,   26,  125, ...,   81,  238, 1220],
       [1682,    2,   12, ..., 6897,  100,  534]]), array([[ 297,   29,   42, ...,  279,  101, 1439],
       [   4, 5350,  118, ...,  136, 1167,  228],
       [2392,  232,   12, ...,  136,   47,   14],
       ...,
       [  50, 2020, 1239, ...,  100,   14, 3595],
       [ 223,   18, 1525, ...,   53,  125,   29],
       [ 101,  274,  381, ...,   29,   42,  735]]), array([[  14,  917,   55, ..., 8039,   26, 1441],
       [   2,   12,  178, ..., 5767,   23,   18],
       [4449,    2, 4222, ...,   23,  349,  273],
       ...,
       [ 157,  534,  100, ...,  100,  863,  162],
       [ 105,    0,   10, ...,   69, 3100,  100],
       [ 534, 1480, 2334, ...,  917,   55,   53]]), array([[  55,   81,  268, ...,    0,   10,  136],
       [ 920,  224,  501, ..., 6678, 1908,  111],
       [2176,   55,   39, ...,  174,  241,   29],
       ...,
       [ 174, 1469,   12, ..., 1350,   61,    0],
       [ 268,  954, 1930, ...,  417,  115,   12],
       [6651, 1590,  118, ...,   81,  268,    0]]), array([[  55,   81, 8046, ..., 2208,   55,   81],
       [  12, 7704,   29, ..., 5292,    2,   12],
       [6445, 2398, 8045, ..., 6779,  274,  183],
       ...,
       [  89,  233,   21, ...,  890,   26,  535],
       [  88, 4554,  228, ...,   47,  710,    7],
       [2568, 8049,  274, ...,   81, 8046,  508]]), array([[ 118,   29,   26, ...,   12,    0, 4790],
       [ 125,   49,    0, ...,   50, 1679,    0],
       [  81,  107,  639, ...,   10,   20,  624],
       ...,
       [1167,   81,  811, ..., 8049,  350,  227],
       [3843, 4362, 1641, ...,   12,  125,  224],
       [2945,  174,   50, ...,   29,   26,  125]]), array([[ 228,   19,  100, ..., 2529,   88,  892],
       [  50,  270,   38, ...,  123,  224,   33],
       [ 224, 3270,   29, ...,  227,  240,  118],
       ...,
       [ 915, 2882,  956, ...,  535,    0,   21],
       [ 663,    0,  102, ...,   18, 1040,  183],
       [ 735,   37,   20, ...,   19,  100,  266]]), array([[3082,   81,  136, ..., 2629, 8049,   47],
       [ 535,  227,  232, ..., 8058,   23, 7959],
       [  47,  535,  227, ...,  576,   81,  136],
       ...,
       [ 107,  735,  335, ...,  100,  534,   27],
       [   0,  270,  100, ...,  227,   14,  120],
       [5072,   55,  227, ...,   81,  136,   26]]), array([[ 382,   24,  225, ...,   23,  228,  896],
       [  12, 4334,  247, ...,  343,   21,    4],
       [  53, 2529,   48, ...,   18,  625,   29],
       ...,
       [4189, 2472,   55, ..., 2282,  509,  253],
       [ 815, 4350,   12, ...,   12, 1541,    0],
       [   0,    0,    0, ...,   24,  225,  241]]), array([[ 873,  343,   51, ...,   51, 4781,   18],
       [7807,   29, 6983, ...,   58,   51, 1391],
       [  23,   14,  125, ..., 5013, 2681, 8059],
       ...,
       [1254,    2,    0, ..., 5083,  178,   50],
       [ 939,    0,   39, ...,   88, 6527,  673],
       [ 337,  227,   37, ...,  343,   51,    0]]), array([[8061,    0,   23, ...,  691,   42,  885],
       [  12,  482,   23, ..., 1984,    2,  911],
       [1648,    0,  474, ...,   24,    0,   85],
       ...,
       [5047,  232,  907, ..., 4567,    0, 1112],
       [  10,   38,  124, ...,  102,   20,    0],
       [ 709,  227,  941, ...,    0,   23, 1157]]), array([[ 239,   10,   12, ...,   81,  228,   14],
       [8066,   26,  224, ...,  908, 5336, 6842],
       [  23,  208,   67, ...,  960,   12,  138],
       ...,
       [   0,  864,  101, ...,   50,   10, 2403],
       [ 125,  209,  907, ..., 6983, 2380, 6657],
       [2403,  911,    0, ...,   10,   12, 4041]]), array([[2408,  233,   12, ..., 1718,    0,    2],
       [  16,   27, 6254, ...,  151, 6388,   39],
       [  12, 4803, 1371, ...,   23, 7563,  344],
       ...,
       [  23,   61,   49, ...,  174, 2480,  238],
       [2272, 1199,  911, ...,   35,  993,  279],
       [  12, 7209,   29, ...,  233,   12, 5155]]), array([[ 842, 1093, 3202, ...,  233,   51,  729],
       [  51, 2713,    0, ...,  148,  409, 3116],
       [1685, 6348, 1162, ...,   88, 7125,  120],
       ...,
       [  88,   12, 2357, ...,   50,  247, 1683],
       [ 232, 2398, 2404, ...,   47,  869,   81],
       [  55,   10,   20, ..., 1093, 3202,    0]]), array([[  36,  107,  967, ...,   12,   13,  142],
       [ 141,    0,   10, ..., 3839,    0, 3055],
       [ 233,   21, 1879, ..., 3838,   61,  548],
       ...,
       [ 118,   61, 7668, ...,  224,   23,  270],
       [1658,    6,  290, ...,  232, 2559, 3414],
       [  12, 1879,  232, ...,  107,  967, 1405]]), array([[1075,   23,  997, ...,  101, 1713,   24],
       [ 534,  100, 1685, ..., 1000,   12, 3064],
       [ 100,  240,  100, ...,  540,  100,   14],
       ...,
       [ 118,   51,    0, ..., 2145,  125, 3078],
       [ 343,  101,  482, ...,   55,  272, 5492],
       [5105,   10,  322, ...,   23,  997,   29]]), array([[6208, 3954, 2593, ...,   21, 3075, 1685],
       [  33,    7,  497, ...,   61,  423,   30],
       [   0,   26, 1191, ...,  101, 2593,  478],
       ...,
       [  50,  869,   29, ...,   29, 6348, 7011],
       [  18,  491,   26, ...,   29, 7495,   23],
       [8091, 7495,  274, ..., 3954, 2593, 3596]]), array([[  88,  118,   20, ...,   27,    0, 3308],
       [1765, 7596, 1632, ...,  197,  227, 6313],
       [  85,  223, 7495, ...,   81,  112,   12],
       ...,
       [  10, 7428,   81, ..., 7390,  238, 1921],
       [   2,   21,  568, ...,  174,  815,  755],
       [ 227,    2,   12, ...,  118,   20,  227]]), array([[3697,   12, 2377, ...,  100,  208, 1525],
       [ 107,    7,   74, ...,   12, 4785, 1685],
       [7153, 1583, 1765, ...,  102,  557,   10],
       ...,
       [ 272,   55,   61, ..., 3596,   48,    6],
       [ 214,  120,  100, ...,   88, 5766,   39],
       [  51, 4854,    0, ...,   12, 2377,  102]]), array([[6175,   10,  920, ..., 2413, 4261, 8100],
       [ 224,   53,   51, ..., 8100,   23,  107],
       [  55,  142,  673, ...,  139, 5330,   20],
       ...,
       [ 115,   27, 2255, ..., 1594,  535, 3625],
       [  23, 3670,  141, ..., 1685,   29, 2376],
       [ 735,  885,  555, ...,   10,  920,   47]]), array([[7851,    0,   39, ...,  697,   29, 1748],
       [   6,   88,   10, ...,   75,  322,  900],
       [2155, 6348, 1981, ...,   12,  406,  228],
       ...,
       [3105, 2753,   29, ..., 1558,  106, 2450],
       [1765, 7596,  201, ...,    0,   10,  526],
       [ 406,   29,  509, ...,    0,   39,   42]]), array([[ 925,   26,   12, ...,  101, 2392,  124],
       [  21, 1358,   61, ...,  115,   42, 1765],
       [  29, 4767, 1765, ..., 6348,   26,  148],
       ...,
       [1629,    2,  284, ...,   29,  141,   26],
       [ 100,  263,    0, ..., 7036,   67,  322],
       [4450, 1685, 6348, ...,   26,   12, 1852]]), array([[  12,    0,    0, ..., 2568,   10, 4869],
       [ 124,  118, 2451, ...,  474, 5704,    2],
       [  18, 7305,    0, ..., 1745,  343,   12],
       ...,
       [3483,  101, 2392, ...,   47,   81,  228],
       [  14, 2450, 8115, ..., 8115, 2975,   88],
       [2974,  546,  815, ...,    0,    0, 1038]]), array([[ 147,  387,    0, ...,  102,  159, 2488],
       [ 100, 1492,   55, ...,  101, 2380,   23],
       [ 227,  142,  204, ...,   27, 1187,  100],
       ...,
       [3394,    0,  100, ...,   24,  350,  465],
       [   0,  540,  100, ...,  478,   17,   18],
       [ 639, 5101,    2, ...,  387,    0, 7866]]), array([[  69,  236,    0, ...,  261,    9,   10],
       [ 273, 2447, 2354, ...,   12,  518, 1580],
       [  51, 5803,  421, ..., 1558,   29, 5085],
       ...,
       [8117,  559,    0, ...,   39,   23,   39],
       [1683,  552,   10, ...,   81,   18, 3309],
       [  10,   12, 7599, ...,  236,    0,  100]]), array([[8106, 3242,  223, ...,  100,  272,   48],
       [ 784,    2,  228, ...,  542,  510,   79],
       [ 150, 8106,  226, ..., 5802, 4906,   69],
       ...,
       [ 238,  938,  546, ...,   55,  160,  227],
       [1616,   85,   12, ...,  111,    6,   55],
       [ 197,   88,  228, ..., 3242,  223,  224]]), array([[2460,  118, 5531, ..., 1359,   10,  356],
       [ 118,  694, 1412, ...,   10,   11, 2447],
       [2318, 2369,  100, ..., 3605,   45, 6797],
       ...,
       [ 937, 3952,   29, ...,  249, 1954,  812],
       [3548,  232,   81, ...,  207, 2416,   23],
       [ 542, 2176,  100, ...,  118, 5531,  359]]), array([[  62,   16,  811, ...,  112,   16, 1885],
       [ 214,  621, 1892, ...,   47, 1014,  101],
       [ 491,   33,  101, ...,   21,  478,  123],
       ...,
       [  23,  136, 2577, ...,    0,   23,   12],
       [   0,  251,   10, ..., 5950,   23,  929],
       [  18, 4906, 2472, ...,   16,  811,  105]]), array([[  53,  540,  100, ...,  228,  508, 8134],
       [  10, 5055,   42, ...,  227,  142,   29],
       [5857,    0, 5908, ...,   17,   18, 1291],
       ...,
       [1419, 3346,   23, ...,  227,  535, 1521],
       [1276, 2404,   10, ..., 1286,   85, 8135],
       [8121,   23, 8132, ...,  540,  100, 7549]]), array([[ 101,  264,   36, ...,  380,   29,  101],
       [5027,   23,    2, ...,   33,   12, 1213],
       [ 505,   81,   36, ...,  376,  308, 1068],
       ...,
       [  20, 8133,  150, ..., 2404,  214,   20],
       [4071,   12,    0, ...,  815,   88,   20],
       [ 791,  500,  100, ...,  264,   36,   23]]), array([[ 332,  101,  972, ..., 3112,  227,   26],
       [1064,   67,   42, ...,   29,   47,   81],
       [ 509,  369,  159, ..., 7980, 8137,  464],
       ...,
       [ 227,   23,   53, ..., 4071,   23,    0],
       [ 123,  704,   10, ...,    0,   62,  101],
       [2392,   23,  159, ...,  101,  972, 8134]]), array([[ 102,  272,  101, ..., 8134,   81,    0],
       [  24,   23, 3611, ...,  471,   81,  136],
       [  26,  112,   61, ...,   17,   10,   20],
       ...,
       [   0, 1093, 3468, ..., 1240,   29, 6041],
       [   2,   16,  100, ...,  272,    0,   10],
       [ 273,  118,   47, ...,  272,  101, 2392]]), array([[ 174,   88,  738, ...,  105,   48,    0],
       [ 101,  750,    0, ...,  540,    7, 2880],
       [ 232,    7,  128, ...,  101, 2392,   23],
       ...,
       [ 510,   29, 3327, ...,    0, 3916,  111],
       [4522, 2313,    0, ..., 2640, 8145,  100],
       [ 148,   85,   69, ...,   88,  738,    7]]), array([[  10, 2840, 3852, ...,   47,   18, 2069],
       [ 842,  465, 1326, ...,  208,    0, 3286],
       [1490,  274,  101, ..., 2326,  100,  356],
       ...,
       [ 208,   27, 2692, ...,  319,  232,  100],
       [  14,  555,  708, ...,   53,  123,    0],
       [   0,   12,    0, ..., 2840, 3852,    0]]), array([[3286,  100,  266, ...,  729, 3245, 8145],
       [5531,  236, 1839, ..., 2196,  387,   10],
       [ 118,   23,  815, ...,  214,  535,   29],
       ...,
       [  10, 2677,  266, ..., 5250,    0, 4647],
       [  23,    0,    0, ...,  142, 1048,    0],
       [2673, 8147, 2203, ...,  100,  266, 3022]]), array([[ 292, 8143, 2404, ..., 2203,   10, 8147],
       [ 387, 2673, 1267, ...,  491, 3120,   53],
       [ 141,   12,  627, ...,   81,  100,  534],
       ...,
       [ 815, 5218,   85, ...,  136,   10,  638],
       [ 223,   47,   18, ...,   10,  322, 4876],
       [  36,  148,  100, ..., 8143, 2404,  101]]), array([[ 228,  369, 2224, ...,    0,  100, 6245],
       [  67,  588,  230, ...,  273,  227,  401],
       [ 815,  129,   10, ..., 1683,    2, 6053],
       ...,
       [ 555,    2,  349, ...,  482,    2, 3519],
       [5699,  509, 3513, ...,  100,  316,  911],
       [4448,   37, 1939, ...,  369, 2224,  232]]), array([[  94,   20,   61, ...,  465,   81,   88],
       [  61,  359,   31, ..., 8155, 4243,  295],
       [   0,  498, 5695, ...,  100,  534,   18],
       ...,
       [  16,   47,   30, ..., 2624,   23, 1941],
       [8154,   74,    7, ...,   12, 2178,    4],
       [  29, 3764,  142, ...,   20,   61, 7780]]), array([[  55,   81,   50, ..., 1124,   53,  232],
       [  55, 3673,   23, ...,  635,   55, 7888],
       [  10, 3975,   21, ...,   16, 1810,   55],
       ...,
       [  23, 1405,   29, ..., 2354, 2355, 3756],
       [  18, 1254,    2, ..., 1508, 4192,   29],
       [3756, 1974,    0, ...,   81,   50,    0]]), array([[ 265,   10,    0, ..., 2395, 2396,   23],
       [3773, 3756,   23, ..., 3015,   29,  101],
       [1163,   85, 3055, ...,  729, 4434,    0],
       ...,
       [ 142, 4179, 3776, ...,   88,  118, 3776],
       [ 279,  118,   61, ...,  118, 8141,  842],
       [  88,  162,    0, ...,   10,    0,  265]]), array([[5952,  465,    0, ..., 1522,   14,  907],
       [2687,   85,  141, ...,   12, 7590,  535],
       [7483, 3181,  723, ...,  136,  196,   18],
       ...,
       [  20, 3163,  101, ...,   79,   29,  885],
       [  50,  535,   29, ..., 4149,  101,  478],
       [ 102,  208, 3837, ...,  465,    0,   39]]), array([[  53,  911, 2401, ...,  815,  976,  295],
       [ 885, 1222,  118, ..., 5628,  708, 8169],
       [ 478, 2398, 3837, ...,   53,  101, 2963],
       ...,
       [ 227,  232, 7302, ...,  100,  272,  636],
       [  10,  920, 2278, ..., 2854,   18, 5720],
       [4303,   12,  519, ...,  911, 2401, 1023]]), array([[  18,    0, 1075, ...,  609, 2398,   12],
       [2386,   85,   18, ...,  207,   88,   20],
       [4204,  500,   12, ...,  850,    7,   88],
       ...,
       [2028, 5699, 4815, ..., 4093,  709,  227],
       [1810, 4093,  709, ..., 3133, 5699, 1162],
       [ 465,  214,  535, ..., 1075,  502,  885]]), array([[5699, 3837,  219, ...,   18, 3146,   29],
       [   0,   39,  413, ..., 2287,    0,  174],
       [3672, 5807,   67, ...,    0, 1648,   29],
       ...,
       [1253, 2332,  197, ...,    0, 1862,    2],
       [ 317,   12,    0, ..., 1245, 3210,    0],
       [ 112,  141,    0, ...,  219,   12, 2617]]), array([[1056,  932, 3903, ...,  204,   12, 1599],
       [1604,  559,  872, ...,   12,    0,    0],
       [   0,   12, 5165, ...,  911,  384, 1358],
       ...,
       [  12, 1733,    6, ...,    0,   55,   81],
       [ 987,  159,    0, ...,   38, 2525,   24],
       [  53,  588,   18, ..., 3903,  540,   78]]), array([[7343,    0,  162, ...,    0,  334, 1425],
       [  29,   18, 2945, ...,    0,  322,  610],
       [1720, 1635,  607, ..., 4066,  118,   23],
       ...,
       [  18,  663,  125, ..., 4555, 7622,   23],
       [8195, 8195,  349, ..., 1984,   23, 5419],
       [   0, 5333, 2962, ...,  162,    2,   42]]), array([[ 349,   18,  308, ...,  227,  951,    0],
       [ 101, 7984,    2, ...,  150,  223,   20],
       [3163,  610,  288, ..., 1382, 2334,  895],
       ...,
       [5191, 1227,    7, ...,  101, 2272,   53],
       [  39,   12, 3181, ..., 2117,   23,  262],
       [ 227, 2380,  815, ...,  308,    0,   29]]), array([[1180,   21,  660, ...,   26, 3406,   10],
       [ 829,  335,  223, ...,  365,   23,    3],
       [6983, 2451, 2354, ...,   26,   12,    0],
       ...,
       [7645, 8192,  148, ..., 2528,    0,  216],
       [ 301, 1653,  485, ..., 2427,  227,    2],
       [  39,  141,   12, ...,  660, 2450,  240]]), array([[3627, 3107,  907, ...,  100,  266,   12],
       [3285,  272, 2061, ...,  359,   10,  510],
       [3196,    0, 5703, ...,  261,   50,   12],
       ...,
       [2568,   81,   55, ...,   47,  102,  907],
       [ 174,  301,  142, ...,    2,  731, 4494],
       [6671,    6,   12, ...,  907,  223,   67]]), array([[  62,   81,  224, ...,   50,  141,  100],
       [ 272,    0,   16, ...,    0,   10,   42],
       [   0,   55,   81, ...,  101,  261, 1411],
       ...,
       [  29,  911,  705, ..., 2416,   18, 1191],
       [  29,   26, 1811, ...,  115,   18, 1807],
       [ 337,  224,  102, ...,  224,  331, 2404]]), array([[ 118,   47, 2664, ...,   42,   88, 2475],
       [ 761,   59,  129, ..., 2364, 2353, 2398],
       [2377, 8200, 8206, ...,    0,   23,    0],
       ...,
       [3286,  100,  219, ...,   29,  136, 5126],
       [3286, 2759, 2404, ..., 2752, 2115,  735],
       [3800,  624,    2, ..., 2664,   26, 2404]]), array([[ 344,  153, 4750, ...,    2,  136,   52],
       [ 100,  102,  769, ...,   29,   12, 5126],
       [ 465,  102,  227, ...,  761,    0, 3514],
       ...,
       [ 179,   51, 1312, ..., 1187,   10, 3071],
       [ 247,   85, 3509, ...,  136,  877,  159],
       [ 161,  118,  100, ..., 4750,   12,  278]]), array([[5126,   10, 1164, ..., 4071,   10,    0],
       [ 509, 4591, 7805, ..., 5882,   23, 3075],
       [   0,  100,  316, ...,   26,    0,   92],
       ...,
       [8218,   29,  141, ..., 8220,  107,   29],
       [  18, 2681,    0, ...,  137,  779,    2],
       [ 967, 8218,  540, ..., 1164,  118,  328]]), array([[  29,   78,  274, ..., 1145,  149, 8218],
       [1162,  224,   29, ...,   88,  638,  322],
       [  62, 8220,   97, ...,   23, 3163,  100],
       ...,
       [   0, 2145,   85, ...,  100,  906,  465],
       [  81,   27,  759, ...,  228,  421,  842],
       [ 136,   12, 1857, ...,  274,  100,  738]]), array([[  33,  100,  842, ..., 2288,   88,  526],
       [2288,    7,   76, ...,  427,   10,  921],
       [  18, 2109,   23, ...,  721, 7647,  204],
       ...,
       [ 465, 8222,  272, ..., 2719,   23,  542],
       [  55,   81,  162, ..., 1162,   26, 1541],
       [ 691,   12,  890, ...,  842, 1583,  227]]), array([[  74,    7,  224, ...,   62, 2555, 2447],
       [8216,  349,   14, ...,  349, 6671,   81],
       [8235,  124,   42, ..., 1063,   23,   49],
       ...,
       [ 115,   55, 3630, ...,   12,  398, 2265],
       [  81,  692,  317, ..., 2290,   29,   62],
       [  23, 1531, 8218, ...,  224,   58,    0]]), array([[7647,   26,  227, ...,   12, 2377, 2398],
       [2377, 2377,  540, ...,   53, 7682, 8118],
       [3179,  295,  885, ..., 5947,   42, 3146],
       ...,
       [2911,  274, 3357, ..., 3937,   23,  907],
       [   0, 1488,  111, ...,   53,  227,  270],
       [  20,    0,   51, ...,  227,  535,   36]]), array([[6934,   10,  223, ...,    0,   26,   51],
       [   0,    0,   85, ..., 1438, 2586, 8242],
       [  61,  907,  207, ...,    0, 2435,  387],
       ...,
       [  10,   12,    0, ...,  815,   20,   23],
       [ 465,  815,   20, ...,   12,    0,    0],
       [   2,  911,    0, ...,  223,   23,    0]]), array([[ 975,   23, 1293, ...,   26,  262,   88],
       [1807, 3269,   10, ...,    0,  535,  214],
       [1315, 3269, 4760, ...,   51,  670,  154],
       ...,
       [ 141,  136, 6366, ..., 2334,  224,  439],
       [3089, 8005,  349, ...,  387,  907,  535],
       [ 141,  154,    0, ..., 1293,  377,   47]]), array([[  23,  908,  274, ..., 4527,   23,  100],
       [1137, 1364,   85, ...,    6,   55,   50],
       [ 241,  100,  990, ...,  253,   23,  101],
       ...,
       [  51, 1803, 2495, ...,   30,   36,    0],
       [ 223, 8243,  270, ...,   12, 2681,  224],
       [2418,   29,   50, ...,  274, 7667,    6]]), array([[ 204,   18, 1157, ...,   12, 2483, 2327],
       [1840, 1475, 1808, ...,  735,  317,   11],
       [ 417, 8243,   33, ...,  610,  107,  526],
       ...,
       [  23, 3131,    0, ...,  272, 1523, 1731],
       [ 937,   10,  118, ...,  638, 2475,  761],
       [  13,  417, 6665, ..., 1157, 2532,   88]]), array([[ 272,    0,    0, ...,   12, 6220,    0],
       [  12,    0,  609, ...,   88,  432,   42],
       [   0,  100,  179, ...,  136, 4602, 3596],
       ...,
       [ 228, 2617, 1023, ...,   18, 1676,  107],
       [   0, 4018, 2505, ..., 8250,  815, 3208],
       [ 101, 2380, 8248, ...,    0,    0, 3049]]), array([[  85,   68,   14, ...,   12, 2373,   10],
       [3208,  111,   27, ...,  522,  101, 2380],
       [8196,   12, 1685, ...,  118,   10,  305],
       ...,
       [  55,  873, 8256, ..., 2664,  761, 6435],
       [2787,   10, 3436, ...,  160,  735,    0],
       [  61,    0,  890, ...,   14,    0,   26]]), array([[  20,   12,  245, ...,  359, 5559,  150],
       [ 967,   20,    0, ...,   23, 1869, 1422],
       [8248,  296,   12, ...,   12, 6274, 1218],
       ...,
       [3285,   23,  227, ...,   82,    0,    0],
       [ 201,   50,   26, ..., 3737, 3404,  227],
       [1163,   10,  118, ...,  245, 2451, 2354]]), array([[1297,  201,   47, ...,   23,    0,   23],
       [  12,    0, 1293, ..., 4601,  954,  241],
       [  33,    0,  107, ...,  761, 1112,  111],
       ...,
       [2255, 8260,   23, ...,  272, 4907,  118],
       [ 349, 2709,  593, ...,  115,   48, 6099],
       [ 542,    0,    0, ...,   47,  148,  224]]), array([[1490, 3819, 3235, ...,  480, 6675,  101],
       [   0, 1346,   10, ...,  100,  179,   12],
       [1238,   61,    0, ..., 2404, 8260,   97],
       ...,
       [8259,  100, 2836, ..., 8259, 1004,   48],
       [  55,   17,  228, ...,   23,    0,  266],
       [  55,  815, 2144, ..., 3235,   12, 3177]]), array([[  76,   88,   20, ...,  526,  346,  114],
       [  18, 1605, 2629, ...,   12, 1184,   18],
       [1254,    2,    0, ...,  227,  272,    0],
       ...,
       [   0,    0, 6818, ...,   29,   62,    0],
       [  53,  870,  105, ...,    0, 2411, 2599],
       [ 159,    0,  100, ...,   20, 2019,   50]]), array([[1805, 2292,   23, ...,   18, 4918,   23],
       [3522,  590,  287, ...,  261,  772,  100],
       [1307, 1295,   29, ..., 2411, 2599,  159],
       ...,
       [   0, 2725,   81, ..., 6204,    0,   12],
       [6722,  101,  557, ...,   29, 2020,  643],
       [  23,  997,   29, ...,   23,   31,   42]]), array([[ 568,   67,   42, ...,   39,  322,    0],
       [6479,  638, 5557, ...,   14,  284,  123],
       [  39,  223, 1621, ...,  554,  107,  284],
       ...,
       [  16,    0,    7, ...,    0,  178,    0],
       [  29,  830,  183, ...,   29,  101,  460],
       [5249,   58,   21, ...,   42, 2107,   51]]), array([[ 900,  663,    0, ...,   21,  353,   23],
       [   0,   85,  798, ...,    0,   29,    0],
       [1399,   23, 1749, ...,    2,  212, 1313],
       ...,
       [ 124,    0, 1310, ...,  449,   51, 2659],
       [  85, 1063, 7458, ..., 2411, 2599,  159],
       [   0, 3939,   14, ...,    0,  618,   81]]), array([[ 437,    2, 8027, ..., 1234, 1996,   29],
       [1576,    0, 1027, ..., 4199, 1945,   42],
       [   0,    0,    0, ..., 1811, 4815,   85],
       ...,
       [ 465,  224,    0, ...,    0, 2816,  224],
       [ 568,  343,   51, ..., 5325,   85,   42],
       [3228,    0,   17, ..., 8027, 1183,    0]]), array([[ 842,    0,   51, ...,   10,    0,  295],
       [   0,    0,   85, ...,    0,   26,   81],
       [7032,  111, 4080, ...,  667,   10, 4048],
       ...,
       [  29,   18, 4686, ..., 4985,   12, 1132],
       [ 821,    0,    0, ...,    0,   10,   12],
       [1530, 2587, 3455, ...,   51, 2207,  479]]), array([[  42, 1239,   23, ..., 1207, 2503,   67],
       [  12, 1232, 1464, ...,   21,  483, 2204],
       [7592,   26, 2854, ...,  112,  223,   94],
       ...,
       [ 709, 4670, 8283, ..., 1132,   18, 5172],
       [   0,    0, 5024, ...,  911,    0,  125],
       [2794,   12, 1334, ...,   23, 5007,   55]])]
100
WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.compat.v1.nn.embedding_lookup_1), but are not present in its tracked objects:   &lt;tf.Variable &#39;embedding_mat:0&#39; shape=(8286, 128) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.linalg.matmul_1), but are not present in its tracked objects:   &lt;tf.Variable &#39;W:0&#39; shape=(128, 8286) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.__operators__.add_1), but are not present in its tracked objects:   &lt;tf.Variable &#39;b:0&#39; shape=(8286,) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
[array([[   0,    1,    2, ...,   39,   40,   41],
       [  42,   43,   44, ...,   66,   26,   67],
       [  68,   69,   70, ...,    7,    0,   88],
       ...,
       [  62,  932,  154, ...,  219,  895,  223],
       [  10,   51,  236, ..., 1129,    2,   12],
       [   0,   61,   81, ...,    1,    2,    3]]), array([[   0,   81,  228, ...,   29,  119, 1207],
       [  23,  227,    2, ...,  557,   50,  227],
       [  17,   79,   79, ...,   18, 1235,  107],
       ...,
       [ 688,   14,   20, ...,    7,  299,  510],
       [  12,  566,   29, ...,  536,  818,   47],
       [ 398,    0,    0, ...,   81,  228,  943]]), array([[  53,  431,   23, ...,    2,  141,   51],
       [1788,   58,  555, ...,  269,  295,  337],
       [ 214, 1644,  279, ...,   85,  735,  197],
       ...,
       [ 124,  322, 2209, ...,  121,   41, 1085],
       [ 376,   21,  517, ...,   21, 2217, 2218],
       [2219,  279,  545, ...,  431,   23,   51]]), array([[ 517, 1104, 1279, ...,  409,    7,  121],
       [ 369,  327,   23, ...,   62,   16,  107],
       [   7,   80,  151, ...,    0, 2235,   10],
       ...,
       [ 496, 1407,   50, ...,  270,   27, 1742],
       [ 596,    0,   20, ...,  228, 2389, 2674],
       [2385, 1411,  100, ..., 1104, 1279,   23]]), array([[1521, 1508,  162, ..., 1014,  842,   27],
       [1915,   26,  263, ...,   12,   82, 1925],
       [2333,   29,  870, ...,  335,  125,  689],
       ...,
       [  29, 1921,  863, ...,   12, 1685,  247],
       [  61,    0,   81, ...,  343,  136, 3019],
       [ 559,    0,  567, ..., 1508,  162, 1356]]), array([[ 578, 2524,   18, ...,  344,  470,  101],
       [ 479, 2379,  535, ...,  100,  207,  273],
       [  16, 2625,  100, ...,   12,    0,   12],
       ...,
       [ 227, 1972,  223, ...,  708,   51, 3145],
       [ 125,  227,  578, ...,  224,   81,    0],
       [  12,    0,   29, ..., 2524,   18, 1541]]), array([[   2,   51, 1391, ...,    0,   23,   10],
       [  47, 3327,  136, ...,  526,  478, 2378],
       [ 540,  349, 2524, ..., 2816,   23,   10],
       ...,
       [   0,  224,   14, ...,  224, 2381, 3161],
       [  18, 1470, 2664, ...,   67,  224, 2794],
       [  12,    0,  498, ...,   51, 1391,    0]]), array([[ 141, 3557, 1132, ..., 1096,  227,    0],
       [  61,    0, 3558, ...,  498, 2479,   26],
       [ 102,  227,   23, ...,  101, 2380, 2379],
       ...,
       [1566,   51,   19, ...,  100,  179, 2388],
       [  27,  101,  274, ..., 1030,  228, 3134],
       [ 136,   55, 1096, ..., 3557, 1132,   10]]), array([[1015,  474,  474, ...,   61,  100, 1590],
       [  16, 1308,   67, ...,    7,   21, 1733],
       [  23,  815,  769, ..., 1925,  186,  162],
       ...,
       [3967,  907,  535, ...,   23,   12, 1056],
       [  81,  509,  101, ...,   53,  223, 3929],
       [3338,   23,    0, ...,  474,  474, 1015]]), array([[1872,  150, 3974, ...,  136,   81,   82],
       [1984,   26,  100, ...,   51,    0,  408],
       [  12,  239,  909, ...,  100,  738,   88],
       ...,
       [1162,  136,   55, ...,    2, 3000,    0],
       [4162,  337,   41, ...,  325,   12,    0],
       [ 111,   12, 2568, ...,  150, 3974, 1703]]), array([[1893,  349,   29, ...,  100,  554,  118],
       [2670,   20,    2, ...,   21, 1006,  100],
       [ 174, 2977,   16, ...,   81, 1899,   61],
       ...,
       [ 941, 1075,   97, ..., 1645,   23, 3950],
       [4120,   23, 2465, ..., 1521,   71, 3781],
       [  36,  204, 3071, ...,  349,   29,  228]]), array([[3769,    0,  100, ...,   39,  761, 4334],
       [  23,    7,  165, ...,   12, 4336, 2664],
       [ 761,   13,  582, ..., 1000,   23, 1523],
       ...,
       [ 174,    0, 3788, ..., 1091,    0,   58],
       [ 112,  122,   23, ...,  201,   10,   12],
       [   0, 4539, 2241, ...,    0,  100, 2809]]), array([[ 946,  101, 1812, ...,  117,   12, 3822],
       [ 102,   11,   10, ...,   53,   51, 2716],
       [  12, 4544,   29, ...,   12, 1438,  100],
       ...,
       [ 226, 2443, 2386, ...,   12, 3510,   29],
       [   0,   53, 2416, ..., 1419,   50,  100],
       [ 262,   12, 2251, ...,  101, 1812,   53]]), array([[   0, 3781,   36, ..., 4699,  136, 1872],
       [4291, 4291,  274, ...,  534, 1068,   23],
       [ 524,  101,  239, ...,  219,   61, 2105],
       ...,
       [4764,  100,  382, ...,  105, 1364, 4786],
       [  97,   18,  274, ...,  265, 4764,   12],
       [2377,  101,  264, ..., 3781,   36, 2427]]), array([[ 101, 1023, 4786, ...,  227,   85,  228],
       [4842,  662,   23, ...,  894,  246,   85],
       [ 118,    6,   85, ...,  174,  141, 2426],
       ...,
       [  90,   12, 3306, ...,   23,    0,    0],
       [  85, 5014,   67, ...,    0,   23, 2995],
       [2053, 4975,  446, ..., 1023, 4786, 1162]]), array([[2377, 4730, 1276, ..., 1021,    7,   74],
       [  88,  818,  910, ...,   12, 2732,    0],
       [  21, 4590,   81, ...,  272,    0, 3425],
       ...,
       [  29,  223, 4928, ...,   29,  790,    0],
       [ 151,    2,   62, ...,  266,   88,    0],
       [1284,   12, 1466, ..., 4730, 1276,  251]]), array([[ 148,   88,   20, ..., 4896, 5155, 5156],
       [ 349,  100,  174, ...,  349,  638,   12],
       [ 864,  509,  421, ...,  534, 1128,  465],
       ...,
       [1064,  100, 1064, ..., 1671,  129,   12],
       [   0,   29,   55, ...,  250,   23,  141],
       [  12, 4265,   26, ...,   88,   20,   21]]), array([[ 232, 1683,  101, ...,    0,   29,   18],
       [3202, 4741, 2037, ..., 1093, 2513,    0],
       [  50,   12,  621, ..., 1973,    0, 1972],
       ...,
       [  88, 1346,  227, ..., 2243,  337,  272],
       [ 227,   31,   12, ...,  118,   23, 5378],
       [  88,  337,   81, ..., 1683,  101, 4764]]), array([[ 102,   20, 3612, ...,  105,  136,  337],
       [  81,   12,  424, ...,  842,   50,   10],
       [3316,  227,  124, ...,   29,    0, 1340],
       ...,
       [ 967,  204,  106, ..., 1321,  227,  174],
       [5465,   67,  509, ...,    0, 3385,  233],
       [ 101,  644, 1475, ...,   20, 3612, 2416]]), array([[  19,   29,  509, ...,  189,   12,  883],
       [1328,   21,  823, ...,   61, 5296,   29],
       [5295,   88, 2127, ...,  274,   10, 1032],
       ...,
       [1752, 2398, 1847, ...,   47,   14,  227],
       [5154,  118,    7, ..., 2498, 3249,   30],
       [   7,  922,   10, ...,   29,  509,  279]]), array([[5554,   23, 1593, ...,  328,   67,   16],
       [4087,  341,  710, ..., 5296,   29, 5293],
       [  23, 5299,   29, ...,  228, 1733,  263],
       ...,
       [ 729,   82, 2526, ...,  105,  219,    0],
       [   2,  228, 4029, ...,  295,  107,  701],
       [ 498, 5643, 1133, ...,   23, 1593,  540]]), array([[  12,  639,    6, ...,  101,  506,  100],
       [  14,    0,    0, ..., 1132,   17,   18],
       [5416,   55,  197, ..., 5008,    0, 5643],
       ...,
       [ 337, 1869,  102, ...,  335, 2398, 5699],
       [5640,   85,   51, ...,   47,  100,  219],
       [   0,  107,  227, ...,  639,    6,   12]]), array([[  10, 1180,  228, ...,  214,    0,  578],
       [ 212,  744, 5631, ...,  228, 1132, 2701],
       [5638,  100, 1590, ...,   23, 5761, 5631],
       ...,
       [ 100,  148,  214, ...,   12, 3238,   61],
       [ 232, 1683,   18, ...,   96, 5637,   27],
       [2404, 2465, 1093, ..., 1180,  228,    0]]), array([[3085, 5637, 3085, ...,  272,    0, 1287],
       [1807, 5643,   50, ..., 2671, 2971, 5643],
       [ 227,  272, 3117, ...,  355,   26,  100],
       ...,
       [  85,  967,  863, ...,    2,   69,    0],
       [  81,  569,   23, ..., 1798,  856,  125],
       [   7,  710,    0, ..., 5637, 3085, 2404]]), array([[5861,  932, 4007, ...,  588,    0,  115],
       [ 874,  898,  111, ...,  101,    0,    0],
       [ 207,    0,  101, ...,  555,  118,  349],
       ...,
       [ 208,  224, 1096, ..., 2404,  224, 2794],
       [ 107,   78,  729, ...,  127,  214,   14],
       [ 328,   29,  322, ...,  932, 4007,  107]]), array([[ 322, 1991, 1315, ...,   29, 3028, 1531],
       [  81,   18, 1521, ...,   29,    0,  498],
       [5954, 1004,   23, ...,   18,  604, 5777],
       ...,
       [  51, 2387,   51, ...,  184,   12, 3697],
       [ 174, 5451,   12, ...,  150,   55,  387],
       [   0,  376,  100, ..., 1991, 1315, 5954]]), array([[ 308,  505,   10, ...,  100,  534,  451],
       [   7,   30, 1207, ...,    0,  387, 2398],
       [  85,  967, 1869, ...,   29,   26, 5657],
       ...,
       [ 227,   53, 3191, ...,  343,    0,   29],
       [   0,   23, 3670, ...,  101, 3894,  842],
       [  88, 3471,  961, ...,  505,   10,    0]]), array([[6124,    2,  604, ..., 2340,  294, 6125],
       [  23,  399,    0, ..., 6088,  932,   61],
       [ 673,    0,  107, ...,  272,  818,  107],
       ...,
       [ 973, 6201, 2398, ..., 2450, 1004,   10],
       [  38,   42, 3080, ...,  227, 1029,  328],
       [ 417,  359,    0, ...,    2,  604,  125]]), array([[ 573,   81,    0, ...,  932, 1385, 6205],
       [6107,   10,  973, ...,  359,  452, 2404],
       [ 227,  555,  118, ...,   86,  227,  125],
       ...,
       [3444,  317, 1607, ...,  101,  482,  554],
       [  88,  588, 4400, ..., 6092, 1162,  100],
       [ 207,   11,   23, ...,   81,    0,  100]]), array([[   0,   29,   12, ..., 6088,   26,  850],
       [1207,  201,  101, ...,  540,   21,    0],
       [ 204,   20,    0, ..., 3208,   23,   10],
       ...,
       [   0, 2278,   27, ...,   53, 6053, 2447],
       [6075,   97,    7, ...,  819,    0,  115],
       [ 223, 4013,   10, ...,   29,   12, 6065]]), array([[ 111,   47,   51, ...,  240,   55, 1071],
       [3245, 6246,   81, ..., 2966,   81, 6081],
       [2127, 2398, 6078, ...,  996,   29,  138],
       ...,
       [ 236,  105,  911, ..., 4562, 4535, 6088],
       [1686, 1033,    7, ...,  272,  596, 1354],
       [  23,  291,  216, ...,   47,   51,  434]]), array([[ 911,    0,   13, ...,    0, 6387,  102],
       [  20,    0, 6388, ...,  885,  111,  216],
       [ 111, 2833,  571, ...,   12, 1541, 2404],
       ...,
       [ 359,  951,  588, ...,  100,  738,  100],
       [ 578,  295,  350, ...,   81, 6451,  465],
       [6451,   18, 3861, ...,    0,   13,   18]]), array([[ 967,  542,  100, ...,   26,  535,   61],
       [   0,  233,  911, ...,   55, 3650, 4148],
       [  23, 1364,   12, ..., 6451,   82,   17],
       ...,
       [  88,  101, 2380, ...,   55, 3877,  227],
       [  63,   12, 5266, ...,    0,  135,  162],
       [4012,  946,   34, ...,  542,  100,  272]]), array([[ 727,  725,  328, ..., 2241,   16, 2451],
       [5622,   23, 6442, ..., 2451, 2354, 2361],
       [  18,  162, 1050, ..., 5622, 5622, 1921],
       ...,
       [  39,   12,    0, ...,   48,  421,   29],
       [ 227,    6,  227, ...,  141,  101, 3836],
       [   0,  141,    0, ...,  725,  328,   23]]), array([[  10,  118,  105, ..., 3966,   29,    0],
       [  23,  270,   10, ..., 3234,   88,    0],
       [6538,   10,  738, ...,  118,   12,    0],
       ...,
       [ 101, 2387,  568, ...,  731,  329,   37],
       [ 960,   51,    4, ...,   12, 1765, 3791],
       [ 223,   23,    0, ...,  118,  105,   18]]), array([[   0, 2018,    2, ...,    0,   23, 5330],
       [3843,   50, 3342, ...,    0,  141, 6492],
       [  14,  214,  273, ...,  911, 3752,  232],
       ...,
       [1475,  224, 3497, ...,  208, 1703,  227],
       [  85,  238, 2692, ..., 6473,  729,   23],
       [ 150,  295,  262, ..., 2018,    2,   12]]), array([[1358,   81,  997, ...,  101, 2380,   85],
       [  12,  778,  585, ..., 1944,   10,   20],
       [   0,   18,    0, ...,   17,   48, 5202],
       ...,
       [  26, 3140,  148, ...,    0, 2397,  498],
       [2386,   81,  159, ..., 2156,    2,   42],
       [ 369,  220, 1315, ...,   81,  997,   29]]), array([[4350,    0,  159, ...,   50,    6,   12],
       [1063,  387,   10, ...,  136,  263,   88],
       [1311,   18, 2389, ...,    0,   23,    0],
       ...,
       [ 174,   16,  274, ...,   12, 2357,   10],
       [ 980, 6442,   12, ..., 1685, 2738, 6471],
       [ 224,   81, 3723, ...,    0,  159,    0]]), array([[1765, 1287,  227, ...,    7, 6754,  933],
       [ 118,   23,  101, ...,  521, 4300,  694],
       [   0,  102,  120, ...,   85, 1088,  387],
       ...,
       [  23, 3105,  107, ...,   10, 5067,   23],
       [ 150,  295, 6500, ...,   12, 3326,   29],
       [  12, 1234,  337, ..., 1287,  227,   26]]), array([[  12,   28,   29, ...,  221,   51, 1358],
       [ 224,  102,   88, ...,  291,  223,   33],
       [ 224,  389, 2326, ...,  540,   10, 6096],
       ...,
       [3596,  863, 4612, ..., 6893, 6786,  107],
       [ 907,  142,    0, ...,  100,  534,   18],
       [   0,   29,    0, ...,   28,   29,   47]]), array([[ 100,  272,  770, ...,   47, 1073,    7],
       [   0,   50,  908, ...,  465,  142,   50],
       [1073,  319,  349, ...,  417, 4381, 6784],
       ...,
       [2450, 6771,   88, ..., 3571,   53,   21],
       [   0,    6,    7, ...,  856,   10, 1749],
       [3109,  111,   20, ...,  272,  770,  908]]), array([[ 169,   23,  204, ...,   53,  907,  102],
       [3133,  272, 1187, ...,  371,   26,    7],
       [  74, 1132,    0, ..., 3349,   29,   21],
       ...,
       [  10,   12, 3113, ...,  240,  223,  204],
       [1425, 6783,   88, ...,   55,  328,   29],
       [ 554,   23,  139, ...,   23,  204,   10]]), array([[ 150,   55,   20, ...,  911, 5526, 1984],
       [   0,  101, 2559, ...,   12,    0, 1720],
       [   0,   23, 1048, ..., 1685,    6,  227],
       ...,
       [1117, 3248,  111, ...,  112,    0,   23],
       [2556,   18,  437, ...,   55, 6821,  100],
       [   0,  223, 1810, ...,   55,   20,  818]]), array([[  55,   85,   12, ...,  214,  266,   53],
       [2849,  224,   26, ..., 6758, 6767, 6768],
       [  23,   64,   85, ...,  911,  898,  550],
       ...,
       [ 209,  100, 1856, ..., 1695,  100,  272],
       [  12, 2528,   53, ..., 1163,   23,  987],
       [ 911,  346,   23, ...,   85,   12,    0]]), array([[1580,    0,   46, ...,   29, 1692, 2380],
       [6788,   12,  205, ..., 3748,  417,    0],
       [ 907,  263,   51, ...,   85,  643, 6962],
       ...,
       [  23,  148,  863, ...,   23,    0,   12],
       [1521,  160, 2522, ...,   81,  509, 3703],
       [5463,    0, 2404, ...,    0,   46,   85]]), array([[2840,  842,    0, ..., 5349,  227, 2056],
       [2056, 2411, 1096, ...,    0,    6,   42],
       [1572, 1749,  552, ..., 1137, 4448,  540],
       ...,
       [ 557, 2404,  100, ...,  398,    0, 1023],
       [ 350,    0,  159, ...,   18,  434,  815],
       [ 392,  246,   18, ...,  842,    0, 2329]]), array([[ 274, 2629, 5468, ..., 2639,  387, 2404],
       [3331,  125, 1073, ...,   29,  227, 2639],
       [2404, 3331, 2404, ...,   18,  491,    0],
       ...,
       [  88,   51, 2710, ...,   23,    7,  710],
       [1823,  223, 6770, ...,  274, 1425,   29],
       [  51,    0, 1430, ..., 2629, 5468, 1411]]), array([[   2,   12, 1229, ...,   85, 4818,  575],
       [ 136, 6770,   23, ...,    0,    0,  111],
       [3170,    0, 7009, ..., 4434, 1685,   82],
       ...,
       [   2, 5292,   33, ..., 4153,   29,    0],
       [  53,  227, 2639, ...,   48,  907,  102],
       [  20, 3086,  349, ...,   12, 1229,   29]]), array([[  88,  129, 5395, ...,   12, 6530, 2670],
       [4557,   18,    0, ..., 2639, 1162,  465],
       [2418,   18, 1685, ..., 6784,  588,   61],
       ...,
       [   2, 2063, 1071, ..., 1346,   64,   17],
       [4676, 2860, 4077, ...,  201,   12, 1394],
       [  12,  639,    0, ...,  129, 5395,   21]]), array([[2815,  387,   10, ...,   10, 2376,  101],
       [2976, 1087,  228, ...,  930,   29,    0],
       [  23, 2009, 1685, ...,  111,  465, 2954],
       ...,
       [   0,   29,    0, ...,  227,  204,    0],
       [1879,   23, 1376, ...,   17,   18, 3737],
       [  26,    6,    0, ...,  387,   10,  123]]), array([[ 778,  494,  127, ...,  907,   14,  305],
       [  29,  136, 2692, ...,  227,  112,  694],
       [1096,  101, 1685, ..., 1315, 5091,   29],
       ...,
       [  10,  228, 2978, ..., 2451, 2354, 2367],
       [  12, 2383, 2366, ...,   12,  160, 1191],
       [  29, 6212, 6123, ...,  494,  127,   12]]), array([[  88,   98,  101, ...,  742,   33,  224],
       [ 391,   55,   12, ...,  442,   23, 1063],
       [1093,  785,    0, ...,   17,   12, 2768],
       ...,
       [  55, 7185,  590, ...,   81, 2838,   12],
       [1112,   29,    0, ...,   23,  266,    0],
       [  81, 2250,    0, ...,   98,  101, 1191]]), array([[  51, 2904,  107, ...,   12,   13,   50],
       [ 907,    2,   55, ...,  214,  142,   88],
       [ 232,   23,  241, ..., 4071,    6,  911],
       ...,
       [5654,  854, 7343, ...,   62,   55,  279],
       [ 228, 1908,   23, ..., 7343,    7,    8],
       [  11, 7189,  227, ..., 2904,  107, 4427]]), array([[5281,  100, 2243, ...,   55,   81,  274],
       [  53,  228,  200, ...,  162, 3975,   10],
       [ 228, 7311,  465, ..., 2243,  227, 1345],
       ...,
       [   0, 2398,  116, ..., 7327,  842, 4588],
       [  12, 5278,  815, ...,   10,    0,   51],
       [ 317,  224, 2409, ...,  100, 2243,  227]]), array([[1250,   23, 5689, ..., 4219,  328, 5342],
       [  23,    0,  946, ..., 1720, 2089,   23],
       [   0,    0,  142, ..., 2202, 1364,   53],
       ...,
       [ 852,   12,  585, ...,  112,  113, 1317],
       [2763,  812, 3022, ..., 1836, 7054,   47],
       [  18, 5820,  224, ...,   23, 5689,    0]]), array([[4974, 6263,   67, ...,   23,   51,    0],
       [  33, 2613,    2, ..., 1745,  125,   29],
       [ 136, 4635,    0, ..., 4670,   42,  639],
       ...,
       [6312, 4758, 2250, ...,  125,  624,    0],
       [ 379,   26,   42, ..., 2713, 4758,   12],
       [   0,   29,  911, ..., 6263,   67,   12]]), array([[ 174, 5807,   67, ...,  615,   29, 2376],
       [2447,   48, 3412, ...,   69,   18, 3681],
       [2465,  997,   29, ...,  150,   27,  642],
       ...,
       [   0,   29,  339, ...,   67,  101, 3184],
       [1191,   23,  815, ...,   18, 1279,   29],
       [ 118,   12,   13, ..., 5807,   67,   12]]), array([[  10,   11, 3331, ...,  535, 1523, 3146],
       [ 954,  849,   12, ...,   23, 1200,  149],
       [ 232,   67,  101, ...,   50,   88,   10],
       ...,
       [ 694,   26,    2, ...,   23, 2933,    1],
       [   7,    8,   20, ...,    0, 4758,  588],
       [ 987, 1545,   26, ...,   11, 3331, 7327]]), array([[   0,  125,  100, ..., 3100,  124,  911],
       [7117,   88,   29, ...,   12,  297,   55],
       [  81,  228,    0, ..., 1132, 6095,  227],
       ...,
       [   0,  107,  159, ...,    0,    0,  159],
       [  14,  956,   10, ...,  907,  272, 1048],
       [2377, 7010,    2, ...,  125,  100,  272]]), array([[7363, 7495, 7184, ...,   20, 3005,   29],
       [  12,   52,  150, ...,  108,  535,   21],
       [4938, 7012, 7012, ..., 5180,  735,  535],
       ...,
       [ 118,  100, 2243, ...,   21,  479,   36],
       [7488, 4448,  100, ...,  112,   16,   85],
       [  18, 5142, 2069, ..., 7495, 7184, 6765]]), array([[   2,    0,  856, ...,  297,   23, 6969],
       [  29,  136,  639, ...,    0, 1275,   85],
       [  51, 7110,    0, ...,    0, 3904,  236],
       ...,
       [7329,   47,    0, ...,    0, 1749,  911],
       [1770,  343,   18, ...,    2,   12,    0],
       [   0,  148,   88, ...,    0,  856, 7111]]), array([[   2,   12, 4405, ...,  254,  101,  482],
       [  23,  319,   85, ...,   23, 3822,  118],
       [ 107,    0,    0, ..., 7012, 7184,   23],
       ...,
       [  30,    7,   18, ...,  223,   85,   51],
       [ 466,   23,    0, ...,   16, 1075,  224],
       [ 102,   20,    0, ...,   12, 4405,   23]]), array([[2710,   85, 3145, ..., 1795,  542, 1612],
       [ 111,   11, 7493, ...,    1,   18,    0],
       [  74,    7,   88, ..., 7493, 1004, 2278],
       ...,
       [  37,   11, 7495, ...,   68, 1107,  228],
       [2978, 1685, 6758, ..., 1153,  946,   12],
       [ 108,   29,   18, ...,   85, 3145,   23]]), array([[3623,  722,  273, ..., 7495,   61,  709],
       [  55,  227,  101, ...,   69,   18,  166],
       [  23, 1093,  120, ...,  223,   10,   51],
       ...,
       [   7,    0,   21, ...,  295,  613,   23],
       [ 613,  214,  102, ...,  359, 1545, 2427],
       [  16,  791, 1685, ...,  722,  273,  118]]), array([[  88, 1709,  694, ...,   23,   17,   48],
       [4400,    0,   67, ..., 2355, 6670,    0],
       [7153, 6348,   23, ..., 3894,  540, 4974],
       ...,
       [1133,   10,    0, ...,  227, 3730, 6946],
       [  10,  911, 1795, ...,   23,    2,   26],
       [3894,  510,   55, ..., 1709,  694, 2451]]), array([[ 809, 3168, 1880, ...,  280, 1685, 6758],
       [1162, 2529,  101, ..., 7598, 7599,    0],
       [7153,  349, 2118, ..., 7153,  100,  534],
       ...,
       [ 432,  322, 2401, ..., 5700, 6348,   23],
       [7104,  350, 2203, ..., 1685, 7153,  204],
       [2427,  228,   43, ..., 3168, 1880,  223]]), array([[ 118,    6,  100, ..., 2480, 2450, 2149],
       [1162,  204,  100, ...,  101, 1498, 1685],
       [7153,   48, 1896, ...,   81,  154,  159],
       ...,
       [1004,    2,  472, ...,   85, 2383, 2396],
       [7012,  561,  118, ...,  106, 7009, 1276],
       [1107, 7012,   23, ...,    6,  100,  240]]), array([[  20,  121,   23, ...,  107, 7645,   23],
       [1397,    0,    0, ..., 3319,  223,  227],
       [  26,   14, 2241, ...,   10, 1160,   12],
       ...,
       [  12, 1765,  124, ...,  118,   88,   53],
       [2710,    6,    7, ...,    0,  118,   10],
       [ 101, 2481, 2710, ...,  121,   23,  349]]), array([[  23,  221, 2489, ...,    0, 1685, 7153],
       [ 694,  673,  911, ...,  151, 2268,   26],
       [  12,    0, 2310, ..., 7422,   88,   18],
       ...,
       [7673,  247,  954, ...,   26,   12,    0],
       [   0,   23, 2376, ...,   50,   33,   12],
       [ 856,  842,    0, ...,  221, 2489,   53]]), array([[ 369, 1425, 7598, ...,  228, 1827, 5518],
       [6294,   55, 5518, ...,  534,    0,  179],
       [ 227, 4782,  124, ..., 3010,   97,  101],
       ...,
       [2377,   23, 3782, ...,  251,  986,  105],
       [ 136,  498, 3177, ...,  100,  534, 2855],
       [ 227,  102, 2404, ..., 1425, 7598,  100]]), array([[6026,  151,    0, ...,  274, 1765,   84],
       [ 223,   85,   18, ...,  136,   81, 1710],
       [1315, 3177,  100, ..., 2354, 2355,   48],
       ...,
       [1508,    0,   29, ...,  114,   61,   50],
       [ 540,   10,  115, ...,  377,    0,   53],
       [  69,  317,  111, ...,  151,    0, 1444]]), array([[1388,  228, 2021, ...,  945, 1490, 2672],
       [ 101,  729,  214, ..., 2404, 7688,  555],
       [ 228, 1334, 1163, ...,  356,   53,  885],
       ...,
       [5189,   10,  228, ...,    0,    0,  337],
       [  12, 4802, 1029, ...,  630,   10,    0],
       [ 337,  159, 1527, ...,  228, 2021,   23]]), array([[5981, 1839,    0, ...,   12, 5834,    0],
       [  85,   12, 1955, ...,  660,    0,    0],
       [   0,   18,  198, ...,   12, 4459, 3782],
       ...,
       [ 102,  262,   78, ...,  100,  179,  228],
       [  59,  588,  101, ...,  715,  993,  100],
       [ 382,  266,  162, ..., 1839,    0, 7340]]), array([[ 228,  274,  603, ...,  174,   88,  100],
       [ 262,  227,   53, ..., 2055,    4,  417],
       [7603,  136,   81, ..., 2357, 2692,    0],
       ...,
       [   0,   10,    0, ...,  369, 1026,  165],
       [4229,  509,  253, ...,   81,  791,  204],
       [ 274,  101, 2387, ...,  274,  603,  417]]), array([[ 842,    0,  115, ...,   88,  356,   18],
       [ 811,  264,  238, ...,  279,   38,   51],
       [4635,  482,  124, ...,   10, 1310,  387],
       ...,
       [1184, 6532,  136, ...,   50,   53, 1021],
       [ 224,   58,   88, ...,  266,  465,   81],
       [  27, 1359,   50, ...,    0,  115, 1254]]), array([[7728, 7073,   23, ...,  242,   55,  954],
       [  20,    7,  710, ...,    0,  118,   53],
       [ 100,  534, 1583, ...,   14, 6213,   23],
       ...,
       [2333,  475, 7247, ..., 2109,   12, 1991],
       [  29,    0,   98, ..., 6053,    6,   50],
       [  18, 3619,    0, ..., 7073,   23, 7184]]), array([[ 322, 2070,  349, ..., 6445, 2398, 7721],
       [  23,  908, 7730, ...,  643,  228, 2919],
       [  14,  246,  328, ..., 3596,  107,   37],
       ...,
       [ 101, 1239,   26, ...,   10,   12, 7386],
       [ 208,  100,   10, ...,   81,   50, 4762],
       [2398,   12, 1442, ..., 2070,  349,   26]]), array([[7034,  308,  385, ..., 4782,  149, 1685],
       [3331,   26, 4087, ...,    0,   12, 1071],
       [  29,   18, 1964, ...,    0,   17,   12],
       ...,
       [7788,   18,  610, ..., 1175, 3802,   10],
       [2333, 7784,   53, ..., 1285,   34, 3082],
       [  20,   24, 1531, ...,  308,  385,   29]]), array([[4119,   18, 3805, ...,   81,   18,    0],
       [ 150,  967,  973, ...,  150,  118,   88],
       [3333, 4120,  228, ..., 4119, 4120,   20],
       ...,
       [ 111,  911, 1163, ..., 2137,  223,  100],
       [ 738,  224,   14, ...,   51, 2832, 5456],
       [ 911,  705,  911, ...,   18, 3805, 1114]]), array([[  88, 2472, 7786, ...,  295,   37,   36],
       [2761,   61,    0, ..., 4771,   29, 3338],
       [ 150,  967,   20, ...,   23,  106,  729],
       ...,
       [   0, 1307,  118, ...,  111,  102,  907],
       [  67,   23,   88, ..., 4119,  111,  301],
       [ 142,  136,   18, ..., 2472, 7786,  102]]), array([[2040,   18,  772, ...,   26,  125,  224],
       [  14, 3715, 4119, ...,   23,  900, 7312],
       [  55,  102, 1425, ..., 3338,   23,  266],
       ...,
       [1068,  247,  359, ...,  240,  118,  228],
       [ 478, 4119,   23, ..., 3477,  738,  228],
       [2387, 4419,   23, ...,   18,  772,  863]]), array([[  50,  434,  102, ...,    0,  348, 4119],
       [2427,  227,  226, ..., 1285,   12,    0],
       [  10, 3261,  322, ..., 1713, 4120,   29],
       ...,
       [  26,    0,    2, ...,  136,  475, 5957],
       [2387,  382,  490, ...,   39, 3111,   29],
       [3141,  238,  147, ...,  434,  102,  332]]), array([[2713, 3177, 6775, ...,  958,  996,   23],
       [ 149,  224,  102, ..., 1440,  465,  262],
       [  26,  907,  272, ...,   18,  294,   14],
       ...,
       [  55, 7829,  272, ...,  908,    0,   10],
       [  18, 2054, 6866, ...,  359,   12, 7499],
       [  29,   51,    0, ..., 3177, 6775, 6775]]), array([[  52,  223,  232, ...,    0, 6866,  136],
       [  81,   88, 3471, ...,   23,  815,  240],
       [  16,    0, 7812, ...,  552,  189,   12],
       ...,
       [ 725,  228,  265, ..., 6522, 4270,  392],
       [ 595,   12, 1201, ..., 2392, 6866,   85],
       [  12, 3248, 2404, ...,  223,  232,   39]]), array([[ 205,    7,  979, ..., 2529, 5805,  150],
       [ 208,   21,  241, ...,   55,  209,   18],
       [1291,  439,   55, ...,   26, 2235,  149],
       ...,
       [ 929,   12, 2629, ..., 6076,  491, 2451],
       [2354, 2361,   18, ...,  842,   88, 3471],
       [ 228, 4775,    0, ...,    7,  979,   36]]), array([[  29, 2376,   97, ...,   21,  264,   81],
       [  26,  224,   37, ...,   16,   23,    7],
       [   8,  291,   18, ...,   14,   88, 1306],
       ...,
       [  97, 2446,  223, ...,  407,  111,  778],
       [2882,  227, 2404, ...,  710,  438,   30],
       [1040,  943,    0, ..., 2376,   97, 3697]]), array([[  12,    0,  739, ...,  201,   61,  540],
       [ 966, 1137,  227, ..., 4345,  224,  263],
       [  18,  622, 6230, ...,  860, 2922,   23],
       ...,
       [ 399,    2,  127, ...,   41, 6775,    7],
       [  30, 3128, 1091, ...,  272,  227,  758],
       [ 231,  540,  272, ...,    0,  739,   61]]), array([[ 963,   29,  434, ..., 6369,  223,  124],
       [1806, 1093, 4084, ..., 1532,  908, 4236],
       [  29,  481,  609, ..., 4071,   10, 4704],
       ...,
       [ 223,  201, 7868, ...,  272,  818,  238],
       [ 102,  179, 7868, ...,   20,  695, 2451],
       [7868,   23, 7852, ...,   29,  434,    0]]), array([[ 125,   81, 2293, ..., 5192,   23,  542],
       [ 417,  359, 4067, ...,    0,   81,    2],
       [  62,  978,  224, ...,   12, 4802,   29],
       ...,
       [2416, 7870,  227, ..., 1885,  107, 5405],
       [  12, 5465,   55, ...,  491, 7871,   48],
       [ 100,  954,  954, ...,   81, 2293,    0]]), array([[2450,  150,   12, ..., 3817,  228,  478],
       [  81,    2, 7852, ..., 2451, 7871,   23],
       [7870, 7852,   39, ...,   12,  123, 1098],
       ...,
       [   0,    0,    0, ...,    0,   10,  115],
       [5552, 5537, 7858, ..., 2404, 2045, 7868],
       [2203,   10, 7852, ...,  150,   12, 1468]]), array([[   0,    7,   74, ...,    0,   82, 6371],
       [5033,   85,   18, ..., 7752,  349,   39],
       [  12, 2719, 5753, ...,    7,    0,   17],
       ...,
       [  33, 2975, 1687, ...,   10, 7871,   23],
       [ 254,   88,    0, ...,  208,  227,  535],
       [6117,   11,   33, ...,    7,   74, 7749]]), array([[1276, 1475,  581, ...,   81,  958, 7852],
       [  88,   61, 2404, ..., 7860,   39, 3737],
       [ 100, 1544, 1106, ...,  534,  107,  214],
       ...,
       [  12, 7567,   29, ...,   29,  295, 3087],
       [ 535,  214, 6345, ...,   20,  748, 7909],
       [ 227,  102,   20, ..., 1475,  581,    7]]), array([[  12,    0,  621, ..., 2916,  107,    0],
       [3612,   85, 3612, ...,  986,  253,  224],
       [2824,  118,  124, ...,   50, 1123, 1040],
       ...,
       [  12, 1052,   29, ...,   12,  610,  478],
       [  29, 1293,  100, ...,  174, 3246,   88],
       [5370,   85,  295, ...,    0,  621,   51]]), array([[7906,  136, 6750, ..., 1184,  135,   12],
       [6445, 2398, 7912, ...,  731, 2318, 6999],
       [  51,  344,  311, ...,   67, 7030, 1438],
       ...,
       [ 389,   23, 1033, ...,   88, 2780, 6374],
       [  20,    0,  517, ...,    0,  274, 4914],
       [ 660, 1817, 1093, ...,  136, 6750, 7107]]), array([[ 141, 1164,   10, ...,   23,  764,   51],
       [ 482,  387,   17, ...,  239,    0,  463],
       [  81,   17,   12, ...,  873,   18,    0],
       ...,
       [6308,  997,   85, ..., 2759, 3889,    2],
       [ 136, 1712, 1192, ...,  204,   81, 2061],
       [  27,  162,   55, ..., 1164,   10,  262]]), array([[3208, 7908,   36, ...,  179,   55,  234],
       [ 100,  266,   18, ...,   12, 5514,   26],
       [ 389,   17,  297, ...,  761, 2831, 2664],
       ...,
       [2291, 4038,  272, ...,  907,  174,   88],
       [ 179,  907, 3067, ...,  224,  207,   11],
       [2398, 7936, 3288, ..., 7908,   36,  266]]), array([[7936, 3288,   36, ...,  149,    6,  735],
       [  20,  274, 3801, ..., 6982,  100,  174],
       [1645,  343, 3015, ...,   29,   23, 7293],
       ...,
       [ 214,  115, 4655, ..., 1290,  907,  535],
       [ 154,   10,   20, ...,    0, 2915,   67],
       [  12,    0, 3618, ..., 3288,   36,  290]]), array([[  88, 7203,    6, ...,   23,   82, 5610],
       [ 506, 1430, 1430, ...,  227,   23,  101],
       [  52, 2475,  761, ...,  214, 2175,   53],
       ...,
       [  53,  118,  232, ...,  100,   14,  129],
       [ 343,  227,  826, ...,   26,    0,   81],
       [  18,    0,    0, ..., 7203,    6,  227]]), array([[ 292,  291,  136, ..., 1132,   18,   93],
       [2475,  761, 1036, ...,  270,  154,  987],
       [  10,    0, 1541, ...,  174,   23,  272],
       ...,
       [ 107,   17,  107, ...,  224,  263,   61],
       [2727,  224,  148, ...,   23,   85,    0],
       [   0,   12,  275, ...,  291,  136,  856]]), array([[ 223,   61, 1356, ...,   18,    0, 7682],
       [ 100,  174,   88, ...,  136, 7682,   20],
       [1028, 7934,   50, ..., 1414, 5669,  228],
       ...,
       [  29,  101,    0, ...,  159,   81,  249],
       [  23,  103,  105, ...,   42,    0,    0],
       [  67,   42, 5919, ...,   61, 1356,  967]]), array([[   0,  100,   94, ...,   10, 7949,   10],
       [ 249, 7785,  208, ...,  308,  585,   81],
       [   0,   29,  136, ...,  542,   10,   20],
       ...,
       [ 735, 2222, 1882, ..., 7958,    6,    7],
       [  38,  851,  100, ...,  272,   88,  154],
       [ 274, 4238, 7954, ...,  100,   94,    0]]), array([[1731,   12, 2208, ..., 7968,   39,  228],
       [ 973, 2404, 7958, ...,   53,  136, 7762],
       [3022,  100,  534, ...,  249, 7971, 7958],
       ...,
       [ 326,  270,  343, ...,  516,  295,    0],
       [  26,  326,   82, ..., 3105, 1056,   12],
       [  91,    0,   48, ...,   12, 2208, 7968]]), array([[   7, 7558, 1009, ...,   97,   62,   20],
       [2433, 6026,   21, ...,  939, 6098,   67],
       [  12, 3081,   29, ...,   42,   41, 4795],
       ...,
       [   0, 1653,  136, ...,  153,   81,   12],
       [2155,  227, 2241, ..., 3502, 7963,   67],
       [  47, 3323,  207, ..., 7558, 1009,  125]]), array([[1764, 2477,  811, ...,    0,   10, 1293],
       [  68,   23,  394, ...,  879,   29, 2259],
       [ 100,  272, 2418, ...,   88, 1660,   10],
       ...,
       [  51,  434,   29, ...,   23,  907,   14],
       [1741,  141,  521, ...,    0,  100,  142],
       [   0,   85,   12, ..., 2477,  811,  105]]), array([[2354, 2360,   18, ...,    2, 7990,  250],
       [2354, 2360,   18, ...,    2,   12, 3054],
       [5328, 2354, 2365, ..., 1892,   10,    0],
       ...,
       [ 349,   18,  945, ...,  100, 3942, 3942],
       [ 204, 2278,  162, ...,   62,  118,   39],
       [ 118,  121,  369, ..., 2360,   18, 1254]]), array([[  18,  475, 3291, ...,  983,  118, 1162],
       [   0,   18, 5132, ..., 1892,   23,  561],
       [ 118,  100,  842, ...,   12, 1362,   97],
       ...,
       [ 227,   53, 7351, ...,  150,  967, 6646],
       [ 204, 4147,  387, ...,  866, 1419, 2488],
       [2404,   12,    0, ...,  475, 3291,   47]]), array([[  29,   45,  100, ...,    0,   29,    0],
       [  18,  622, 1750, ..., 7225, 7991, 1368],
       [1276,  742,   10, ..., 3055, 2398, 1892],
       ...,
       [  61, 2404,  197, ..., 5291,   23,  112],
       [  51, 3228,   18, ...,  227,  102,  578],
       [ 107,  274,  276, ...,   45,  100,    0]]), array([[   0, 2020, 6535, ...,    0,    2,   12],
       [4691,   18,    0, ...,  322, 4691,  100],
       [   0,    0,  925, ...,  498,   48,    0],
       ...,
       [2387,  319,  874, ...,   10,   12, 2807],
       [ 349,  694,   55, ...,  214,  207,  141],
       [  20,  846,   23, ..., 2020, 6535, 4681]]), array([[ 125,  346,  159, ..., 7131,  564, 3301],
       [  36, 1733,  228, ..., 6784, 3819,   27],
       [ 162,    0,  208, ..., 2427,  227,   18],
       ...,
       [1885,   62,   81, ..., 3146,  204,   10],
       [  12, 2103,   14, ..., 8012, 8013, 3326],
       [8014, 8015,   23, ...,  346,  159,   10]]), array([[2377,   23, 7486, ...,   18, 1521,  274],
       [3861,   29,  669, ..., 1075,   53, 8016],
       [3326,   47,   81, ...,    0, 1461,   10],
       ...,
       [ 228, 6581,   81, ..., 4448, 1570, 4448],
       [ 915, 2451,    0, ...,   17, 1191, 5516],
       [7844,  246, 1068, ...,   23, 7486,    0]]), array([[ 228,  369,  174, ...,  219, 3106, 2115],
       [ 201,   23, 1075, ...,  663, 3326,  742],
       [  12,    0,   12, ...,   61, 8020,  100],
       ...,
       [ 118,   23,   42, ...,   37,  141,   10],
       [3764,  552,  242, ..., 2328,  889,  189],
       [3159, 1393,   58, ...,  369,  174,  227]]), array([[  41,  174,    0, ...,  597, 8019, 2137],
       [1839,   24, 1839, ...,   29, 4947,  790],
       [   0,   85, 8005, ...,  911, 3146,  273],
       ...,
       [8039, 4833,   10, ..., 7952, 1683,  136],
       [6845, 3759, 2706, ...,    0,  746, 1346],
       [   0,  100,  291, ...,  174,    0,   51]]), array([[  29,   18, 3202, ...,   61,  359,   26],
       [ 609,  382,   88, ...,  105,   10,  609],
       [   0, 8039,  100, ..., 7980, 2706,   97],
       ...,
       [3555,   55,   81, ...,  582,   12,  557],
       [  29, 2450, 1595, ...,  223,   18, 4608],
       [   0, 2158,  111, ...,   18, 3202,  224]]), array([[  76,    7,  115, ...,  929,   55,  272],
       [3664,   51,    0, ..., 1084,   42,  159],
       [  81,  359, 3925, ...,  100,  842,   12],
       ...,
       [  64,  266,    7, ..., 2335,  101, 2380],
       [   6,    0,  118, ...,  660,   10,   12],
       [ 919,   29,   51, ...,    7,  115,   18]]), array([[8033, 6897,  100, ..., 4654, 3089, 8033],
       [ 100,  272,   12, ..., 2794,   55, 6691],
       [ 270,  266,  100, ...,  124,    0, 3379],
       ...,
       [  10,   42,    0, ...,   29,  526,  491],
       [   0,   26,  125, ...,   81,  238, 1220],
       [1682,    2,   12, ..., 6897,  100,  534]]), array([[ 297,   29,   42, ...,  279,  101, 1439],
       [   4, 5350,  118, ...,  136, 1167,  228],
       [2392,  232,   12, ...,  136,   47,   14],
       ...,
       [  50, 2020, 1239, ...,  100,   14, 3595],
       [ 223,   18, 1525, ...,   53,  125,   29],
       [ 101,  274,  381, ...,   29,   42,  735]]), array([[  14,  917,   55, ..., 8039,   26, 1441],
       [   2,   12,  178, ..., 5767,   23,   18],
       [4449,    2, 4222, ...,   23,  349,  273],
       ...,
       [ 157,  534,  100, ...,  100,  863,  162],
       [ 105,    0,   10, ...,   69, 3100,  100],
       [ 534, 1480, 2334, ...,  917,   55,   53]]), array([[  55,   81,  268, ...,    0,   10,  136],
       [ 920,  224,  501, ..., 6678, 1908,  111],
       [2176,   55,   39, ...,  174,  241,   29],
       ...,
       [ 174, 1469,   12, ..., 1350,   61,    0],
       [ 268,  954, 1930, ...,  417,  115,   12],
       [6651, 1590,  118, ...,   81,  268,    0]]), array([[  55,   81, 8046, ..., 2208,   55,   81],
       [  12, 7704,   29, ..., 5292,    2,   12],
       [6445, 2398, 8045, ..., 6779,  274,  183],
       ...,
       [  89,  233,   21, ...,  890,   26,  535],
       [  88, 4554,  228, ...,   47,  710,    7],
       [2568, 8049,  274, ...,   81, 8046,  508]]), array([[ 118,   29,   26, ...,   12,    0, 4790],
       [ 125,   49,    0, ...,   50, 1679,    0],
       [  81,  107,  639, ...,   10,   20,  624],
       ...,
       [1167,   81,  811, ..., 8049,  350,  227],
       [3843, 4362, 1641, ...,   12,  125,  224],
       [2945,  174,   50, ...,   29,   26,  125]]), array([[ 228,   19,  100, ..., 2529,   88,  892],
       [  50,  270,   38, ...,  123,  224,   33],
       [ 224, 3270,   29, ...,  227,  240,  118],
       ...,
       [ 915, 2882,  956, ...,  535,    0,   21],
       [ 663,    0,  102, ...,   18, 1040,  183],
       [ 735,   37,   20, ...,   19,  100,  266]]), array([[3082,   81,  136, ..., 2629, 8049,   47],
       [ 535,  227,  232, ..., 8058,   23, 7959],
       [  47,  535,  227, ...,  576,   81,  136],
       ...,
       [ 107,  735,  335, ...,  100,  534,   27],
       [   0,  270,  100, ...,  227,   14,  120],
       [5072,   55,  227, ...,   81,  136,   26]]), array([[ 382,   24,  225, ...,   23,  228,  896],
       [  12, 4334,  247, ...,  343,   21,    4],
       [  53, 2529,   48, ...,   18,  625,   29],
       ...,
       [4189, 2472,   55, ..., 2282,  509,  253],
       [ 815, 4350,   12, ...,   12, 1541,    0],
       [   0,    0,    0, ...,   24,  225,  241]]), array([[ 873,  343,   51, ...,   51, 4781,   18],
       [7807,   29, 6983, ...,   58,   51, 1391],
       [  23,   14,  125, ..., 5013, 2681, 8059],
       ...,
       [1254,    2,    0, ..., 5083,  178,   50],
       [ 939,    0,   39, ...,   88, 6527,  673],
       [ 337,  227,   37, ...,  343,   51,    0]]), array([[8061,    0,   23, ...,  691,   42,  885],
       [  12,  482,   23, ..., 1984,    2,  911],
       [1648,    0,  474, ...,   24,    0,   85],
       ...,
       [5047,  232,  907, ..., 4567,    0, 1112],
       [  10,   38,  124, ...,  102,   20,    0],
       [ 709,  227,  941, ...,    0,   23, 1157]]), array([[ 239,   10,   12, ...,   81,  228,   14],
       [8066,   26,  224, ...,  908, 5336, 6842],
       [  23,  208,   67, ...,  960,   12,  138],
       ...,
       [   0,  864,  101, ...,   50,   10, 2403],
       [ 125,  209,  907, ..., 6983, 2380, 6657],
       [2403,  911,    0, ...,   10,   12, 4041]]), array([[2408,  233,   12, ..., 1718,    0,    2],
       [  16,   27, 6254, ...,  151, 6388,   39],
       [  12, 4803, 1371, ...,   23, 7563,  344],
       ...,
       [  23,   61,   49, ...,  174, 2480,  238],
       [2272, 1199,  911, ...,   35,  993,  279],
       [  12, 7209,   29, ...,  233,   12, 5155]]), array([[ 842, 1093, 3202, ...,  233,   51,  729],
       [  51, 2713,    0, ...,  148,  409, 3116],
       [1685, 6348, 1162, ...,   88, 7125,  120],
       ...,
       [  88,   12, 2357, ...,   50,  247, 1683],
       [ 232, 2398, 2404, ...,   47,  869,   81],
       [  55,   10,   20, ..., 1093, 3202,    0]]), array([[  36,  107,  967, ...,   12,   13,  142],
       [ 141,    0,   10, ..., 3839,    0, 3055],
       [ 233,   21, 1879, ..., 3838,   61,  548],
       ...,
       [ 118,   61, 7668, ...,  224,   23,  270],
       [1658,    6,  290, ...,  232, 2559, 3414],
       [  12, 1879,  232, ...,  107,  967, 1405]]), array([[1075,   23,  997, ...,  101, 1713,   24],
       [ 534,  100, 1685, ..., 1000,   12, 3064],
       [ 100,  240,  100, ...,  540,  100,   14],
       ...,
       [ 118,   51,    0, ..., 2145,  125, 3078],
       [ 343,  101,  482, ...,   55,  272, 5492],
       [5105,   10,  322, ...,   23,  997,   29]]), array([[6208, 3954, 2593, ...,   21, 3075, 1685],
       [  33,    7,  497, ...,   61,  423,   30],
       [   0,   26, 1191, ...,  101, 2593,  478],
       ...,
       [  50,  869,   29, ...,   29, 6348, 7011],
       [  18,  491,   26, ...,   29, 7495,   23],
       [8091, 7495,  274, ..., 3954, 2593, 3596]]), array([[  88,  118,   20, ...,   27,    0, 3308],
       [1765, 7596, 1632, ...,  197,  227, 6313],
       [  85,  223, 7495, ...,   81,  112,   12],
       ...,
       [  10, 7428,   81, ..., 7390,  238, 1921],
       [   2,   21,  568, ...,  174,  815,  755],
       [ 227,    2,   12, ...,  118,   20,  227]]), array([[3697,   12, 2377, ...,  100,  208, 1525],
       [ 107,    7,   74, ...,   12, 4785, 1685],
       [7153, 1583, 1765, ...,  102,  557,   10],
       ...,
       [ 272,   55,   61, ..., 3596,   48,    6],
       [ 214,  120,  100, ...,   88, 5766,   39],
       [  51, 4854,    0, ...,   12, 2377,  102]]), array([[6175,   10,  920, ..., 2413, 4261, 8100],
       [ 224,   53,   51, ..., 8100,   23,  107],
       [  55,  142,  673, ...,  139, 5330,   20],
       ...,
       [ 115,   27, 2255, ..., 1594,  535, 3625],
       [  23, 3670,  141, ..., 1685,   29, 2376],
       [ 735,  885,  555, ...,   10,  920,   47]]), array([[7851,    0,   39, ...,  697,   29, 1748],
       [   6,   88,   10, ...,   75,  322,  900],
       [2155, 6348, 1981, ...,   12,  406,  228],
       ...,
       [3105, 2753,   29, ..., 1558,  106, 2450],
       [1765, 7596,  201, ...,    0,   10,  526],
       [ 406,   29,  509, ...,    0,   39,   42]]), array([[ 925,   26,   12, ...,  101, 2392,  124],
       [  21, 1358,   61, ...,  115,   42, 1765],
       [  29, 4767, 1765, ..., 6348,   26,  148],
       ...,
       [1629,    2,  284, ...,   29,  141,   26],
       [ 100,  263,    0, ..., 7036,   67,  322],
       [4450, 1685, 6348, ...,   26,   12, 1852]]), array([[  12,    0,    0, ..., 2568,   10, 4869],
       [ 124,  118, 2451, ...,  474, 5704,    2],
       [  18, 7305,    0, ..., 1745,  343,   12],
       ...,
       [3483,  101, 2392, ...,   47,   81,  228],
       [  14, 2450, 8115, ..., 8115, 2975,   88],
       [2974,  546,  815, ...,    0,    0, 1038]]), array([[ 147,  387,    0, ...,  102,  159, 2488],
       [ 100, 1492,   55, ...,  101, 2380,   23],
       [ 227,  142,  204, ...,   27, 1187,  100],
       ...,
       [3394,    0,  100, ...,   24,  350,  465],
       [   0,  540,  100, ...,  478,   17,   18],
       [ 639, 5101,    2, ...,  387,    0, 7866]]), array([[  69,  236,    0, ...,  261,    9,   10],
       [ 273, 2447, 2354, ...,   12,  518, 1580],
       [  51, 5803,  421, ..., 1558,   29, 5085],
       ...,
       [8117,  559,    0, ...,   39,   23,   39],
       [1683,  552,   10, ...,   81,   18, 3309],
       [  10,   12, 7599, ...,  236,    0,  100]]), array([[8106, 3242,  223, ...,  100,  272,   48],
       [ 784,    2,  228, ...,  542,  510,   79],
       [ 150, 8106,  226, ..., 5802, 4906,   69],
       ...,
       [ 238,  938,  546, ...,   55,  160,  227],
       [1616,   85,   12, ...,  111,    6,   55],
       [ 197,   88,  228, ..., 3242,  223,  224]]), array([[2460,  118, 5531, ..., 1359,   10,  356],
       [ 118,  694, 1412, ...,   10,   11, 2447],
       [2318, 2369,  100, ..., 3605,   45, 6797],
       ...,
       [ 937, 3952,   29, ...,  249, 1954,  812],
       [3548,  232,   81, ...,  207, 2416,   23],
       [ 542, 2176,  100, ...,  118, 5531,  359]]), array([[  62,   16,  811, ...,  112,   16, 1885],
       [ 214,  621, 1892, ...,   47, 1014,  101],
       [ 491,   33,  101, ...,   21,  478,  123],
       ...,
       [  23,  136, 2577, ...,    0,   23,   12],
       [   0,  251,   10, ..., 5950,   23,  929],
       [  18, 4906, 2472, ...,   16,  811,  105]]), array([[  53,  540,  100, ...,  228,  508, 8134],
       [  10, 5055,   42, ...,  227,  142,   29],
       [5857,    0, 5908, ...,   17,   18, 1291],
       ...,
       [1419, 3346,   23, ...,  227,  535, 1521],
       [1276, 2404,   10, ..., 1286,   85, 8135],
       [8121,   23, 8132, ...,  540,  100, 7549]]), array([[ 101,  264,   36, ...,  380,   29,  101],
       [5027,   23,    2, ...,   33,   12, 1213],
       [ 505,   81,   36, ...,  376,  308, 1068],
       ...,
       [  20, 8133,  150, ..., 2404,  214,   20],
       [4071,   12,    0, ...,  815,   88,   20],
       [ 791,  500,  100, ...,  264,   36,   23]]), array([[ 332,  101,  972, ..., 3112,  227,   26],
       [1064,   67,   42, ...,   29,   47,   81],
       [ 509,  369,  159, ..., 7980, 8137,  464],
       ...,
       [ 227,   23,   53, ..., 4071,   23,    0],
       [ 123,  704,   10, ...,    0,   62,  101],
       [2392,   23,  159, ...,  101,  972, 8134]]), array([[ 102,  272,  101, ..., 8134,   81,    0],
       [  24,   23, 3611, ...,  471,   81,  136],
       [  26,  112,   61, ...,   17,   10,   20],
       ...,
       [   0, 1093, 3468, ..., 1240,   29, 6041],
       [   2,   16,  100, ...,  272,    0,   10],
       [ 273,  118,   47, ...,  272,  101, 2392]]), array([[ 174,   88,  738, ...,  105,   48,    0],
       [ 101,  750,    0, ...,  540,    7, 2880],
       [ 232,    7,  128, ...,  101, 2392,   23],
       ...,
       [ 510,   29, 3327, ...,    0, 3916,  111],
       [4522, 2313,    0, ..., 2640, 8145,  100],
       [ 148,   85,   69, ...,   88,  738,    7]]), array([[  10, 2840, 3852, ...,   47,   18, 2069],
       [ 842,  465, 1326, ...,  208,    0, 3286],
       [1490,  274,  101, ..., 2326,  100,  356],
       ...,
       [ 208,   27, 2692, ...,  319,  232,  100],
       [  14,  555,  708, ...,   53,  123,    0],
       [   0,   12,    0, ..., 2840, 3852,    0]]), array([[3286,  100,  266, ...,  729, 3245, 8145],
       [5531,  236, 1839, ..., 2196,  387,   10],
       [ 118,   23,  815, ...,  214,  535,   29],
       ...,
       [  10, 2677,  266, ..., 5250,    0, 4647],
       [  23,    0,    0, ...,  142, 1048,    0],
       [2673, 8147, 2203, ...,  100,  266, 3022]]), array([[ 292, 8143, 2404, ..., 2203,   10, 8147],
       [ 387, 2673, 1267, ...,  491, 3120,   53],
       [ 141,   12,  627, ...,   81,  100,  534],
       ...,
       [ 815, 5218,   85, ...,  136,   10,  638],
       [ 223,   47,   18, ...,   10,  322, 4876],
       [  36,  148,  100, ..., 8143, 2404,  101]]), array([[ 228,  369, 2224, ...,    0,  100, 6245],
       [  67,  588,  230, ...,  273,  227,  401],
       [ 815,  129,   10, ..., 1683,    2, 6053],
       ...,
       [ 555,    2,  349, ...,  482,    2, 3519],
       [5699,  509, 3513, ...,  100,  316,  911],
       [4448,   37, 1939, ...,  369, 2224,  232]]), array([[  94,   20,   61, ...,  465,   81,   88],
       [  61,  359,   31, ..., 8155, 4243,  295],
       [   0,  498, 5695, ...,  100,  534,   18],
       ...,
       [  16,   47,   30, ..., 2624,   23, 1941],
       [8154,   74,    7, ...,   12, 2178,    4],
       [  29, 3764,  142, ...,   20,   61, 7780]]), array([[  55,   81,   50, ..., 1124,   53,  232],
       [  55, 3673,   23, ...,  635,   55, 7888],
       [  10, 3975,   21, ...,   16, 1810,   55],
       ...,
       [  23, 1405,   29, ..., 2354, 2355, 3756],
       [  18, 1254,    2, ..., 1508, 4192,   29],
       [3756, 1974,    0, ...,   81,   50,    0]]), array([[ 265,   10,    0, ..., 2395, 2396,   23],
       [3773, 3756,   23, ..., 3015,   29,  101],
       [1163,   85, 3055, ...,  729, 4434,    0],
       ...,
       [ 142, 4179, 3776, ...,   88,  118, 3776],
       [ 279,  118,   61, ...,  118, 8141,  842],
       [  88,  162,    0, ...,   10,    0,  265]]), array([[5952,  465,    0, ..., 1522,   14,  907],
       [2687,   85,  141, ...,   12, 7590,  535],
       [7483, 3181,  723, ...,  136,  196,   18],
       ...,
       [  20, 3163,  101, ...,   79,   29,  885],
       [  50,  535,   29, ..., 4149,  101,  478],
       [ 102,  208, 3837, ...,  465,    0,   39]]), array([[  53,  911, 2401, ...,  815,  976,  295],
       [ 885, 1222,  118, ..., 5628,  708, 8169],
       [ 478, 2398, 3837, ...,   53,  101, 2963],
       ...,
       [ 227,  232, 7302, ...,  100,  272,  636],
       [  10,  920, 2278, ..., 2854,   18, 5720],
       [4303,   12,  519, ...,  911, 2401, 1023]]), array([[  18,    0, 1075, ...,  609, 2398,   12],
       [2386,   85,   18, ...,  207,   88,   20],
       [4204,  500,   12, ...,  850,    7,   88],
       ...,
       [2028, 5699, 4815, ..., 4093,  709,  227],
       [1810, 4093,  709, ..., 3133, 5699, 1162],
       [ 465,  214,  535, ..., 1075,  502,  885]]), array([[5699, 3837,  219, ...,   18, 3146,   29],
       [   0,   39,  413, ..., 2287,    0,  174],
       [3672, 5807,   67, ...,    0, 1648,   29],
       ...,
       [1253, 2332,  197, ...,    0, 1862,    2],
       [ 317,   12,    0, ..., 1245, 3210,    0],
       [ 112,  141,    0, ...,  219,   12, 2617]]), array([[1056,  932, 3903, ...,  204,   12, 1599],
       [1604,  559,  872, ...,   12,    0,    0],
       [   0,   12, 5165, ...,  911,  384, 1358],
       ...,
       [  12, 1733,    6, ...,    0,   55,   81],
       [ 987,  159,    0, ...,   38, 2525,   24],
       [  53,  588,   18, ..., 3903,  540,   78]]), array([[7343,    0,  162, ...,    0,  334, 1425],
       [  29,   18, 2945, ...,    0,  322,  610],
       [1720, 1635,  607, ..., 4066,  118,   23],
       ...,
       [  18,  663,  125, ..., 4555, 7622,   23],
       [8195, 8195,  349, ..., 1984,   23, 5419],
       [   0, 5333, 2962, ...,  162,    2,   42]]), array([[ 349,   18,  308, ...,  227,  951,    0],
       [ 101, 7984,    2, ...,  150,  223,   20],
       [3163,  610,  288, ..., 1382, 2334,  895],
       ...,
       [5191, 1227,    7, ...,  101, 2272,   53],
       [  39,   12, 3181, ..., 2117,   23,  262],
       [ 227, 2380,  815, ...,  308,    0,   29]]), array([[1180,   21,  660, ...,   26, 3406,   10],
       [ 829,  335,  223, ...,  365,   23,    3],
       [6983, 2451, 2354, ...,   26,   12,    0],
       ...,
       [7645, 8192,  148, ..., 2528,    0,  216],
       [ 301, 1653,  485, ..., 2427,  227,    2],
       [  39,  141,   12, ...,  660, 2450,  240]]), array([[3627, 3107,  907, ...,  100,  266,   12],
       [3285,  272, 2061, ...,  359,   10,  510],
       [3196,    0, 5703, ...,  261,   50,   12],
       ...,
       [2568,   81,   55, ...,   47,  102,  907],
       [ 174,  301,  142, ...,    2,  731, 4494],
       [6671,    6,   12, ...,  907,  223,   67]]), array([[  62,   81,  224, ...,   50,  141,  100],
       [ 272,    0,   16, ...,    0,   10,   42],
       [   0,   55,   81, ...,  101,  261, 1411],
       ...,
       [  29,  911,  705, ..., 2416,   18, 1191],
       [  29,   26, 1811, ...,  115,   18, 1807],
       [ 337,  224,  102, ...,  224,  331, 2404]]), array([[ 118,   47, 2664, ...,   42,   88, 2475],
       [ 761,   59,  129, ..., 2364, 2353, 2398],
       [2377, 8200, 8206, ...,    0,   23,    0],
       ...,
       [3286,  100,  219, ...,   29,  136, 5126],
       [3286, 2759, 2404, ..., 2752, 2115,  735],
       [3800,  624,    2, ..., 2664,   26, 2404]]), array([[ 344,  153, 4750, ...,    2,  136,   52],
       [ 100,  102,  769, ...,   29,   12, 5126],
       [ 465,  102,  227, ...,  761,    0, 3514],
       ...,
       [ 179,   51, 1312, ..., 1187,   10, 3071],
       [ 247,   85, 3509, ...,  136,  877,  159],
       [ 161,  118,  100, ..., 4750,   12,  278]]), array([[5126,   10, 1164, ..., 4071,   10,    0],
       [ 509, 4591, 7805, ..., 5882,   23, 3075],
       [   0,  100,  316, ...,   26,    0,   92],
       ...,
       [8218,   29,  141, ..., 8220,  107,   29],
       [  18, 2681,    0, ...,  137,  779,    2],
       [ 967, 8218,  540, ..., 1164,  118,  328]]), array([[  29,   78,  274, ..., 1145,  149, 8218],
       [1162,  224,   29, ...,   88,  638,  322],
       [  62, 8220,   97, ...,   23, 3163,  100],
       ...,
       [   0, 2145,   85, ...,  100,  906,  465],
       [  81,   27,  759, ...,  228,  421,  842],
       [ 136,   12, 1857, ...,  274,  100,  738]]), array([[  33,  100,  842, ..., 2288,   88,  526],
       [2288,    7,   76, ...,  427,   10,  921],
       [  18, 2109,   23, ...,  721, 7647,  204],
       ...,
       [ 465, 8222,  272, ..., 2719,   23,  542],
       [  55,   81,  162, ..., 1162,   26, 1541],
       [ 691,   12,  890, ...,  842, 1583,  227]]), array([[  74,    7,  224, ...,   62, 2555, 2447],
       [8216,  349,   14, ...,  349, 6671,   81],
       [8235,  124,   42, ..., 1063,   23,   49],
       ...,
       [ 115,   55, 3630, ...,   12,  398, 2265],
       [  81,  692,  317, ..., 2290,   29,   62],
       [  23, 1531, 8218, ...,  224,   58,    0]]), array([[7647,   26,  227, ...,   12, 2377, 2398],
       [2377, 2377,  540, ...,   53, 7682, 8118],
       [3179,  295,  885, ..., 5947,   42, 3146],
       ...,
       [2911,  274, 3357, ..., 3937,   23,  907],
       [   0, 1488,  111, ...,   53,  227,  270],
       [  20,    0,   51, ...,  227,  535,   36]]), array([[6934,   10,  223, ...,    0,   26,   51],
       [   0,    0,   85, ..., 1438, 2586, 8242],
       [  61,  907,  207, ...,    0, 2435,  387],
       ...,
       [  10,   12,    0, ...,  815,   20,   23],
       [ 465,  815,   20, ...,   12,    0,    0],
       [   2,  911,    0, ...,  223,   23,    0]]), array([[ 975,   23, 1293, ...,   26,  262,   88],
       [1807, 3269,   10, ...,    0,  535,  214],
       [1315, 3269, 4760, ...,   51,  670,  154],
       ...,
       [ 141,  136, 6366, ..., 2334,  224,  439],
       [3089, 8005,  349, ...,  387,  907,  535],
       [ 141,  154,    0, ..., 1293,  377,   47]]), array([[  23,  908,  274, ..., 4527,   23,  100],
       [1137, 1364,   85, ...,    6,   55,   50],
       [ 241,  100,  990, ...,  253,   23,  101],
       ...,
       [  51, 1803, 2495, ...,   30,   36,    0],
       [ 223, 8243,  270, ...,   12, 2681,  224],
       [2418,   29,   50, ...,  274, 7667,    6]]), array([[ 204,   18, 1157, ...,   12, 2483, 2327],
       [1840, 1475, 1808, ...,  735,  317,   11],
       [ 417, 8243,   33, ...,  610,  107,  526],
       ...,
       [  23, 3131,    0, ...,  272, 1523, 1731],
       [ 937,   10,  118, ...,  638, 2475,  761],
       [  13,  417, 6665, ..., 1157, 2532,   88]]), array([[ 272,    0,    0, ...,   12, 6220,    0],
       [  12,    0,  609, ...,   88,  432,   42],
       [   0,  100,  179, ...,  136, 4602, 3596],
       ...,
       [ 228, 2617, 1023, ...,   18, 1676,  107],
       [   0, 4018, 2505, ..., 8250,  815, 3208],
       [ 101, 2380, 8248, ...,    0,    0, 3049]]), array([[  85,   68,   14, ...,   12, 2373,   10],
       [3208,  111,   27, ...,  522,  101, 2380],
       [8196,   12, 1685, ...,  118,   10,  305],
       ...,
       [  55,  873, 8256, ..., 2664,  761, 6435],
       [2787,   10, 3436, ...,  160,  735,    0],
       [  61,    0,  890, ...,   14,    0,   26]]), array([[  20,   12,  245, ...,  359, 5559,  150],
       [ 967,   20,    0, ...,   23, 1869, 1422],
       [8248,  296,   12, ...,   12, 6274, 1218],
       ...,
       [3285,   23,  227, ...,   82,    0,    0],
       [ 201,   50,   26, ..., 3737, 3404,  227],
       [1163,   10,  118, ...,  245, 2451, 2354]]), array([[1297,  201,   47, ...,   23,    0,   23],
       [  12,    0, 1293, ..., 4601,  954,  241],
       [  33,    0,  107, ...,  761, 1112,  111],
       ...,
       [2255, 8260,   23, ...,  272, 4907,  118],
       [ 349, 2709,  593, ...,  115,   48, 6099],
       [ 542,    0,    0, ...,   47,  148,  224]]), array([[1490, 3819, 3235, ...,  480, 6675,  101],
       [   0, 1346,   10, ...,  100,  179,   12],
       [1238,   61,    0, ..., 2404, 8260,   97],
       ...,
       [8259,  100, 2836, ..., 8259, 1004,   48],
       [  55,   17,  228, ...,   23,    0,  266],
       [  55,  815, 2144, ..., 3235,   12, 3177]]), array([[  76,   88,   20, ...,  526,  346,  114],
       [  18, 1605, 2629, ...,   12, 1184,   18],
       [1254,    2,    0, ...,  227,  272,    0],
       ...,
       [   0,    0, 6818, ...,   29,   62,    0],
       [  53,  870,  105, ...,    0, 2411, 2599],
       [ 159,    0,  100, ...,   20, 2019,   50]]), array([[1805, 2292,   23, ...,   18, 4918,   23],
       [3522,  590,  287, ...,  261,  772,  100],
       [1307, 1295,   29, ..., 2411, 2599,  159],
       ...,
       [   0, 2725,   81, ..., 6204,    0,   12],
       [6722,  101,  557, ...,   29, 2020,  643],
       [  23,  997,   29, ...,   23,   31,   42]]), array([[ 568,   67,   42, ...,   39,  322,    0],
       [6479,  638, 5557, ...,   14,  284,  123],
       [  39,  223, 1621, ...,  554,  107,  284],
       ...,
       [  16,    0,    7, ...,    0,  178,    0],
       [  29,  830,  183, ...,   29,  101,  460],
       [5249,   58,   21, ...,   42, 2107,   51]]), array([[ 900,  663,    0, ...,   21,  353,   23],
       [   0,   85,  798, ...,    0,   29,    0],
       [1399,   23, 1749, ...,    2,  212, 1313],
       ...,
       [ 124,    0, 1310, ...,  449,   51, 2659],
       [  85, 1063, 7458, ..., 2411, 2599,  159],
       [   0, 3939,   14, ...,    0,  618,   81]]), array([[ 437,    2, 8027, ..., 1234, 1996,   29],
       [1576,    0, 1027, ..., 4199, 1945,   42],
       [   0,    0,    0, ..., 1811, 4815,   85],
       ...,
       [ 465,  224,    0, ...,    0, 2816,  224],
       [ 568,  343,   51, ..., 5325,   85,   42],
       [3228,    0,   17, ..., 8027, 1183,    0]]), array([[ 842,    0,   51, ...,   10,    0,  295],
       [   0,    0,   85, ...,    0,   26,   81],
       [7032,  111, 4080, ...,  667,   10, 4048],
       ...,
       [  29,   18, 4686, ..., 4985,   12, 1132],
       [ 821,    0,    0, ...,    0,   10,   12],
       [1530, 2587, 3455, ...,   51, 2207,  479]]), array([[  42, 1239,   23, ..., 1207, 2503,   67],
       [  12, 1232, 1464, ...,   21,  483, 2204],
       [7592,   26, 2854, ...,  112,  223,   94],
       ...,
       [ 709, 4670, 8283, ..., 1132,   18, 5172],
       [   0,    0, 5024, ...,  911,    0,  125],
       [2794,   12, 1334, ...,   23, 5007,   55]])]
100
Starting Epoch #1 of 10.
Starting Epoch #1 of 10.
Loss = 81098.984375
Loss = 81180.53125
Loss = 80351.40625
Loss = 79704.5390625
Loss = 79948.578125
Loss = 80354.546875
Loss = 79260.296875
Loss = 78573.0625
Loss = 78911.984375
Loss = 80180.671875
Loss = 78855.625
Loss = 78319.5859375
Loss = 77332.09375
Loss = 77847.59375
Loss = 77584.78125
Loss = 76921.0
Loss = 78775.859375
Loss = 77531.140625
Loss = 76006.703125
Loss = 77455.3125
Loss = 78466.6796875
Loss = 77259.4140625
Loss = 76701.046875
Loss = 75929.0546875
Loss = 78008.4453125
Loss = 76219.7109375
Loss = 75837.0546875
Loss = 76074.515625
Loss = 76193.3984375
Loss = 76968.234375
Loss = 75572.28125
Loss = 77019.40625
Loss = 76605.140625
Loss = 76536.4921875
Loss = 73645.078125
Loss = 74703.46875
Loss = 75555.125
Loss = 76150.734375
Loss = 77419.984375
Loss = 74360.21875
Loss = 75860.96875
Loss = 77016.3515625
Loss = 74354.15625
Loss = 74569.6640625
Loss = 76349.109375
Loss = 74382.7578125
Loss = 75181.0859375
Loss = 77506.234375
Loss = 75453.671875
Loss = 75727.734375
Loss = 77064.7890625
Loss = 72568.5390625
Loss = 74763.953125
Loss = 75226.40625
Loss = 71977.1953125
Loss = 73886.984375
Loss = 74310.609375
Loss = 73779.8125
Loss = 73041.4453125
Loss = 73372.890625
Loss = 74170.0234375
Loss = 71625.921875
Loss = 72991.03125
Loss = 74306.0859375
Loss = 71995.15625
Loss = 73914.84375
Loss = 71469.625
Loss = 72108.8515625
Loss = 72035.921875
Loss = 73183.25
Loss = 71817.359375
Loss = 69037.375
Loss = 72115.546875
Loss = 69831.5703125
Loss = 71537.0625
Loss = 70760.640625
Loss = 73167.8515625
Loss = 70633.375
Loss = 69261.3125
Loss = 70935.3359375
Loss = 69500.84375
Loss = 70282.9921875
Loss = 70884.2890625
Loss = 69378.609375
Loss = 71130.8671875
Loss = 70429.234375
Loss = 69399.3515625
Loss = 68076.734375
Loss = 69318.328125
Loss = 70698.0859375
Loss = 70584.546875
Loss = 68269.203125
Loss = 68969.046875
Loss = 69411.796875
Loss = 69764.6171875
Loss = 67131.671875
Loss = 68557.546875
Loss = 69400.078125
Loss = 67843.234375
Loss = 69779.5625
Loss = 68201.140625
Loss = 68141.703125
Loss = 68617.15625
Loss = 67021.828125
Loss = 67374.578125
Loss = 69211.2109375
Loss = 67953.3125
Loss = 68849.2890625
Loss = 67434.2890625
Loss = 67225.8046875
Loss = 68796.0234375
Loss = 69178.484375
Loss = 66623.3125
Loss = 67912.3828125
Loss = 65492.421875
Loss = 66840.3203125
Loss = 64568.546875
Loss = 66845.578125
Loss = 67550.171875
Loss = 65334.90625
Loss = 69456.109375
Loss = 66369.578125
Loss = 67000.546875
Loss = 66022.421875
Loss = 65443.734375
Loss = 65480.77734375
Loss = 65962.109375
Loss = 65555.0
Loss = 65949.8359375
Loss = 66327.5234375
Loss = 64681.421875
Loss = 66977.078125
Loss = 64438.5234375
Loss = 66098.21875
Loss = 63803.78125
Loss = 65793.6953125
Loss = 65380.0078125
Loss = 65746.765625
Loss = 64206.0
Loss = 63481.68359375
Loss = 63608.44921875
Loss = 63262.3984375
Loss = 63960.03125
Loss = 64243.0
Loss = 63339.640625
Loss = 63940.125
Loss = 66036.453125
Loss = 64497.27734375
Loss = 63655.3203125
Loss = 63204.328125
Loss = 64851.65625
Loss = 62084.9765625
Loss = 65347.53125
Loss = 63119.6640625
Loss = 62860.01171875
Loss = 65611.6328125
Loss = 65526.88671875
Loss = 62037.4375
Loss = 63851.8828125
Loss = 63637.796875
Loss = 63759.76953125
Loss = 64300.625
Loss = 63603.41796875
Loss = 62134.31640625
Loss = 63022.2265625
Loss = 63752.0390625
Loss = 61122.546875
Loss = 61965.40625
Loss = 60764.82421875
Loss = 64073.7578125
Loss = 63792.375
Loss = 62891.09375
Loss = 60925.6484375
Loss = 64671.63671875
Loss = 59842.62890625
Loss = 59981.17578125
Loss = 62107.078125
Loss = 62110.8828125
Loss = 62046.98828125
Loss = 62419.12109375
Loss = 62055.87109375
Loss = 62066.6171875
Loss = 61205.171875
Loss = 61622.203125
Loss = 61501.8828125
Loss = 58952.484375
Loss = 61216.21875
Starting Epoch #2 of 10.
Starting Epoch #2 of 10.
Loss = 60550.77734375
Loss = 63197.34765625
Loss = 61630.73828125
Loss = 60523.0625
Loss = 61516.5703125
Loss = 60744.14453125
Loss = 63256.34375
Loss = 62374.984375
Loss = 63169.14453125
Loss = 60239.390625
Loss = 60028.96484375
Loss = 62405.5390625
Loss = 61760.3984375
Loss = 60122.90625
Loss = 60863.37890625
Loss = 61547.921875
Loss = 62740.82421875
Loss = 61204.44921875
Loss = 60851.01953125
Loss = 60409.74609375
Loss = 59250.19140625
Loss = 58761.71875
Loss = 60045.171875
Loss = 62712.6796875
Loss = 61905.8515625
Loss = 57789.0703125
Loss = 62993.4921875
Loss = 59210.3203125
Loss = 60733.25390625
Loss = 59171.12890625
Loss = 60850.58984375
Loss = 61184.421875
Loss = 59512.5390625
Loss = 59493.109375
Loss = 60192.84375
Loss = 60465.046875
Loss = 60522.546875
Loss = 59970.421875
Loss = 58810.56640625
Loss = 61351.69140625
Loss = 60116.1875
Loss = 58978.234375
Loss = 60970.50390625
Loss = 60321.72265625
Loss = 59886.28515625
Loss = 59257.40625
Loss = 61238.078125
Loss = 56919.765625
Loss = 59880.12890625
Loss = 58565.046875
Loss = 59115.0
Loss = 58941.6796875
Loss = 58800.2734375
Loss = 59756.3984375
Loss = 62329.109375
Loss = 59229.76171875
Loss = 58829.36328125
Loss = 57196.48046875
Loss = 58585.69921875
Loss = 60320.078125
Loss = 57690.3984375
Loss = 58986.15625
Loss = 58526.4296875
Loss = 58651.109375
Loss = 57597.10546875
Loss = 58310.78515625
Loss = 60349.8984375
Loss = 59248.53515625
Loss = 58023.19921875
Loss = 59823.71875
Loss = 57560.2734375
Loss = 59080.078125
Loss = 59432.2109375
Loss = 60306.30078125
Loss = 59421.1953125
Loss = 58516.19140625
Loss = 58294.8671875
Loss = 57395.6328125
Loss = 58768.765625
Loss = 57290.953125
Loss = 57654.703125
Loss = 59026.16796875
Loss = 60744.765625
Loss = 57593.0390625
Loss = 57652.96484375
Loss = 57553.0703125
Loss = 55487.3515625
Loss = 59146.83203125
Loss = 58195.546875
Loss = 58730.3046875
Loss = 57669.1875
Loss = 59337.6875
Loss = 59139.34765625
Loss = 58936.6953125
Loss = 57391.4765625
Loss = 55981.6328125
Loss = 59529.3828125
Loss = 56596.546875
Loss = 57961.2734375
Loss = 57909.90234375
Loss = 60623.2421875
Loss = 56057.07421875
Loss = 56576.7890625
Loss = 57942.421875
Loss = 57467.78125
Loss = 58330.5546875
Loss = 58019.484375
Loss = 58957.80078125
Loss = 59566.1484375
Loss = 57570.19921875
Loss = 58197.21875
Loss = 56574.8125
Loss = 57595.546875
Loss = 56660.19140625
Loss = 58525.75
Loss = 57299.07421875
Loss = 55168.15625
Loss = 57415.34375
Loss = 56837.3046875
Loss = 56895.796875
Loss = 58042.38671875
Loss = 58946.31640625
Loss = 57693.4765625
Loss = 56806.93359375
Loss = 57417.578125
Loss = 57929.96875
Loss = 59524.90234375
Loss = 57110.484375
Loss = 57799.8203125
Loss = 56435.58984375
Loss = 58468.296875
Loss = 58001.921875
Loss = 57819.70703125
Loss = 55635.1875
Loss = 58611.6171875
Loss = 54865.703125
Loss = 56247.01171875
Loss = 56466.046875
Loss = 56431.640625
Loss = 60904.3984375
Loss = 55187.390625
Loss = 59334.71484375
Loss = 58454.4375
Loss = 55550.13671875
Loss = 55649.6171875
Loss = 55281.3984375
Loss = 57161.1328125
Loss = 55268.69921875
Loss = 57084.09375
Loss = 56171.94921875
Loss = 57840.296875
Loss = 58110.06640625
Loss = 57268.80078125
Loss = 57993.984375
Loss = 57547.5
Loss = 58172.1875
Loss = 59432.66015625
Loss = 56699.84375
Loss = 57070.76953125
Loss = 56740.4453125
Loss = 53924.125
Loss = 56721.26953125
Loss = 57143.66796875
Loss = 57841.578125
Loss = 57704.4140625
Loss = 57060.03125
Loss = 56308.0078125
Loss = 56424.4296875
Loss = 57852.8203125
Loss = 56771.2734375
Loss = 57294.9453125
Loss = 55029.3125
Loss = 56993.06640625
Loss = 55670.46875
Loss = 54640.92578125
Loss = 57987.265625
Loss = 55636.765625
Loss = 56342.41015625
Loss = 58860.7265625
Loss = 59617.67578125
Loss = 54880.6015625
Loss = 57033.20703125
Loss = 54495.80078125
Loss = 56116.078125
Loss = 55666.7265625
Loss = 57006.375
Loss = 55246.89453125
Starting Epoch #3 of 10.
Starting Epoch #3 of 10.
Loss = 54832.5390625
Loss = 57242.0546875
Loss = 54945.7890625
Loss = 56454.96484375
Loss = 55491.75
Loss = 57685.7421875
Loss = 57679.5546875
Loss = 57026.6875
Loss = 55010.328125
Loss = 54570.83984375
Loss = 55978.0625
Loss = 55255.69921875
Loss = 58520.44140625
Loss = 54988.078125
Loss = 54094.93359375
Loss = 56261.34765625
Loss = 55888.88671875
Loss = 55713.5625
Loss = 55415.46484375
Loss = 53849.984375
Loss = 57115.71484375
Loss = 55703.859375
Loss = 55598.078125
Loss = 52752.1015625
Loss = 54330.453125
Loss = 57418.35546875
Loss = 56646.828125
Loss = 56510.78515625
Loss = 53993.79296875
Loss = 54210.1875
Loss = 57508.94921875
Loss = 54221.5703125
Loss = 54536.984375
Loss = 55501.4765625
Loss = 52981.484375
Loss = 54167.0546875
Loss = 55613.9921875
Loss = 56212.50390625
Loss = 56435.06640625
Loss = 55666.4921875
Loss = 55758.11328125
Loss = 56558.4453125
Loss = 53315.08984375
Loss = 57163.9296875
Loss = 56394.875
Loss = 55253.5703125
Loss = 56303.83203125
Loss = 57250.140625
Loss = 54596.8203125
Loss = 55284.96875
Loss = 55799.078125
Loss = 55866.609375
Loss = 56116.6484375
Loss = 54497.34375
Loss = 56244.234375
Loss = 57297.3515625
Loss = 56504.0546875
Loss = 56777.8125
Loss = 54789.609375
Loss = 56526.91015625
Loss = 54752.234375
Loss = 54996.67578125
Loss = 54196.4375
Loss = 55030.1953125
Loss = 56163.34765625
Loss = 52373.70703125
Loss = 55276.53125
Loss = 55867.9765625
Loss = 54526.82421875
Loss = 55567.7890625
Loss = 56148.09375
Loss = 55454.140625
Loss = 56226.359375
Loss = 58379.703125
Loss = 51589.01171875
Loss = 56306.76171875
Loss = 54396.48046875
Loss = 55606.9375
Loss = 56852.453125
Loss = 55127.53125
Loss = 56918.08203125
Loss = 58164.3828125
Loss = 56190.375
Loss = 55035.90234375
Loss = 56102.546875
Loss = 58799.515625
Loss = 53798.453125
Loss = 56090.8671875
Loss = 57098.90625
Loss = 56790.49609375
Loss = 57276.5
Loss = 55161.8984375
Loss = 55748.94140625
Loss = 56886.359375
Loss = 55976.06640625
Loss = 56474.6953125
Loss = 57172.11328125
Loss = 57489.1796875
Loss = 55662.88671875
Loss = 56499.04296875
Loss = 54898.9140625
Loss = 55851.66796875
Loss = 57744.5
Loss = 57498.734375
Loss = 53738.62109375
Loss = 56447.734375
Loss = 55054.0
Loss = 58149.828125
Loss = 56039.15625
Loss = 55978.28125
Loss = 57401.65234375
Loss = 54980.09765625
Loss = 55212.8828125
Loss = 55220.80078125
Loss = 56376.7734375
Loss = 56255.8515625
Loss = 56045.91796875
Loss = 55556.6953125
Loss = 54351.65234375
Loss = 54422.328125
Loss = 54931.21875
Loss = 57538.796875
Loss = 54976.03515625
Loss = 56647.34375
Loss = 55737.2265625
Loss = 56125.8828125
Loss = 56390.359375
Loss = 53276.5
Loss = 58380.3125
Loss = 56729.96875
Loss = 54769.953125
Loss = 54383.7734375
Loss = 59670.40625
Loss = 58494.1171875
Loss = 55373.59765625
Loss = 56135.046875
Loss = 53299.4296875
Loss = 54846.796875
Loss = 54969.171875
Loss = 53109.29296875
Loss = 55727.3671875
Loss = 54603.39453125
Loss = 52828.12890625
Loss = 55653.375
Loss = 54395.6640625
Loss = 57328.453125
Loss = 54979.4140625
Loss = 53913.53515625
Loss = 55570.5078125
Loss = 54014.2109375
Loss = 53944.34765625
Loss = 56544.40234375
Loss = 55359.671875
Loss = 53563.6640625
Loss = 54174.296875
Loss = 54808.9921875
Loss = 55330.90625
Loss = 57554.3515625
Loss = 54573.671875
Loss = 56755.84375
Loss = 54055.46875
Loss = 55433.8671875
Loss = 56137.6484375
Loss = 54998.34375
Loss = 54972.56640625
Loss = 55103.453125
Loss = 57389.6015625
Loss = 53200.734375
Loss = 56546.21875
Loss = 54725.9765625
Loss = 56152.9609375
Loss = 57556.046875
Loss = 55812.05859375
Loss = 57260.8203125
Loss = 56526.703125
Loss = 55697.34765625
Loss = 54230.53125
Loss = 55029.015625
Loss = 54311.25
Loss = 53971.703125
Loss = 58581.9140625
Loss = 58260.7109375
Loss = 53426.67578125
Loss = 55191.4921875
Loss = 55870.71875
Loss = 53401.75390625
Loss = 60144.89453125
Starting Epoch #4 of 10.
Starting Epoch #4 of 10.
Loss = 54697.4921875
Loss = 53166.28125
Loss = 57147.0234375
Loss = 53765.1015625
Loss = 54663.1171875
Loss = 55549.3984375
Loss = 53301.203125
Loss = 52938.77734375
Loss = 56922.93359375
Loss = 55386.38671875
Loss = 55236.390625
Loss = 53883.36328125
Loss = 55124.03125
Loss = 54284.375
Loss = 52245.3515625
Loss = 57153.15234375
Loss = 55164.80859375
Loss = 55701.078125
Loss = 53617.015625
Loss = 54338.5703125
Loss = 54582.0234375
Loss = 56035.625
Loss = 56997.4375
Loss = 55553.296875
Loss = 54554.27734375
Loss = 57123.7109375
Loss = 54947.6484375
Loss = 54389.8984375
Loss = 53210.890625
Loss = 56396.4140625
Loss = 52742.1484375
Loss = 56538.9296875
Loss = 54346.60546875
Loss = 53655.37109375
Loss = 56065.875
Loss = 54136.0546875
Loss = 53726.64453125
Loss = 53912.01171875
Loss = 55782.8671875
Loss = 54445.41796875
Loss = 53866.546875
Loss = 56639.7890625
Loss = 52987.7578125
Loss = 53224.18359375
Loss = 54507.0703125
Loss = 55547.8359375
Loss = 53154.9140625
Loss = 54142.7265625
Loss = 55100.9765625
Loss = 55994.2890625
Loss = 54834.41796875
Loss = 54226.57421875
Loss = 55902.3203125
Loss = 53222.26171875
Loss = 54969.2421875
Loss = 55081.9375
Loss = 54184.58984375
Loss = 53388.9765625
Loss = 55100.7578125
Loss = 53409.0625
Loss = 53783.8203125
Loss = 54421.9453125
Loss = 54927.41796875
Loss = 53428.40234375
Loss = 53265.984375
Loss = 52574.75
Loss = 54699.3359375
Loss = 51449.35546875
Loss = 53570.3984375
Loss = 55844.7421875
Loss = 53249.8359375
Loss = 55259.171875
Loss = 57307.15625
Loss = 54738.7734375
Loss = 54266.92578125
Loss = 55526.41796875
Loss = 52460.609375
Loss = 53912.0078125
Loss = 59854.1484375
Loss = 55017.421875
Loss = 52837.08984375
Loss = 54300.1796875
Loss = 57166.265625
Loss = 53243.62109375
Loss = 54040.9609375
Loss = 56335.5078125
Loss = 51312.796875
Loss = 54300.984375
Loss = 56139.5859375
Loss = 54279.0390625
Loss = 55231.4921875
Loss = 54174.15234375
Loss = 55142.3203125
Loss = 52980.36328125
Loss = 54575.734375
Loss = 53259.75
Loss = 55637.9609375
Loss = 54524.98046875
Loss = 54179.56640625
Loss = 56508.96875
Loss = 55366.4296875
Loss = 53818.046875
Loss = 54914.75
Loss = 52658.546875
Loss = 53438.61328125
Loss = 55329.37890625
Loss = 52912.3828125
Loss = 56770.3359375
Loss = 55428.8125
Loss = 53393.9453125
Loss = 56115.29296875
Loss = 52442.8125
Loss = 53749.8203125
Loss = 54970.2734375
Loss = 55200.015625
Loss = 54427.578125
Loss = 56928.8671875
Loss = 53018.36328125
Loss = 53938.66015625
Loss = 54454.7421875
Loss = 54397.4765625
Loss = 54086.7734375
Loss = 56550.31640625
Loss = 53363.7890625
Loss = 54317.3984375
Loss = 54283.71875
Loss = 53295.5078125
Loss = 55348.8828125
Loss = 54062.8046875
Loss = 54739.5234375
Loss = 54334.9296875
Loss = 55066.65234375
Loss = 56276.1640625
Loss = 54440.2734375
Loss = 54998.7109375
Loss = 54136.4296875
Loss = 54944.0859375
Loss = 54571.84375
Loss = 53747.90234375
Loss = 51946.890625
Loss = 55788.859375
Loss = 53652.0703125
Loss = 53597.3125
Loss = 55053.32421875
Loss = 54593.359375
Loss = 56262.3046875
Loss = 57542.34765625
Loss = 49972.828125
Loss = 53849.8046875
Loss = 53441.7421875
Loss = 50783.7421875
Loss = 55668.41015625
Loss = 52845.2421875
Loss = 53474.28125
Loss = 57110.203125
Loss = 54667.0078125
Loss = 51688.2734375
Loss = 55204.796875
Loss = 58078.0859375
Loss = 50728.4921875
Loss = 54337.8984375
Loss = 54630.43359375
Loss = 53161.8125
Loss = 54054.4453125
Loss = 59345.953125
Loss = 52272.45703125
Loss = 52680.28515625
Loss = 54055.0625
Loss = 54534.9375
Loss = 52689.171875
Loss = 55083.0390625
Loss = 54810.6484375
Loss = 55782.60546875
Loss = 54589.0078125
Loss = 54731.28125
Loss = 53594.25
Loss = 52743.26953125
Loss = 53556.5703125
Loss = 51867.484375
Loss = 52709.0625
Loss = 54434.6953125
Loss = 52610.7421875
Loss = 51941.3828125
Loss = 55181.1171875
Loss = 53693.60546875
Loss = 55557.90234375
Loss = 53441.671875
Starting Epoch #5 of 10.
Starting Epoch #5 of 10.
Loss = 54679.625
Loss = 52066.78125
Loss = 53593.85546875
Loss = 53364.55078125
Loss = 54205.546875
Loss = 53458.8984375
Loss = 53807.65625
Loss = 55737.3828125
Loss = 54833.05078125
Loss = 52939.59375
Loss = 53536.14453125
Loss = 56381.890625
Loss = 54165.7734375
Loss = 52967.22265625
Loss = 54501.65625
Loss = 53004.765625
Loss = 54442.7890625
Loss = 55666.6171875
Loss = 54789.38671875
Loss = 52474.046875
Loss = 54564.359375
Loss = 56125.3671875
Loss = 52603.0546875
Loss = 52833.07421875
Loss = 55563.5703125
Loss = 55523.9921875
Loss = 53668.890625
Loss = 53279.125
Loss = 55895.6171875
Loss = 52954.8203125
Loss = 53911.640625
Loss = 54966.546875
Loss = 53295.203125
Loss = 59564.984375
Loss = 51600.51953125
Loss = 57637.796875
Loss = 54394.23046875
Loss = 54142.890625
Loss = 51564.0
Loss = 54440.171875
Loss = 54302.6796875
Loss = 54683.43359375
Loss = 57157.35546875
Loss = 50467.06640625
Loss = 54359.0078125
Loss = 53891.81640625
Loss = 52171.15234375
Loss = 53897.0546875
Loss = 55656.140625
Loss = 52587.8515625
Loss = 51378.5546875
Loss = 53427.34375
Loss = 52155.51953125
Loss = 54527.265625
Loss = 56823.5390625
Loss = 52771.8828125
Loss = 54970.2890625
Loss = 53811.52734375
Loss = 56805.9375
Loss = 52679.90234375
Loss = 52694.58203125
Loss = 55531.0703125
Loss = 53255.4453125
Loss = 54921.9609375
Loss = 53672.3515625
Loss = 53551.8828125
Loss = 54671.56640625
Loss = 54423.453125
Loss = 54202.55078125
Loss = 56231.40234375
Loss = 56060.2890625
Loss = 54288.7109375
Loss = 53392.75
Loss = 56624.3984375
Loss = 55706.8984375
Loss = 54824.3046875
Loss = 50735.2421875
Loss = 53531.1953125
Loss = 52110.9296875
Loss = 52018.984375
Loss = 49416.65234375
Loss = 53465.2890625
Loss = 52935.09765625
Loss = 54984.81640625
Loss = 55107.13671875
Loss = 55989.578125
Loss = 52326.40625
Loss = 53732.4375
Loss = 54866.71875
Loss = 53179.11328125
Loss = 53955.28125
Loss = 53635.328125
Loss = 52934.3125
Loss = 52266.49609375
Loss = 53491.3671875
Loss = 55620.79296875
Loss = 52600.94921875
Loss = 55062.8828125
Loss = 52489.59375
Loss = 54171.578125
Loss = 54670.2578125
Loss = 53587.84375
Loss = 54121.39453125
Loss = 52442.49609375
Loss = 53617.234375
Loss = 52988.296875
Loss = 51969.2421875
Loss = 53944.41015625
Loss = 52084.90625
Loss = 57046.04296875
Loss = 52567.66015625
Loss = 52347.328125
Loss = 52594.734375
Loss = 51445.08984375
Loss = 54444.46875
Loss = 52364.734375
Loss = 54349.14453125
Loss = 53411.9453125
Loss = 53580.5
Loss = 53908.8515625
Loss = 53389.93359375
Loss = 53183.3828125
Loss = 52428.9140625
Loss = 55126.3203125
Loss = 51615.828125
Loss = 52359.1328125
Loss = 54306.78125
Loss = 53782.84375
Loss = 54810.1796875
Loss = 54878.26953125
Loss = 54283.703125
Loss = 52690.8359375
Loss = 53762.52734375
Loss = 52683.328125
Loss = 54997.9375
Loss = 54784.80859375
Loss = 54614.9375
Loss = 53243.70703125
Loss = 53927.11328125
Loss = 55592.8984375
Loss = 53788.8828125
Loss = 52957.6953125
Loss = 53865.953125
Loss = 55225.3125
Loss = 55364.59375
Loss = 52975.92578125
Loss = 52005.2109375
Loss = 56812.1015625
Loss = 55761.921875
Loss = 53892.8671875
Loss = 52473.55078125
Loss = 56544.171875
Loss = 53238.41015625
Loss = 55896.64453125
Loss = 54004.73828125
Loss = 53474.76953125
Loss = 57013.015625
Loss = 53036.8125
Loss = 54672.55859375
Loss = 54027.67578125
Loss = 52893.984375
Loss = 51211.10546875
Loss = 54283.81640625
Loss = 55071.13671875
Loss = 52795.33203125
Loss = 53428.43359375
Loss = 53529.44140625
Loss = 54273.2109375
Loss = 52282.7578125
Loss = 53061.78125
Loss = 58840.359375
Loss = 55040.265625
Loss = 54869.171875
Loss = 53826.87890625
Loss = 54351.30078125
Loss = 53306.671875
Loss = 54457.0859375
Loss = 55186.7734375
Loss = 52886.828125
Loss = 54495.59375
Loss = 54961.88671875
Loss = 50325.09375
Loss = 54556.64453125
Loss = 52205.1640625
Loss = 54658.390625
Loss = 53860.328125
Loss = 55662.05859375
Starting Epoch #6 of 10.
Starting Epoch #6 of 10.
Loss = 52422.40625
Loss = 54842.9296875
Loss = 54318.0390625
Loss = 53857.1640625
Loss = 53563.2109375
Loss = 56636.41796875
Loss = 54662.7578125
Loss = 54068.4609375
Loss = 53829.99609375
Loss = 52096.9609375
Loss = 56015.59375
Loss = 58716.890625
Loss = 53879.75
Loss = 55158.6171875
Loss = 54134.9921875
Loss = 52310.890625
Loss = 53715.0
Loss = 56471.60546875
Loss = 53382.09375
Loss = 54789.78515625
Loss = 54119.6484375
Loss = 52743.6484375
Loss = 51915.078125
Loss = 54229.1328125
Loss = 54958.2890625
Loss = 55265.0625
Loss = 51375.09375
Loss = 57323.8203125
Loss = 51729.3828125
Loss = 52553.796875
Loss = 54155.8203125
Loss = 54253.8046875
Loss = 53450.09375
Loss = 52056.7421875
Loss = 55250.52734375
Loss = 52213.9375
Loss = 52970.59375
Loss = 51863.9296875
Loss = 54470.1640625
Loss = 54276.4609375
Loss = 53967.5078125
Loss = 52577.21484375
Loss = 52440.2890625
Loss = 53478.85546875
Loss = 53427.05078125
Loss = 55074.5234375
Loss = 52226.484375
Loss = 53052.3828125
Loss = 55426.140625
Loss = 53071.40625
Loss = 53508.75
Loss = 50273.4921875
Loss = 55481.1796875
Loss = 58994.6171875
Loss = 52218.06640625
Loss = 56637.703125
Loss = 53976.28515625
Loss = 54530.0859375
Loss = 52037.5625
Loss = 51747.07421875
Loss = 54112.61328125
Loss = 56949.390625
Loss = 54189.8359375
Loss = 52969.7734375
Loss = 54641.3515625
Loss = 53262.203125
Loss = 55310.6953125
Loss = 52841.328125
Loss = 56735.0390625
Loss = 52693.62890625
Loss = 51095.81640625
Loss = 53071.84375
Loss = 53037.8671875
Loss = 52049.140625
Loss = 54750.04296875
Loss = 54331.85546875
Loss = 55290.4921875
Loss = 51559.03515625
Loss = 53826.8046875
Loss = 51665.234375
Loss = 53494.5234375
Loss = 54663.66015625
Loss = 53027.8125
Loss = 52957.125
Loss = 53760.46875
Loss = 53066.8359375
Loss = 52169.484375
Loss = 54848.62890625
Loss = 55552.296875
Loss = 52763.0390625
Loss = 53468.71875
Loss = 52855.82421875
Loss = 51267.6328125
Loss = 51944.5234375
Loss = 54390.9296875
Loss = 52331.27734375
Loss = 53589.90625
Loss = 54971.67578125
Loss = 52308.3828125
Loss = 56120.609375
Loss = 53048.0078125
Loss = 54095.53125
Loss = 54898.15625
Loss = 53068.30859375
Loss = 54765.84375
Loss = 53663.73046875
Loss = 54376.078125
Loss = 52783.30859375
Loss = 50732.37109375
Loss = 52819.6328125
Loss = 52244.703125
Loss = 53387.0390625
Loss = 51289.5078125
Loss = 54336.24609375
Loss = 53358.50390625
Loss = 52833.90625
Loss = 53351.421875
Loss = 51382.93359375
Loss = 53259.9765625
Loss = 51425.6953125
Loss = 53691.71875
Loss = 51956.6484375
Loss = 54474.9609375
Loss = 51777.359375
Loss = 52534.484375
Loss = 55120.79296875
Loss = 56427.49609375
Loss = 49834.8515625
Loss = 52720.2421875
Loss = 52996.390625
Loss = 53352.65625
Loss = 56220.0703125
Loss = 54383.75390625
Loss = 53786.75
Loss = 53774.578125
Loss = 54261.90625
Loss = 53018.6171875
Loss = 53157.921875
Loss = 55209.38671875
Loss = 53987.1328125
Loss = 53675.96875
Loss = 51634.3046875
Loss = 52945.34765625
Loss = 51772.7265625
Loss = 52996.48828125
Loss = 51563.46484375
Loss = 51907.61328125
Loss = 48542.30859375
Loss = 52278.4765625
Loss = 54232.203125
Loss = 54559.8203125
Loss = 55995.56640625
Loss = 53855.91015625
Loss = 54470.609375
Loss = 52744.54296875
Loss = 51498.859375
Loss = 54044.109375
Loss = 52392.59375
Loss = 54524.3984375
Loss = 52496.53125
Loss = 53518.7890625
Loss = 51358.3046875
Loss = 52769.26953125
Loss = 52063.171875
Loss = 49847.2734375
Loss = 55559.21484375
Loss = 53553.234375
Loss = 53521.8046875
Loss = 52537.7890625
Loss = 54366.8984375
Loss = 54152.80078125
Loss = 54598.03125
Loss = 54877.2265625
Loss = 54545.17578125
Loss = 52109.4296875
Loss = 54485.8984375
Loss = 53699.73828125
Loss = 53281.046875
Loss = 55623.55078125
Loss = 54146.7421875
Loss = 55287.0078125
Loss = 53237.77734375
Loss = 52389.68359375
Loss = 52748.18359375
Loss = 53570.18359375
Loss = 55381.3828125
Loss = 52299.4375
Starting Epoch #7 of 10.
Starting Epoch #7 of 10.
Loss = 54710.125
Loss = 53957.42578125
Loss = 54332.75
Loss = 53607.16015625
Loss = 51379.91015625
Loss = 53437.9609375
Loss = 54216.83203125
Loss = 51606.0859375
Loss = 50531.0234375
Loss = 53719.125
Loss = 52452.9921875
Loss = 52516.6171875
Loss = 53106.1328125
Loss = 51655.10546875
Loss = 55086.53125
Loss = 52115.3125
Loss = 52610.37890625
Loss = 49914.98046875
Loss = 53965.35546875
Loss = 52422.5859375
Loss = 53346.40625
Loss = 52065.37890625
Loss = 53907.828125
Loss = 53606.96875
Loss = 53979.8984375
Loss = 53165.70703125
Loss = 53105.9765625
Loss = 54294.80078125
Loss = 52429.52734375
Loss = 53334.0703125
Loss = 56217.03515625
Loss = 52112.2890625
Loss = 55110.2890625
Loss = 51407.359375
Loss = 52939.39453125
Loss = 52579.42578125
Loss = 52314.73828125
Loss = 51935.578125
Loss = 58702.73046875
Loss = 53607.921875
Loss = 53090.6171875
Loss = 53013.19140625
Loss = 54587.61328125
Loss = 54905.44140625
Loss = 56791.07421875
Loss = 55072.2109375
Loss = 52672.0625
Loss = 51406.8984375
Loss = 51750.390625
Loss = 52836.734375
Loss = 52214.515625
Loss = 52777.5
Loss = 51918.2734375
Loss = 54888.484375
Loss = 52692.69921875
Loss = 51385.171875
Loss = 52641.296875
Loss = 53661.21875
Loss = 58137.60546875
Loss = 52700.390625
Loss = 53027.515625
Loss = 51770.9609375
Loss = 51173.34375
Loss = 53246.1484375
Loss = 53728.3359375
Loss = 50343.921875
Loss = 52998.2734375
Loss = 53842.046875
Loss = 54479.4140625
Loss = 51262.57421875
Loss = 54498.7421875
Loss = 54456.140625
Loss = 55264.453125
Loss = 51533.1640625
Loss = 53616.79296875
Loss = 54273.32421875
Loss = 54737.55859375
Loss = 50654.765625
Loss = 53517.015625
Loss = 53797.265625
Loss = 53872.7265625
Loss = 54186.4453125
Loss = 54364.06640625
Loss = 55830.2109375
Loss = 49017.703125
Loss = 53245.8515625
Loss = 51865.921875
Loss = 52299.2109375
Loss = 53130.22265625
Loss = 53413.21484375
Loss = 54676.2109375
Loss = 52904.8359375
Loss = 54519.80859375
Loss = 54434.9609375
Loss = 52647.3046875
Loss = 51687.4453125
Loss = 52539.48828125
Loss = 51786.125
Loss = 52733.85546875
Loss = 53870.5625
Loss = 54655.4140625
Loss = 53946.09375
Loss = 51839.9921875
Loss = 52505.1015625
Loss = 53380.7421875
Loss = 50859.296875
Loss = 51304.62109375
Loss = 55078.9140625
Loss = 52839.390625
Loss = 51985.9609375
Loss = 55060.5
Loss = 53292.43359375
Loss = 53353.859375
Loss = 54359.0625
Loss = 53060.94921875
Loss = 51879.37109375
Loss = 53299.0234375
Loss = 54693.953125
Loss = 51010.25
Loss = 54097.6484375
Loss = 51559.5
Loss = 51259.4921875
Loss = 54221.5390625
Loss = 52225.3203125
Loss = 54693.859375
Loss = 51517.31640625
Loss = 54538.32421875
Loss = 52317.4140625
Loss = 53768.29296875
Loss = 54236.65625
Loss = 52244.421875
Loss = 53458.4453125
Loss = 52771.15625
Loss = 55135.90234375
Loss = 54213.15625
Loss = 56649.62890625
Loss = 55801.37890625
Loss = 53430.625
Loss = 51694.7265625
Loss = 52680.125
Loss = 54584.28125
Loss = 55200.421875
Loss = 52877.4296875
Loss = 55846.671875
Loss = 54595.203125
Loss = 55231.859375
Loss = 53214.3515625
Loss = 53574.46875
Loss = 57228.015625
Loss = 51843.890625
Loss = 53110.515625
Loss = 55867.3984375
Loss = 53324.20703125
Loss = 54303.0390625
Loss = 53771.37890625
Loss = 53366.609375
Loss = 51676.52734375
Loss = 53356.3203125
Loss = 53006.421875
Loss = 52738.4140625
Loss = 53673.34375
Loss = 54095.375
Loss = 52516.9375
Loss = 52847.9375
Loss = 52706.859375
Loss = 50636.01171875
Loss = 54177.046875
Loss = 54407.609375
Loss = 52906.3125
Loss = 55057.7109375
Loss = 52558.2109375
Loss = 55988.53515625
Loss = 56090.234375
Loss = 53709.5078125
Loss = 53965.1640625
Loss = 52412.91015625
Loss = 54718.578125
Loss = 52350.0703125
Loss = 53819.6796875
Loss = 51974.02734375
Loss = 54659.328125
Loss = 49719.0703125
Loss = 50641.171875
Loss = 55031.58203125
Loss = 54398.3828125
Loss = 52495.85546875
Loss = 56352.109375
Starting Epoch #8 of 10.
Starting Epoch #8 of 10.
Loss = 51212.7109375
Loss = 52707.734375
Loss = 49663.75
Loss = 55912.0078125
Loss = 51775.53125
Loss = 52920.84765625
Loss = 49984.3828125
Loss = 52833.26953125
Loss = 52246.609375
Loss = 53255.375
Loss = 53101.3671875
Loss = 51743.2890625
Loss = 55884.83203125
Loss = 51622.40625
Loss = 53094.1875
Loss = 51847.546875
Loss = 55116.9296875
Loss = 54647.44140625
Loss = 54135.078125
Loss = 51981.90625
Loss = 52906.4453125
Loss = 51241.953125
Loss = 51610.6875
Loss = 53747.140625
Loss = 52853.57421875
Loss = 54156.80078125
Loss = 53334.28125
Loss = 52300.03515625
Loss = 53396.3984375
Loss = 52524.90234375
Loss = 52914.3515625
Loss = 50751.6796875
Loss = 53696.6953125
Loss = 53159.203125
Loss = 53538.09375
Loss = 52490.9921875
Loss = 53028.8828125
Loss = 51368.4765625
Loss = 54527.109375
Loss = 51390.5546875
Loss = 50177.5234375
Loss = 53190.51953125
Loss = 55535.3828125
Loss = 53687.734375
Loss = 54770.40625
Loss = 51408.0625
Loss = 53772.6640625
Loss = 52120.8671875
Loss = 52658.765625
Loss = 54031.61328125
Loss = 52097.01171875
Loss = 52216.18359375
Loss = 53566.4296875
Loss = 52628.28125
Loss = 53321.56640625
Loss = 54583.7734375
Loss = 54949.0625
Loss = 51187.375
Loss = 51610.734375
Loss = 53662.9609375
Loss = 53062.03515625
Loss = 53991.70703125
Loss = 52432.15234375
Loss = 51323.8671875
Loss = 55288.671875
Loss = 53814.046875
Loss = 58023.5
Loss = 53657.296875
Loss = 53627.578125
Loss = 55954.109375
Loss = 52590.2734375
Loss = 54854.05078125
Loss = 53495.15234375
Loss = 53751.203125
Loss = 52117.08203125
Loss = 53757.6484375
Loss = 52247.81640625
Loss = 54243.4375
Loss = 55012.3203125
Loss = 56012.984375
Loss = 51886.921875
Loss = 58615.8828125
Loss = 54782.82421875
Loss = 55183.70703125
Loss = 54219.40234375
Loss = 52297.49609375
Loss = 52448.48828125
Loss = 51654.41015625
Loss = 53202.27734375
Loss = 54658.2421875
Loss = 54567.3125
Loss = 54406.05859375
Loss = 48909.3515625
Loss = 51471.1328125
Loss = 51635.0234375
Loss = 52156.10546875
Loss = 52335.78125
Loss = 53324.5625
Loss = 55161.1015625
Loss = 51948.03515625
Loss = 53192.91015625
Loss = 54388.7578125
Loss = 55179.65234375
Loss = 54085.6875
Loss = 52951.9453125
Loss = 53103.3125
Loss = 55355.8515625
Loss = 53079.91796875
Loss = 51311.125
Loss = 53470.375
Loss = 51393.91796875
Loss = 53370.34375
Loss = 53430.7578125
Loss = 57214.75390625
Loss = 55408.359375
Loss = 56245.15625
Loss = 51870.078125
Loss = 54636.5234375
Loss = 51524.75
Loss = 54930.56640625
Loss = 53995.22265625
Loss = 51392.453125
Loss = 49971.0
Loss = 55892.3125
Loss = 51011.41796875
Loss = 56159.9140625
Loss = 53028.4375
Loss = 54473.25
Loss = 52339.546875
Loss = 53850.359375
Loss = 55609.359375
Loss = 55024.171875
Loss = 53989.0078125
Loss = 54501.703125
Loss = 53255.96875
Loss = 52541.4765625
Loss = 53923.421875
Loss = 53712.21875
Loss = 54211.078125
Loss = 54005.3046875
Loss = 52764.69921875
Loss = 52888.85546875
Loss = 52717.33984375
Loss = 51542.2109375
Loss = 54504.578125
Loss = 53341.05859375
Loss = 53421.15234375
Loss = 53588.7421875
Loss = 53087.2265625
Loss = 57336.3984375
Loss = 55350.35546875
Loss = 52941.8125
Loss = 54244.703125
Loss = 56509.875
Loss = 52606.35546875
Loss = 52421.5
Loss = 53712.2734375
Loss = 52800.09765625
Loss = 54705.86328125
Loss = 56069.25
Loss = 52661.390625
Loss = 51391.6875
Loss = 55527.20703125
Loss = 56127.4296875
Loss = 53349.109375
Loss = 52839.4453125
Loss = 53393.5546875
Loss = 53697.68359375
Loss = 54188.96484375
Loss = 54211.68359375
Loss = 52183.5
Loss = 53392.46484375
Loss = 51551.5546875
Loss = 53628.4375
Loss = 53789.7109375
Loss = 52890.7265625
Loss = 55978.56640625
Loss = 52765.66796875
Loss = 50966.6875
Loss = 54504.0546875
Loss = 53986.8046875
Loss = 52957.28125
Loss = 54501.10546875
Loss = 51691.765625
Loss = 54527.84765625
Loss = 52952.671875
Loss = 54534.953125
Starting Epoch #9 of 10.
Starting Epoch #9 of 10.
Loss = 54142.3828125
Loss = 54958.07421875
Loss = 53849.89453125
Loss = 53301.375
Loss = 51594.3046875
Loss = 54798.34375
Loss = 53674.4609375
Loss = 52840.79296875
Loss = 53788.421875
Loss = 51445.83203125
Loss = 51070.3984375
Loss = 54439.73046875
Loss = 52972.1875
Loss = 53837.2265625
Loss = 55265.96484375
Loss = 54972.24609375
Loss = 54222.81640625
Loss = 52805.140625
Loss = 52691.6796875
Loss = 53184.29296875
Loss = 52778.4375
Loss = 53952.484375
Loss = 53450.2421875
Loss = 50910.3515625
Loss = 53520.140625
Loss = 54335.79296875
Loss = 53108.484375
Loss = 52581.90625
Loss = 53036.63671875
Loss = 52812.17578125
Loss = 54228.203125
Loss = 53812.58984375
Loss = 55957.0859375
Loss = 55329.99609375
Loss = 53025.07421875
Loss = 55234.421875
Loss = 53462.2421875
Loss = 52800.9921875
Loss = 54941.859375
Loss = 56058.65625
Loss = 51564.69921875
Loss = 51896.515625
Loss = 51525.1875
Loss = 51734.44921875
Loss = 52436.05859375
Loss = 52116.59765625
Loss = 54370.7734375
Loss = 54001.375
Loss = 55846.60546875
Loss = 49529.546875
Loss = 53560.65625
Loss = 51832.4921875
Loss = 52678.25390625
Loss = 51612.75
Loss = 52113.9140625
Loss = 55731.828125
Loss = 53198.9375
Loss = 48965.8671875
Loss = 52749.7890625
Loss = 54171.62109375
Loss = 56098.953125
Loss = 54589.94140625
Loss = 54494.3359375
Loss = 54394.015625
Loss = 55034.8359375
Loss = 51101.375
Loss = 57178.015625
Loss = 53551.72265625
Loss = 51633.328125
Loss = 54925.21875
Loss = 51288.4765625
Loss = 54034.1640625
Loss = 54421.984375
Loss = 52358.03515625
Loss = 53255.8984375
Loss = 51714.359375
Loss = 53482.1015625
Loss = 53947.390625
Loss = 54516.625
Loss = 52223.15625
Loss = 51443.3203125
Loss = 53081.80859375
Loss = 52444.015625
Loss = 51043.01953125
Loss = 52356.4765625
Loss = 54505.62890625
Loss = 53595.171875
Loss = 54010.3125
Loss = 52650.5390625
Loss = 52791.92578125
Loss = 51409.12890625
Loss = 55086.87109375
Loss = 53403.42578125
Loss = 54282.58203125
Loss = 53151.0390625
Loss = 53859.8359375
Loss = 49763.8203125
Loss = 52463.99609375
Loss = 51851.328125
Loss = 51818.73046875
Loss = 55621.15234375
Loss = 51470.9375
Loss = 52774.640625
Loss = 53584.6015625
Loss = 53630.328125
Loss = 52922.734375
Loss = 59114.078125
Loss = 52776.203125
Loss = 49960.578125
Loss = 52142.9765625
Loss = 51891.3671875
Loss = 52541.6328125
Loss = 54605.6875
Loss = 52285.1796875
Loss = 54381.05078125
Loss = 52750.83984375
Loss = 52333.3984375
Loss = 53129.20703125
Loss = 52298.1640625
Loss = 51527.2265625
Loss = 58177.9765625
Loss = 52602.42578125
Loss = 53248.828125
Loss = 54491.28125
Loss = 52920.2265625
Loss = 51565.5546875
Loss = 52153.9921875
Loss = 54634.5234375
Loss = 54311.90625
Loss = 52081.06640625
Loss = 55056.03515625
Loss = 56267.359375
Loss = 52564.2265625
Loss = 51895.984375
Loss = 53123.03125
Loss = 50159.7890625
Loss = 54958.375
Loss = 52737.296875
Loss = 56119.421875
Loss = 52907.4453125
Loss = 52492.609375
Loss = 52939.1953125
Loss = 50832.1875
Loss = 53899.8515625
Loss = 56521.875
Loss = 52317.15234375
Loss = 49791.91015625
Loss = 52361.296875
Loss = 52962.984375
Loss = 53159.046875
Loss = 53587.8828125
Loss = 51556.4609375
Loss = 51716.6171875
Loss = 52946.234375
Loss = 53513.0390625
Loss = 53391.16015625
Loss = 53863.59375
Loss = 50334.6171875
Loss = 56062.19921875
Loss = 54540.2109375
Loss = 53295.8984375
Loss = 50596.546875
Loss = 52666.45703125
Loss = 55241.5078125
Loss = 54789.32421875
Loss = 56048.93359375
Loss = 53778.6015625
Loss = 54841.82421875
Loss = 53014.46484375
Loss = 55546.51953125
Loss = 52770.40625
Loss = 51742.2578125
Loss = 53597.53515625
Loss = 53706.7578125
Loss = 57085.27734375
Loss = 53287.0234375
Loss = 53974.0
Loss = 50510.6015625
Loss = 53593.890625
Loss = 51955.7890625
Loss = 55010.9765625
Loss = 50949.09375
Loss = 51402.89453125
Loss = 52999.33984375
Loss = 53038.6484375
Loss = 51721.859375
Loss = 53256.23046875
Starting Epoch #10 of 10.
Starting Epoch #10 of 10.
Loss = 54717.96875
Loss = 54843.4609375
Loss = 54284.43359375
Loss = 55477.046875
Loss = 52286.5546875
Loss = 50114.84375
Loss = 51461.8828125
Loss = 51533.39453125
Loss = 53686.0625
Loss = 52561.51953125
Loss = 54507.546875
Loss = 54447.4765625
Loss = 55910.265625
Loss = 51249.9765625
Loss = 53077.484375
Loss = 54461.6875
Loss = 49981.5859375
Loss = 51673.9765625
Loss = 53360.78515625
Loss = 53894.9375
Loss = 51206.71875
Loss = 54158.671875
Loss = 52963.29296875
Loss = 52427.578125
Loss = 55881.74609375
Loss = 56858.3359375
Loss = 54509.1875
Loss = 53146.609375
Loss = 55309.70703125
Loss = 50797.90625
Loss = 51133.2421875
Loss = 53227.828125
Loss = 52303.390625
Loss = 52207.44140625
Loss = 52934.84375
Loss = 52004.71875
Loss = 52602.03515625
Loss = 53282.7421875
Loss = 52408.6875
Loss = 52653.65625
Loss = 51171.00390625
Loss = 55547.2265625
Loss = 52862.5234375
Loss = 58832.1796875
Loss = 51874.453125
Loss = 54090.41796875
Loss = 51853.78125
Loss = 52596.421875
Loss = 53645.39453125
Loss = 52447.328125
Loss = 54464.6796875
Loss = 54230.4609375
Loss = 52976.0625
Loss = 52150.1484375
Loss = 53217.28125
Loss = 52861.9140625
Loss = 51912.99609375
Loss = 53557.62890625
Loss = 52176.5
Loss = 53076.296875
Loss = 52826.046875
Loss = 55149.6484375
Loss = 52742.046875
Loss = 48817.734375
Loss = 52934.45703125
Loss = 53642.08203125
Loss = 52353.46875
Loss = 54155.4765625
Loss = 53108.96875
Loss = 53475.48046875
Loss = 51027.85546875
Loss = 53249.30859375
Loss = 52426.56640625
Loss = 53079.9609375
Loss = 54808.078125
Loss = 55761.30078125
Loss = 53871.94140625
Loss = 54293.0625
Loss = 52182.7109375
Loss = 52863.90625
Loss = 53849.47265625
Loss = 51047.60546875
Loss = 52686.984375
Loss = 52610.359375
Loss = 53879.91015625
Loss = 53354.09375
Loss = 52524.99609375
Loss = 52242.0546875
Loss = 52952.3203125
Loss = 54236.546875
Loss = 53360.1640625
Loss = 50774.19921875
Loss = 52383.515625
Loss = 50784.828125
Loss = 51398.2734375
Loss = 56242.109375
Loss = 51587.3671875
Loss = 54497.203125
Loss = 56093.65234375
Loss = 52569.765625
Loss = 55142.28515625
Loss = 50562.54296875
Loss = 51543.5234375
Loss = 52346.09375
Loss = 53919.296875
Loss = 49974.69140625
Loss = 51716.59375
Loss = 49381.2421875
Loss = 51212.0234375
Loss = 49050.5234375
Loss = 53571.0
Loss = 52965.6953125
Loss = 51356.39453125
Loss = 54974.70703125
Loss = 51636.125
Loss = 53106.625
Loss = 52916.3828125
Loss = 56113.5859375
Loss = 54528.51171875
Loss = 52618.65625
Loss = 55087.76171875
Loss = 50363.24609375
Loss = 51367.71875
Loss = 53428.4765625
Loss = 56982.7109375
Loss = 52351.1015625
Loss = 52138.7265625
Loss = 55012.06640625
Loss = 53134.01171875
Loss = 52528.640625
Loss = 52457.04296875
Loss = 50982.6875
Loss = 56244.0859375
Loss = 54803.3984375
Loss = 51537.03515625
Loss = 53351.85546875
Loss = 51950.9375
Loss = 53296.875
Loss = 52796.46875
Loss = 53913.0
Loss = 53606.83203125
Loss = 53769.81640625
Loss = 52329.5625
Loss = 54035.9375
Loss = 57916.49609375
Loss = 55077.2890625
Loss = 50755.25
Loss = 53488.390625
Loss = 54676.015625
Loss = 51492.625
Loss = 52518.1953125
Loss = 52850.4609375
Loss = 53430.171875
Loss = 54836.5625
Loss = 51540.515625
Loss = 53333.2109375
Loss = 52914.078125
Loss = 51078.1484375
Loss = 52440.625
Loss = 52717.0703125
Loss = 54330.13671875
Loss = 51784.02734375
Loss = 54128.4921875
Loss = 52564.7734375
Loss = 52358.40625
Loss = 53189.97265625
Loss = 52478.11328125
Loss = 52080.03125
Loss = 52572.34765625
Loss = 52141.3984375
Loss = 52969.359375
Loss = 51985.16796875
Loss = 53566.5703125
Loss = 51135.5
Loss = 52771.25
Loss = 53729.62890625
Loss = 55716.203125
Loss = 53748.421875
Loss = 54063.4765625
Loss = 52385.7578125
Loss = 51167.2421875
Loss = 55236.32421875
Loss = 54621.06640625
Loss = 49752.40234375
Loss = 51112.8125
Loss = 53975.3515625
Loss = 54715.58984375
oh
tf.Tensor([[6482]], shape=(1, 1), dtype=int32)
oh my
</pre></div>
</div>
</div>
</div>
<p>Plot loss over time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loss</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="s1">&#39;k-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Generation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sequence to Sequence Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/lstm_36_0.png" src="../_images/lstm_36_0.png" />
</div>
</div>
</section>
</section>
<section id="your-turn">
<h2><span class="section-number">25.3. </span>Your turn! üöÄ<a class="headerlink" href="#your-turn" title="Permalink to this headline">#</a></h2>
<p>Practice the Long-Short Term Memory Networks by following this TBD.</p>
</section>
<section id="acknowledgments">
<h2><span class="section-number">25.4. </span>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">#</a></h2>
<p>Thanks to <a class="reference external" href="https://github.com/nfmcclure">Nick</a> for creating the open-source project <a class="reference external" href="https://github.com/nfmcclure/tensorflow_cookbook">tensorflow_cookbook</a> and <a class="reference external" href="https://github.com/rasbt">Sebastian Raschka</a> for creating the open-source project <a class="reference external" href="https://github.com/rasbt/stat453-deep-learning-ss20">stat453-deep-learning-ss20</a>. They inspire the majority of the content in this chapter.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ocademy-ai/machine-learning",
            ref: "release",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./deep-learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="autoencoder.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">24. </span>Autoencoder</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="time-series.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">26. </span>Time series</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ocademy<br/>
  
      &copy; Copyright 2022-2023.<br/>
    <div class="extra_footer">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> Text content of this work is licensed under the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>