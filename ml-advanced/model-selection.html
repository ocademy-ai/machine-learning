
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>20. Model selection &#8212; Ocademy Open Machine Learning Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/drawio.css" />
    <link rel="stylesheet" type="text/css" href="../_static/youtube.css" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "ocademy-ai/machine-learning-utterances");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "üí¨ comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script src="https://wavedrom.com/skins/default.js"></script>
    <script src="https://wavedrom.com/wavedrom.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script src="https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"></script>
    <script>quizdown.init({"quizdown_js": "https://cdn.jsdelivr.net/gh/bonartm/quizdown-js@latest/public/build/quizdown.js"});</script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="21. Intro to Deep Learning" href="../deep-learning/dl-overview.html" />
    <link rel="prev" title="19. Unsupervised learning: PCA and clustering" href="unsupervised-learning-pca-and-clustering.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint">Learn AI together, for free! At <a color='lightblue' href='https://ocademy.cc'><u style='color:lightblue;'>Ocademy</u></a>.</div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo-long.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Ocademy Open Machine Learning Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  PREREQUISITES
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-introduction.html">
   1. Python programming introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-basics.html">
   2. Python programming basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prerequisites/python-programming-advanced.html">
   3. Python programming advanced
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DATA SCIENCE
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/introduction/introduction.html">
   4. Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/defining-data-science.html">
     4.1. Defining data science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/data-science-ethics.html">
     4.2. Data Science ethics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/defining-data.html">
     4.3. Defining data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/introduction/introduction-to-statistics-and-probability.html">
     4.4. Introduction to statistics and probability
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/working-with-data/working-with-data.html">
   5. Working with data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/relational-databases.html">
     5.1. Relational databases
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/non-relational-data.html">
     5.2. Non-relational data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/numpy.html">
     5.3. NumPy
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../data-science/working-with-data/pandas/pandas.html">
     5.4. Pandas
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/introduction-and-data-structures.html">
       5.4.1. Introduction and Data Structures
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/data-selection.html">
       5.4.2. Data Selection
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../data-science/working-with-data/pandas/advanced-pandas-techniques.html">
       5.4.3. Advanced Pandas Techniques
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/working-with-data/data-preparation.html">
     5.5. Data preparation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-visualization/data-visualization.html">
   6. Data visualization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-distributions.html">
     6.1. Visualizing distributions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-proportions.html">
     6.2. Visualizing proportions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/visualization-relationships.html">
     6.3. Visualizing relationships: all about honey üçØ
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-visualization/meaningful-visualizations.html">
     6.4. Making meaningful visualizations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-science-lifecycle/data-science-lifecycle.html">
   7. Data Science lifecycle
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/introduction.html">
     7.1. Introduction to the Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/analyzing.html">
     7.2. Analyzing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-lifecycle/communication.html">
     7.3. Communication
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../data-science/data-science-in-the-cloud/data-science-in-the-cloud.html">
   8. Data Science in the cloud
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/introduction.html">
     8.1. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/the-low-code-no-code-way.html">
     8.2. The ‚Äúlow code/no code‚Äù way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../data-science/data-science-in-the-cloud/the-azure-ml-sdk-way.html">
     8.3. Data Science in the cloud: The ‚ÄúAzure ML SDK‚Äù way
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../data-science/data-science-in-the-wild.html">
   9. Data Science in the real world
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING BASICS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-fundamentals/ml-overview.html">
   10. Machine Learning overview
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fundamentals/regression/regression-models-for-machine-learning.html">
   11. Regression models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/tools-of-the-trade.html">
     11.1. Tools of the trade
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/managing-data.html">
     11.2. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/linear-and-polynomial-regression.html">
     11.3. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/logistic-regression.html">
     11.4. Logistic regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/regression/Univariate-linear-regression.html">
     11.5. Univariate linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fundamentals/classification/getting-started-with-classification.html">
   12. Getting started with classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/introduction-to-classification.html">
     12.1. Introduction to classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/more-classifiers.html">
     12.2. More classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/classification/yet-other-classifiers.html">
     12.3. Yet other classifiers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/build-a-web-app-to-use-a-machine-learning-model.html">
     12.4. Build a web app to use a Machine Learning model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ml-fundamentals/parameter-optimization/parameter-optimization.html">
   13. Parameter Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/parameter-optimization/loss-function.html">
     13.1. Loss function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-fundamentals/parameter-optimization/gradient-descent.html">
     13.2. Gradient descent
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  ADVANCED MACHINE LEARNING
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering/clustering-models-for-machine-learning.html">
   14. Clustering models for Machine Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering/introduction-to-clustering.html">
     14.1. Introduction to clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering/k-means-clustering.html">
     14.2. K-Means clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="ensemble-learning/getting-started-with-ensemble-learning.html">
   15. Getting started with ensemble learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="ensemble-learning/bagging.html">
     15.2. Bagging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ensemble-learning/random-forest.html">
     15.3. Random forest
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ensemble-learning/feature-importance.html">
     15.4. Feature importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="gradient-boosting/introduction-to-gradient-boosting.html">
   16. Introduction to Gradient Boosting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient-boosting/gradient-boosting.html">
     16.1. Gradient Boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient-boosting/gradient-boosting-example.html">
     16.2. Gradient boosting example
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient-boosting/xgboost.html">
     16.3. XGBoost
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="gradient-boosting/xgboost-k-fold-cv-feature-importance.html">
     16.4. XGBoost + k-fold CV + Feature Importance
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised-learning.html">
   17. Unsupervised learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kernel-method.html">
   18. Kernel method
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unsupervised-learning-pca-and-clustering.html">
   19. Unsupervised learning: PCA and clustering
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   20. Model selection
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DEEP LEARNING
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/dl-overview.html">
   21. Intro to Deep Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/nn.html">
   22. Neural Networks
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep-learning/cnn/cnn.html">
   23. Convolutional Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/cnn/cnn-vgg.html">
     23.9. Stylenet / Neural-Style
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/cnn/cnn-deepdream.html">
     23.10. Deepdream in TensorFlow
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep-learning/rnn/rnn.html">
   24. Recurrent Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/rnn/lstm.html">
     24.1.1. Long-short term memory
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/rnn/bi-rnn.html">
     24.1.2. Bidirectional RNN
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/time-series.html">
   25. Time series
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/autoencoder.html">
   26. Autoencoder
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/object-detection.html">
   27. Object detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/image-classification.html">
   28. Image classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/image-segmentation.html">
   29. Image segmentation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../deep-learning/nlp/nlp.html">
   30. Natural Language Processing
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/nlp/text-preprocessing.html">
     30.7.1. Text Preprocessing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../deep-learning/nlp/text-representation.html">
     30.7.2. Word embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/gan.html">
   31. Generative adversarial networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/difussion-model.html">
   32. Diffusion Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deep-learning/dqn.html">
   33. Deep Q-learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MACHINE LEARNING OPERATIONS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/overview.html">
   34. Overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/problem-framing.html">
   35. Problem framing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/data-engineering.html">
   36. Data engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/model-training-and-evaluation.html">
   37. Model training &amp; evaluation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machine-learning-productionization/model-deployment.html">
   38. Model deployment
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Large Language Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../llm/introduction.html">
   39. Introduction
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../llm/basic/basic.html">
   40. Large Language Models Basic
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../llm/basic/attention.html">
     40.1. Coding Attention Mechanisms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../llm/basic/transformer.html">
     40.2. Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../llm/basic/language-modelling.html">
     40.3. Transformers for Language Modelling
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../llm/pretrained-model/implementing-a-GPT-model.html">
   41. Implementing a GPT model from Scratch To Generate Text
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../llm/pretrained-model/pretraining-on-unlabeled-data.html">
     41.10. Pretraining on Unlabeled Data
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  OTHERS
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../assignments/README.html">
   42. Self-paced assignments
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/set-up-env/first-assignment.html">
     42.3. First assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/set-up-env/second-assignment.html">
     42.4. Second assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/project-plan-template.html">
     42.5. Project Plan‚Äã Template
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-introduction.html">
     42.6. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-basics.html">
     42.7. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/prerequisites/python-programming-advanced.html">
     42.8. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-text-about-data-science.html">
     42.9. Analyzing text about Data Science
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-scenarios.html">
     42.10. Data Science scenarios
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/write-a-data-ethics-case-study.html">
     42.11. Write a data ethics case study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/lines-scatters-and-bars.html">
     42.12. Lines, scatters and bars
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/apply-your-skills.html">
     42.13. Apply your skills
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/try-it-in-excel.html">
     42.14. Try it in Excel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/dive-into-the-beehive.html">
     42.15. Dive into the beehive
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/build-your-own-custom-vis.html">
     42.16. Build your own custom vis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/classifying-datasets.html">
     42.17. Classifying datasets
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/small-diabetes-study.html">
     42.18. Small diabetes study
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/introduction-to-statistics-and-probability.html">
     42.19. Introduction to probability and statistics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/displaying-airport-data.html">
     42.20. Displaying airport data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/soda-profits.html">
     42.21. Soda profits
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-COVID-19-papers.html">
     42.22. Analyzing COVID-19 papers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/estimation-of-COVID-19-pandemic.html">
     42.23. Estimation of COVID-19 pandemic
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-processing-in-python.html">
     42.24. Data processing in Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/evaluating-data-from-a-form.html">
     42.25. Evaluating data from a form
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-preparation.html">
     42.26. Data preparation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/analyzing-data.html">
     42.27. Analyzing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/nyc-taxi-data-in-winter-and-summer.html">
     42.28. NYC taxi data in winter and summer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/matplotlib-applied.html">
     42.29. Matplotlib applied
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/tell-a-story.html">
     42.35. Tell a story
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/explore-a-planetary-computer-dataset.html">
     42.36. Explore a planetary computer dataset
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/exploring-for-anwser.html">
     42.37. Exploring for answers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/market-research.html">
     42.38. Market research
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/low-code-no-code-data-science-project-on-azure-ml.html">
     42.39. Low code/no code Data Science project on Azure ML
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-project-using-azure-ml-sdk.html">
     42.40. Data Science project using Azure ML SDK
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/data-science/data-science-in-the-cloud-the-azure-ml-sdk-way.html">
     42.41. Data Science in the cloud: The ‚ÄúAzure ML SDK‚Äù way
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-overview-iris.html">
     42.42. Machine Learning overview - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-overview-mnist-digits.html">
     42.43. Machine Learning overview - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/regression-with-scikit-learn.html">
     42.44. Regression with Scikit-learn
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/california_housing.html">
     42.45. Linear regression - California Housing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/linear-regression-metrics.html">
     42.46. Linear Regression Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/loss-function.html">
     42.47. Loss Function
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/gradient-descent.html">
     42.48. Gradient descent
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-regression/linear-regression-from-scratch.html">
     42.49. Linear Regression Implementation from Scratch
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-logistic-regression-1.html">
     42.50. ML logistic regression - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-logistic-regression-2.html">
     42.51. ML logistic regression - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/ml-neural-network-1.html">
     42.52. ML neural network - Assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-ml-web-app-1.html">
     42.53. Build ML web app - assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-ml-web-app-2.html">
     42.54. Build ML web app - assignment 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/regression-tools.html">
     42.55. Regression tools
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/managing-data.html">
     42.56. Managing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/exploring-visualizations.html">
     42.57. Exploring visualizations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/try-a-different-model.html">
     42.58. Try a different model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/create-a-regression-model.html">
     42.59. Create a regression model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/linear-and-polynomial-regression.html">
     42.60. Linear and polynomial regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/retrying-some-regression.html">
     42.61. Retrying some regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/pumpkin-varieties-and-color.html">
     42.62. Pumpkin varieties and color
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/delicious-asian-and-indian-cuisines.html">
     42.63. Delicious asian and indian cuisines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/explore-classification-methods.html">
     42.64. Explore classification methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/kernel-method-assignment-1.html">
     42.65. Kernel method assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/support_vector_machines_for_regression.html">
     42.66. Support Vector Machines (SVM) - Intro and SVM for Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/support_vector_machines_for_classification.html">
     42.67. Support Vector Machines (SVM) - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/decision_trees_for_regression.html">
     42.68. Decision Trees - Intro and Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/kernel-method/decision_trees_for_classification.html">
     42.69. Decision Trees - Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/model-selection-assignment-1.html">
     42.70. Model selection assignment 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/learning-curve-to-identify-overfit-underfit.html">
     42.71. Learning Curve To Identify Overfit &amp; Underfit
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/dropout-and-batch-normalization.html">
     42.72. Dropout and Batch Normalization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/lasso-and-ridge-regression.html">
     42.73. Lasso and Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/model-selection/regularized-linear-models.html">
     42.74. Regularized Linear Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/unsupervised-learning/customer-segmentation-clustering.html">
     42.75. Customer segmentation: clustering - assignment 3
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forests-intro-and-regression.html">
     42.76. Random forests intro and regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forests-for-classification.html">
     42.77. Random forests for classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/beyond-random-forests-more-ensemble-models.html">
     42.78. Beyond random forests: more ensemble models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/decision-trees.html">
     42.79. Decision trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/hyperparameter-tuning-gradient-boosting.html">
     42.80. Hyperparameter tuning gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/gradient-boosting-assignment.html">
     42.81. Gradient boosting
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/gradient-boosting/boosting-with-tuning.html">
     42.82. Boosting with tuning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-advanced/ensemble-learning/random-forest-classifier-feature-importance.html">
     42.83. Random Forest Classifier with Feature Importance
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/data-engineering.html">
     42.85. Data engineering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/counterintuitive-challenges-in-ml-debugging.html">
     42.86. Counterintuitive Challenges in ML Debugging
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/debugging-in-classification.html">
     42.87. Case Study: Debugging in Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/debugging-in-regression.html">
     42.88. Case Study: Debugging in Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/machine-learning-productionization/random-forest-classifier.html">
     42.89. Introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/study-the-solvers.html">
     42.90. Study the solvers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-classification-models.html">
     42.91. Build classification models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/build-classification-model.html">
     42.92. Build Classification Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/ml-fundamentals/parameter-play.html">
     42.93. Parameter play
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/how-to-choose-cnn-architecture-mnist.html">
     42.94. How to choose cnn architecture mnist
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/sign-language-digits-classification-with-cnn.html">
     42.96. Sign Language Digits Classification with CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/object-recognition-in-images-using-cnn.html">
     42.98. Object Recognition in Images using CNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/cnn/image-classification.html">
     42.99. Image classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/tensorflow/intro_to_tensorflow_for_deeplearning.html">
     42.100. Intro to TensorFlow for Deep Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/lstm/bitcoin-lstm-model-with-tweet-volume-and-sentiment.html">
     42.102. Bitcoin LSTM Model with Tweet Volume and Sentiment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/rnn/google-stock-price-prediction-rnn.html">
     42.104. Google Stock Price Prediction RNN
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/autoencoder.html">
     42.106. Intro to Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/base-denoising-autoencoder-dimension-reduction.html">
     42.107. Base/Denoising Autoencoder &amp; Dimension Reduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/autoencoder/variational-autoencoder-and-faces-generation.html">
     42.108. Fun with Variational Autoencoders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/time-series-forecasting-assignment.html">
     42.109. Time Series Forecasting Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nn-for-classification-assignment.html">
     42.111. Neural Networks for Classification with TensorFlow
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nn-classify-15-fruits-assignment.html">
     42.112. NN Classify 15 Fruits Assignment
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/dqn/dqn-on-foreign-exchange-market.html">
     42.117. DQN On Foreign Exchange Market
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/gan/art-by-gan.html">
     42.118. Art by gan
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/gan/gan-introduction.html">
     42.120. Generative Adversarial Networks (GANs)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/image-segmentation/comparing-edge-based-and-region-based-segmentation.html">
     42.121. Comparing edge-based and region-based segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/difussion-model/denoising-difussion-model.html">
     42.122. Summary
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/object-detection/car-object-detection.html">
     42.123. Car Object Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/overview/basic-classification-classify-images-of-clothing.html">
     42.125. Basic classification: Classify images of clothing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nlp/getting-start-nlp-with-classification-task.html">
     42.126. Getting Start NLP with classification task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nlp/beginner-guide-to-text-preprocessing.html">
     42.128. Beginner‚Äôs Guide to Text Pre-Processing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/deep-learning/nlp/news-topic-classification-tasks.html">
     42.129. News topic classification tasks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../assignments/llm/basic/transformer-architecture.html">
     42.130. Complete the transformer architecture
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../slides/introduction.html">
   43. Slides
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-introduction.html">
     43.1. Python programming introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-basics.html">
     43.2. Python programming basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/python-programming/python-programming-advanced.html">
     43.3. Python programming advanced
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-introduction.html">
     43.4. Data Science introduction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/relational-vs-non-relational-database.html">
     43.5. Relational vs. non-relational database
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/numpy-and-pandas.html">
     43.6. NumPy and Pandas
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-visualization.html">
     43.7. Data visualization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-lifecycle.html">
     43.8. Data Science lifecycle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-in-the-cloud.html">
     43.9. Data Science in the cloud
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/data-science/data-science-in-real-world.html">
     43.10. Data Science in real world
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/ml-overview.html">
     43.11. Machine Learning overview
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/linear-regression.html">
     43.12. Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/logistic-regression.html">
     43.13. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/logistic-regression-condensed.html">
     43.14. Logistic Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/neural-network.html">
     43.15. Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-fundamentals/build-an-ml-web-app.html">
     43.16. Build an machine learning web application
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/unsupervised-learning.html">
     43.17. Unsupervised learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/kernel-method.html">
     43.18. Kernel method
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/ml-advanced/model-selection.html">
     43.19. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/deep-learning/cnn.html">
     43.20. Convolutional Neural Network
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../slides/deep-learning/gan.html">
     43.21. Generative Adversarial Network
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/ocademy-ai/machine-learning/release?urlpath=lab/tree/open-machine-learning-jupyter-book/ml-advanced/model-selection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/ocademy-ai/machine-learning/blob/release/open-machine-learning-jupyter-book/ml-advanced/model-selection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning//issues/new?title=Issue%20on%20page%20%2Fml-advanced/model-selection.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/ocademy-ai/machine-learning/edit/release/open-machine-learning-jupyter-book/ml-advanced/model-selection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/ml-advanced/model-selection.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#over-fitting-and-under-fitting">
   20.1. Over-fitting and under-fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     20.1.1. Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting">
     20.1.2. Underfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-avoid-underfitting">
       20.1.2.1. How To Avoid Underfitting?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting">
     20.1.3. Overfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-avoid-overfitting">
       20.1.3.1. How To Avoid Overfitting?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#good-fitting">
     20.1.4. Good Fitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-example-of-linear-regression">
     20.1.5. A simple example of linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-tradeoff">
   20.2. Bias variance tradeoff
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics">
   20.3. Metrics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confusion-matrix">
     20.3.1. Confusion matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#method">
   20.4. Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#holdout-method">
     20.4.1. Holdout Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     20.4.2. Cross-Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bootstrapping">
     20.4.3. Bootstrapping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpreting-the-learning-curves">
   20.5. Interpreting the Learning Curves
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#capacity">
   20.6. Capacity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l1-and-l2-regularization">
   20.7. L1 and L2 Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#early-stopping">
   20.8. Early Stopping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-early-stopping">
   20.9. Adding Early Stopping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-impact-of-the-value-of-lambda">
   20.10. The impact of the value of
   <span class="math notranslate nohighlight">
    \(\lambda\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dropout">
   20.11. Dropout
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-after-dropout">
     20.11.1. Prediction after dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   20.12. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   20.13. Your turn! üöÄ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-study">
   20.14. Self study
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   20.15. Acknowledgments
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model selection</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#over-fitting-and-under-fitting">
   20.1. Over-fitting and under-fitting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overview">
     20.1.1. Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#underfitting">
     20.1.2. Underfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-avoid-underfitting">
       20.1.2.1. How To Avoid Underfitting?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting">
     20.1.3. Overfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-avoid-overfitting">
       20.1.3.1. How To Avoid Overfitting?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#good-fitting">
     20.1.4. Good Fitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-simple-example-of-linear-regression">
     20.1.5. A simple example of linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-tradeoff">
   20.2. Bias variance tradeoff
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics">
   20.3. Metrics
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#confusion-matrix">
     20.3.1. Confusion matrix
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#method">
   20.4. Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#holdout-method">
     20.4.1. Holdout Method
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     20.4.2. Cross-Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bootstrapping">
     20.4.3. Bootstrapping
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpreting-the-learning-curves">
   20.5. Interpreting the Learning Curves
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#capacity">
   20.6. Capacity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#l1-and-l2-regularization">
   20.7. L1 and L2 Regularization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#early-stopping">
   20.8. Early Stopping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-early-stopping">
   20.9. Adding Early Stopping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-impact-of-the-value-of-lambda">
   20.10. The impact of the value of
   <span class="math notranslate nohighlight">
    \(\lambda\)
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dropout">
   20.11. Dropout
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prediction-after-dropout">
     20.11.1. Prediction after dropout
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   20.12. Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#your-turn">
   20.13. Your turn! üöÄ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#self-study">
   20.14. Self study
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#acknowledgments">
   20.15. Acknowledgments
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the necessary dependencies</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="o">!{</span>sys.executable<span class="o">}</span><span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--quiet<span class="w"> </span>pandas<span class="w"> </span>scikit-learn<span class="w"> </span>numpy<span class="w"> </span>matplotlib<span class="w"> </span>jupyterlab_myst<span class="w"> </span>ipython
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="model-selection">
<h1><span class="section-number">20. </span>Model selection<a class="headerlink" href="#model-selection" title="Permalink to this headline">#</a></h1>
<section id="over-fitting-and-under-fitting">
<h2><span class="section-number">20.1. </span>Over-fitting and under-fitting<a class="headerlink" href="#over-fitting-and-under-fitting" title="Permalink to this headline">#</a></h2>
<section id="overview">
<h3><span class="section-number">20.1.1. </span>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h3>
<p>Remember that the main objective of any machine learning model is to generalize the learning based on training data, so that it will be able to do predictions accurately on unknown data. Here are a few concepts: the first is ‚ÄòHypothesis‚Äô, the second is ‚ÄòTruth‚Äô. When we obtain data and train it, we propose a hypothesis, and the process of forcing the hypothesis to be as close to the truth as possible is our training process. This process is called ‚Äòfitting‚Äô, which means the model tries to learn the patterns, relationships, or rules in the data in order to make predictions or classifications on unknown data. Due to the existence of errors in the hypothesis, we introduce the concepts of generalization error and empirical error (training error). The generalization error represents the error in unknown samples when we fit the model to the truth. It is uncertain. On the other hand, the empirical error represents the error on the training set, and it can be determined. In order to reduce the error and approach the truth, we need model evaluation. However, due to the occurrence of overfitting, a smaller error does not necessarily indicate a better model.</p>
<p>As you can notice the words ‚ÄòOverfitting‚Äô and ‚ÄòUnderfitting‚Äô are kind of opposite of the term ‚ÄòGeneralization‚Äô. Overfitting and underfitting models don‚Äôt generalize well and results in poor performance.</p>
<p>These are the samples of over-fitting and under-fitting in regression:</p>
<figure class="align-default" id="over-fitting-regression-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/under_over_justalright.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/under_over_justalright.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.1 </span><span class="caption-text">Over-fitting and under-fitting in regression</span><a class="headerlink" href="#over-fitting-regression-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>During the fitting process, we have an important parameter called ‚Äòbias‚Äô. It refers to the deviation of the model from the true relationship when attempting to fit the data.</p>
</section>
<section id="underfitting">
<h3><span class="section-number">20.1.2. </span>Underfitting<a class="headerlink" href="#underfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Underfitting occurs when machine learning model don‚Äôt fit the training data well enough. It is usually caused by simple function that cannot capture the underlying trend in the data.</p></li>
<li><p>Underfitting models have high error in training as well as test set. This behavior is called as ‚ÄòLow Bias‚Äô</p></li>
<li><p>This usually happens when we try to fit linear function for non-linear data.</p></li>
<li><p>Since underfitting models don‚Äôt perform well on training set, it‚Äôs very easy to detect underfitting</p></li>
</ul>
<section id="how-to-avoid-underfitting">
<h4><span class="section-number">20.1.2.1. </span>How To Avoid Underfitting?<a class="headerlink" href="#how-to-avoid-underfitting" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Increasing the model complexity. e.g. If linear function under fit then try using polynomial features</p></li>
<li><p>Increase the number of features by performing the feature engineering</p></li>
</ul>
</section>
</section>
<section id="overfitting">
<h3><span class="section-number">20.1.3. </span>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Overfitting occurs when machine learning model tries to fit the training data too well. It is usually caused by complicated function that creates lots of unnecessary curves and angles that are not related with data and end up capturing the noise in data.</p></li>
<li><p>Overfitting models have low error in training set but high error in test set. This behavior is called as ‚ÄòHigh Variance‚Äô</p></li>
</ul>
<section id="how-to-avoid-overfitting">
<h4><span class="section-number">20.1.3.1. </span>How To Avoid Overfitting?<a class="headerlink" href="#how-to-avoid-overfitting" title="Permalink to this headline">#</a></h4>
<ul class="simple">
<li><p>Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same</p></li>
<li><p>We can also use the ‚ÄòRegularization‚Äô technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter ‚Äòalpha‚Äô to control the size of the coefficients by imposing a penalty. Please refer below tutorials for more details.</p></li>
</ul>
</section>
</section>
<section id="good-fitting">
<h3><span class="section-number">20.1.4. </span>Good Fitting<a class="headerlink" href="#good-fitting" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>It is a sweet spot between Underfitting and Overfitting model</p></li>
<li><p>A good fitting model generalizes the learnings from training data and provide accurate predictions on new data</p></li>
<li><p>To get the good fitting model, keep training and testing the model till you get the minimum train and test error. Here important parameter is ‚Äòtest error‚Äô because low train error may cause overfitting so always keep an eye on test error fluctuations. The sweet spot is just before the test error start to rise.</p></li>
</ul>
<p>In summary the goal of model selection is to find a model that fits the training data well and has low prediction error on new unknown data. If a model that is too simple is chosen, it may not fit the training data well, resulting in underfitting. On the other hand, if a model that is too complex is chosen, overfitting may occur, leading to a decrease in predictive performance on new data.
Now let‚Äôs take a look at another example, hoping it will be helpful for your understanding.</p>
<figure class="align-default" id="over-fitting-classification-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/classification.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/classification.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.2 </span><span class="caption-text">Over-fitting and under-fitting in classification</span><a class="headerlink" href="#over-fitting-classification-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="a-simple-example-of-linear-regression">
<h3><span class="section-number">20.1.5. </span>A simple example of linear regression<a class="headerlink" href="#a-simple-example-of-linear-regression" title="Permalink to this headline">#</a></h3>
<p>This is a simple graphical representation of linear regression training.</p>
<figure class="align-default" id="datapoints-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-datapoints.jpg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-datapoints.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 20.3 </span><span class="caption-text">Training data points</span><a class="headerlink" href="#datapoints-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>First we have some data points, then we‚Äôre going to train it by linear regression.</p>
<p><strong>Over-fitting model</strong></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><img alt="Over-fitting-train-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-overfitting.jpg" /></p></th>
<th class="text-align:center head"><p><img alt="Over-fitting-test-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-overfitting-testdata.jpg" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Over-fitting-train-ms</p></td>
<td class="text-align:center"><p>Over-fitting-test-ms</p></td>
</tr>
</tbody>
</table>
<p>As we can see, over-fitting model fits very well on training data, but over-fitting model fits poorly on test data.</p>
<p><strong>Under-fitting model</strong></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><img alt="Under-fitting-train-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-underfitting.jpg" /></p></th>
<th class="text-align:center head"><p><img alt="Under-fitting-test-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-underfitting-test-data.jpg" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Under-fitting-train-ms</p></td>
<td class="text-align:center"><p>Under-fitting-test-ms</p></td>
</tr>
</tbody>
</table>
<p>As for under-fitting model, it fits poorly on training data and test data.</p>
<p><strong>Perfect-fitting model</strong></p>
<p>After seeing the under-fitting model and the over-fitting model we are eager to know what is a good-fitting model.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><img alt="Perfect-fitting-train-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-perfect-fit.jpg" /></p></th>
<th class="text-align:center head"><p><img alt="Perfect-fitting-test-ms" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-perfect-fit-test-data.jpg" /></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Perfect-fitting-train-ms</p></td>
<td class="text-align:center"><p>Perfect-fitting-test-ms</p></td>
</tr>
</tbody>
</table>
<p>Perfect-fitting model fits well on training data and test data!</p>
<p>When over-fitting occurs, the model demonstrates high accuracy or low error on the training data but performs poorly on the testing data or new data in practical applications. In contrast, under-fitting indicates that the model is unable to capture the complex relationships or patterns within the data.</p>
</section>
</section>
<section id="bias-variance-tradeoff">
<h2><span class="section-number">20.2. </span>Bias variance tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Permalink to this headline">#</a></h2>
<p>In this section we talk about Bias Variance tradeoff</p>
<p>So what is Bias and Variance? Or to say why they are so importent in model selection?</p>
<p>Bias refers to the model‚Äôs incorrect assumptions or simplifications about the problem. When a model has high bias, it may overlook some key features or patterns in the data, resulting in systematic errors in the predictions. In other words, a high-bias model tends to produce incorrect predictions.</p>
<p>Variance refers to the sensitivity or volatility of the model to the training data. When a model has high variance, it is very sensitive to small perturbations in the training data and may overfit the noise and details in the training data, leading to poor generalization to new data. In other words, a high-variance model is more prone to the influence of randomness and produces larger prediction errors.</p>
<p>Here are some illustrations showing the relationship between bias and variance in data fitting.</p>
<figure class="align-default" id="graphicalillustration-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/graphicalillustration.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/graphicalillustration.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.4 </span><span class="caption-text">Graphical illustration of variance and bias</span><a class="headerlink" href="#graphicalillustration-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="model-complexity-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/total_error.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/total_error.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.5 </span><span class="caption-text">Model complexity v.s. error</span><a class="headerlink" href="#model-complexity-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="metrics">
<h2><span class="section-number">20.3. </span>Metrics<a class="headerlink" href="#metrics" title="Permalink to this headline">#</a></h2>
<p>Were there some ways that can be used to represent the bias and variance of a model?</p>
<p>First, when we start training, how to evaluate the goodness of fit?</p>
<p>The simplest way is to output some metrics that can substitute for bias and variance. Here are several metrics that can be used for calculation:</p>
<p><strong>Accuracy</strong>ÔºöAccuracy is a commonly used evaluation metric in classification models. It represents the proportion of correctly classified samples in the predictions made by the model. A higher accuracy indicates better performance. However, when there is class imbalance in the dataset, accuracy may underestimate the model‚Äôs performance.</p>
<p><strong>Precision and Recall</strong>ÔºöPrecision and recall are primarily used to evaluate the performance of binary classification models, especially in the presence of class imbalance. Precision represents the proportion of true positive samples among those predicted as positive, while recall represents the proportion of true positive samples among all actual positive samples. Precision and recall can help provide a comprehensive evaluation of the model‚Äôs classification performance.</p>
<p><strong>F1 Score</strong>ÔºöThe F1 score is the harmonic mean of precision and recall, providing a balanced assessment of a model‚Äôs accuracy and recall performance. A higher F1 score indicates better performance.</p>
<p><strong>Mean Squared Error (MSE)</strong>ÔºöMSE is a commonly used evaluation metric in regression models. It represents the average of the squared differences between predicted values and true values. A smaller MSE indicates better performance.</p>
<p><strong>Log Loss</strong>: Log loss is commonly used in binary or multi-class probability prediction problems. It measures the difference between predicted probabilities and true labels. A lower log loss indicates better performance.</p>
<p>These metrics are used to evaluate the performance of models in the model selection process. However, it‚Äôs important to note that these metrics only reflect the fit of the model to a particular dataset and may not fully capture its generalization performance.</p>
<section id="confusion-matrix">
<h3><span class="section-number">20.3.1. </span>Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a note of confusion matrix</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="c1"># Create actual labels and predicted labels</span>
<span class="n">actual_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">predicted_labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Compute the confusion matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">actual_labels</span><span class="p">,</span> <span class="n">predicted_labels</span><span class="p">)</span>

<span class="c1"># Plot the confusion matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Blues</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Confusion Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Predicted 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Predicted 1&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Actual 0&#39;</span><span class="p">,</span> <span class="s1">&#39;Actual 1&#39;</span><span class="p">])</span>

<span class="c1"># Display counts in each cell</span>
<span class="n">thresh</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]),</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;white&quot;</span> <span class="k">if</span> <span class="n">cm</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span> <span class="k">else</span> <span class="s2">&quot;black&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Predicted label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True label&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/model-selection_8_0.png" src="../_images/model-selection_8_0.png" />
</div>
</div>
<p>Above, we output a confusion matrix on actual_labels = [0, 1, 0, 1, 1, 0, 0, 1] and the predicted_labels = [0, 1, 1, 1, 0, 1, 0, 0]</p>
<p>Of course, here we are just demonstrating how to output the confusion matrix to understand its meaning after obtaining these two sets of data. In the subsequent experiment, we will explain how to obtain the desired confusion matrix through code.</p>
<p>There are four values in the matrix their meanings are as follows:</p>
<p><strong>True Positive (TP)</strong>: The number of positive instances correctly predicted as positive by the model.</p>
<p><strong>False Negative (FN)</strong>: The number of positive instances incorrectly predicted as negative by the model.</p>
<p><strong>False Positive (FP)</strong>: The number of negative instances incorrectly predicted as positive by the model.</p>
<p><strong>True Negative (TN)</strong>: The number of negative instances correctly predicted as negative by the model.</p>
<p>As for the matrix we have above, TP is where we predicted as 1 and actually it is 1. FN is the acount that we predicted as 0 but actually it is 1. FP is predicted as 1 but actually it‚Äôs 0. TN is we predicted as 0 and it‚Äôs actually 0.</p>
<p>After understanding the meaning of the matrix, we can use the following algorithms to calculate the desired metrics:</p>
<p><strong>Accuracy</strong>: The ratio of the number of correctly predicted samples to the total number of samples.</p>
<div class="math notranslate nohighlight">
\[Accuracy = \frac{TP + TN}{TP + TN + FP + FN}\]</div>
<p><strong>Precision</strong>: The proportion of true positive predictions among the predicted positive instances, measuring the prediction accuracy of the model.</p>
<div class="math notranslate nohighlight">
\[Precision = \frac{TP}{TP + FP}\]</div>
<p><strong>Recall</strong>: The proportion of true positive predictions among the actual positive instances, measuring the model‚Äôs ability to identify positives.</p>
<div class="math notranslate nohighlight">
\[Recall = \frac{TP}{TP + FN}\]</div>
<p><strong>F1 Score</strong>: The harmonic mean of precision and recall, considering both the accuracy and the identification ability of the model.</p>
<div class="math notranslate nohighlight">
\[F_1 \text{ Score} = \frac{2 \cdot (Precision \cdot Recall)}{Precision + Recall}\]</div>
<p>When evaluating the bias of a model, we usually consider metrics such as precision, accuracy, and F1 score. A lower F1 score may indicate that the model has issues in balancing accuracy and identification ability, but it cannot be simply equated to lower bias. By considering multiple metrics and the specific requirements of the application scenario, a more comprehensive assessment of the model‚Äôs performance can be achieved.</p>
</section>
</section>
<section id="method">
<h2><span class="section-number">20.4. </span>Method<a class="headerlink" href="#method" title="Permalink to this headline">#</a></h2>
<p>Does a lower recall rate indicate better bias?</p>
<p><strong>No</strong>, a lower recall rate does not indicate better bias. In machine learning, recall rate is a metric that measures the model‚Äôs ability to identify positive instances. A higher recall rate indicates that the model can better identify positive instances, while a lower recall rate means that the model may miss some true positive instances.</p>
<p>Then does a lower F1 score indicate better bias?</p>
<p><strong>No</strong>, a lower F1 score does not indicate better bias. The F1 score is the harmonic mean of precision and recall, which considers both the accuracy and the identification ability of the model.Bias refers to the extent to which a model makes incorrect assumptions or oversimplifies the problem, and it is related to the model‚Äôs prediction accuracy. A lower bias indicates that the model can better fit the training data and is closer to the true underlying relationship.</p>
<p>The F1 score aims to consider both the precision and recall of the model. For certain applications, we are concerned with both the model‚Äôs prediction accuracy (precision) and its ability to identify positive instances (recall). Therefore, a higher F1 score indicates that the model performs well in balancing prediction accuracy and identification ability.</p>
<p>All these metrics are primarily used to measure the performance of a model on a specific dataset, while model bias typically refers to the systematic deviation of the model from the trends in the dataset, which may affect the model‚Äôs ability to generalize.</p>
<p>Then is there any way to indirectly indicate the bias of a model?</p>
<p>Analyzing the difference between training error and validation error, Holdout Method,Cross-Validation, and Bootstrapping are all viable approaches.</p>
<p>So what are these method?</p>
<section id="holdout-method">
<h3><span class="section-number">20.4.1. </span>Holdout Method<a class="headerlink" href="#holdout-method" title="Permalink to this headline">#</a></h3>
<p>Splitting the dataset into mutually exclusive training and testing sets, using the training set to train the model, and then evaluating the model‚Äôs performance using the testing set. By comparing the performance on different models using the validation set, we can select the best-performing model. The sampling criteria require stratified sampling, which means dividing the data proportionally based on data types.</p>
<p>However, since different partitioning methods yield different data samples, the results of model evaluation also differ. Typically, we choose a large portion of the dataset (70-80%) as the training set and the remaining portion as the testing set.
By splitting the dataset, we can observe that the testing set only represents a small portion of the total dataset, which can lead to unstable evaluation results.</p>
</section>
<section id="cross-validation">
<h3><span class="section-number">20.4.2. </span>Cross-Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">#</a></h3>
<p>Splitting the dataset into K mutually exclusive subsets (K-fold cross-validation), using each subset as a validation set in turn and the remaining subsets as training sets to train the model and evaluate its performance. By averaging or aggregating the results from K validations, the best model can be selected.</p>
<p>The stability and fidelity of the results in cross-validation evaluation method largely depend on the value of K. Additionally, when the sample size is small but can be clearly separated, leave-one-out method (LOOCV) can be used.</p>
<p>Cross-validation provides high precision, but it can be time-consuming when dealing with large datasets.</p>
<p>In general, using 10-fold cross-validation is sufficient to indirectly assess the generalization ability of a model.</p>
</section>
<section id="bootstrapping">
<h3><span class="section-number">20.4.3. </span>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Permalink to this headline">#</a></h3>
<p>Bootstrapping, also known as resampling or sampling with replacement, is a technique where each time a copy of a sample is selected from a dataset containing m samples and added to the resulting dataset. This process is repeated m times, resulting in a dataset with m samples. (Some samples may appear multiple times in the resulting dataset.) This resulting dataset is then used as the training set.</p>
<p>Since the sampling is conducted independently, the probability that a specific sample is never selected in m iterations of sampling is <span class="math notranslate nohighlight">\( [(1-\frac{1}{m})^m] \)</span>. As m approaches infinity, <span class="math notranslate nohighlight">\( lim_{m \to \infty} (1 - \frac{1}{m})^m = \frac{1}{e} \)</span> the limit of this probability is <span class="math notranslate nohighlight">\(1/e\)</span> , where e is the base of the natural logarithm and approximately equal to 2.71828. Therefore, when m is sufficiently large, the probability that a specific sample is never selected in m iterations of sampling is close to <span class="math notranslate nohighlight">\(\frac{1}{e} ‚âà 0.36787944117\)</span> .</p>
</section>
</section>
<section id="interpreting-the-learning-curves">
<h2><span class="section-number">20.5. </span>Interpreting the Learning Curves<a class="headerlink" href="#interpreting-the-learning-curves" title="Permalink to this headline">#</a></h2>
<p>You might think about the information in the training data as being of two kinds: <em>signal</em> and <em>noise</em>. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is <em>only</em> true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can‚Äôt actually help the model make predictions. The noise is the part might look useful but really isn‚Äôt.</p>
<p>We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model‚Äôs performance, we need to evaluate it on a new set of data, the <em>validation</em> data.</p>
<p>When we train a model we‚Äôve been plotting the loss on the training set epoch by epoch. To this we‚Äôll add a plot the validation data too. These plots we call the <strong>learning curves</strong>. To train deep learning models effectively, we need to be able to interpret them.</p>
<p>Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won‚Äôt generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a <em>gap</em> is created in the curves. The size of the gap tells you how much noise the model has learned.</p>
<p>Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.</p>
<p>This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn‚Äôt learned enough <em>signal</em>. Overfitting the training set is when the loss is not as low as it could be because the model learned too much <em>noise</em>. The trick to training deep learning models is finding the best balance between the two.</p>
<p>We‚Äôll look at a couple ways of getting more signal out of the training data while reducing the amount of noise later.</p>
<p>Let‚Äôs first take a look at a learning curve. In this part we‚Äôre using a datasets called iris in scikit-learn.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a note of a learning curve by using the iris dataset in sklearn</span>

<span class="kn">import</span> <span class="nn">warnings</span>

<span class="c1"># Ignore warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">learning_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Feature scaling</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Define a logistic regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># Define the range of training set sizes</span>
<span class="n">train_sizes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Generate learning curve data using the learning_curve function</span>
<span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores</span><span class="p">,</span> <span class="n">test_scores</span> <span class="o">=</span> <span class="n">learning_curve</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_sizes</span><span class="o">=</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Calculate the average accuracy for the training and test sets</span>
<span class="n">train_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_scores_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot the learning curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning Curve&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Training Examples&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>

<span class="c1"># Plot the accuracy curves for the training and test sets</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">train_scores_mean</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_sizes</span><span class="p">,</span> <span class="n">test_scores_mean</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Cross-validation Accuracy&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/model-selection_16_0.png" src="../_images/model-selection_16_0.png" />
</div>
</div>
<p>First of all, let‚Äôs take a look at a plot, this is a simple learning curve using an iris dataset in sklearn.dataset. We can simply notice the two curve we plot fells far apart when we have less examples, and when we enlarge the training examples we can see the two lines are approaching convergence.</p>
<p>Why?</p>
<p>To train a model, it is necessary to have a sufficient number of samples so that it can generalize patterns from the data. Assuming we have a function y=f(x), essentially, machine learning algorithms summarize and fit the f function based on a large number of (x, y) pairs. Therefore, if you have too few (x, y) pairs, the algorithm will not be able to summarize the function effectively. This is the impact of the sample size on the degree of fitting.</p>
</section>
<section id="capacity">
<h2><span class="section-number">20.6. </span>Capacity<a class="headerlink" href="#capacity" title="Permalink to this headline">#</a></h2>
<p>A model‚Äôs <strong>capacity</strong> refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.</p>
<p>You can increase the capacity of a network either by making it <em>wider</em> (more units to existing layers) or by making it <em>deeper</em> (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.</p>
<p>You‚Äôll explore how the capacity of a network can affect its performance in the exercise.</p>
<p>Determining an appropriate model capacity is a crucial task in model selection. Here are some common methods and guidelines to help determine the right model capacity:</p>
<p><strong>Rule of thumb</strong>: In general, if the dataset is small or the task is relatively simple, choosing a lower-capacity model may be more suitable to avoid overfitting. For larger datasets or complex tasks, a higher-capacity model may be able to better fit the data.</p>
<p><strong>Cross-validation</strong>: This method has been mentioned earlier in the previous text, and it is an extremely important approach in model selection. Therefore, it is necessary to mention this method multiple times and gain a deeper understanding of it.</p>
<p><strong>Learning curves</strong>: Learning curves can help determine if the model capacity is appropriate. By plotting the performance of the model on the training set and the validation set as the number of training samples increases, one can observe the model‚Äôs fitting and generalization abilities. If the model performs poorly on both the training set and the validation set, it may be underfitting due to low capacity. If the model performs well on the training set but poorly on the validation set, it may be overfitting due to high capacity. Adjustments to the model capacity can be made based on the trend of the learning curve.</p>
<p><strong>Regularization</strong>: Adjusting the model capacity through regularization techniques (which we will also mention in the text later). Increasing the regularization parameter can reduce model capacity and decrease the risk of overfitting. Decreasing the regularization parameter can increase model capacity and improve fitting ability. By evaluating the model performance on the validation set with different regularization parameters, an appropriate regularization parameter value can be chosen.</p>
<p><strong>Model comparison experiments</strong>: Train and evaluate models with different capacities and compare their performance on the validation set. By comparing the generalization performance of different-capacity models, select the model capacity with the best performance.</p>
<p>Considering the above methods and guidelines, selecting an appropriate model capacity requires a balance between theory and practice and decision-making based on the specific problem and available resources. The ultimate goal is to choose a model that performs well on both the training data and new data, achieving good generalization ability.</p>
</section>
<section id="l1-and-l2-regularization">
<h2><span class="section-number">20.7. </span>L1 and L2 Regularization<a class="headerlink" href="#l1-and-l2-regularization" title="Permalink to this headline">#</a></h2>
<p>You may be familiar with Occam‚Äôs Razor principle: given two explanations for something, the explanation most likely to be correct is the ‚Äòsimplest‚Äô one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simple models are less likely to overfit than complex ones.</p>
<p>A ‚Äòsimple model‚Äô in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parmeters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weeights only to  take small values, which makes the distribution of weight values more ‚Äòregular‚Äô. This is called ‚Äòweight regularization‚Äô, and it is done by adding to the loss function of the network a cost associated with having large weights.</p>
<p>Let‚Äôs consider a target function with a regularization term, which can be represented as:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = L(\theta) + \lambda R(\theta)\]</div>
<p>Here, <span class="math notranslate nohighlight">\(J(\theta)\)</span> is the target function, <span class="math notranslate nohighlight">\(\theta\)</span> represents the model‚Äôs parameters, <span class="math notranslate nohighlight">\(L(\theta)\)</span> is the loss function (typically the model‚Äôs error on the training data), <span class="math notranslate nohighlight">\(R(\theta)\)</span> is the regularization term, and \lambda is the regularization parameter.</p>
<p>The loss function <span class="math notranslate nohighlight">\(L(\theta)\)</span> measures how well the model fits the training data, and our goal is to minimize it. The regularization term <span class="math notranslate nohighlight">\(R(\theta)\)</span> constrains or penalizes the values of the model‚Äôs parameters, and it controls the complexity of the model.</p>
<p>The regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span> determines the weight of the regularization term in the target function. When <span class="math notranslate nohighlight">\(\lambda\)</span> approaches <span class="math notranslate nohighlight">\(\theta\)</span>, the impact of the regularization term becomes negligible, and the model‚Äôs objective is primarily to minimize the loss function. On the other hand, when <span class="math notranslate nohighlight">\(\lambda\)</span> approaches infinity, the regularization term‚Äôs impact becomes significant, and the model‚Äôs objective is to minimize the regularization term as much as possible, leading to parameter values tending towards zero.</p>
<p>There are two forms of this cost: L1 regularization (also known as Lasso regression) with the regularization term <span class="math notranslate nohighlight">\(R(\theta)\)</span> represented as the sum of the absolute values of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>: <span class="math notranslate nohighlight">\(R(\theta) = ||\theta||_1\)</span>. L1 regularization can induce certain parameters of the model to become zero, thereby achieving feature selection and sparsity.</p>
<p>L2 regularization (also known as Ridge regression) with the regularization term <span class="math notranslate nohighlight">\(R(\theta)\)</span> represented as the square root of the sum of the squares of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>: <span class="math notranslate nohighlight">\(R(\theta) = ||\theta||_2\)</span>. L2 regularization encourages the parameter values of the model to gradually approach zero but not exactly become zero, hence it does not possess the ability for feature selection.</p>
<p>In <code class="docutils literal notranslate"><span class="pre">tf.keras</span></code>, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let‚Äôs add L2 weight regularization now.</p>
<div class="math notranslate nohighlight">
\[L2\ Loss = Loss + \textcolor{red}{\lambda}\sum_{i} w_i^2\]</div>
<div class="math notranslate nohighlight">
\[L1\ Loss = Loss + \textcolor{red}{\lambda}\sum_{i} \lvert w \rvert\]</div>
<figure class="align-default" id="circlesquare-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/circlesquare.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/circlesquare.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.6 </span><span class="caption-text">L1 and L2 regularization</span><a class="headerlink" href="#circlesquare-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Both are very common regularization techniques, but they are suitable for different scenarios. L1 regularization is suitable for situations that require feature selection or demand model interpretability. On the other hand, L2 regularization is more general and applicable in most cases to prevent overfitting and improve model generalization ability.</p>
</section>
<section id="early-stopping">
<h2><span class="section-number">20.8. </span>Early Stopping<a class="headerlink" href="#early-stopping" title="Permalink to this headline">#</a></h2>
<p>We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn‚Äôt decreasing anymore. Interrupting the training this way is called <strong>early stopping</strong>.</p>
<p>Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won‚Äôt continue to learn noise and overfit the data.</p>
<p>Training with early stopping also means we‚Äôre in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent <em>underfitting</em> from not training long enough. Just set your training epochs to some large number (more than you‚Äôll need), and early stopping will take care of the rest.</p>
</section>
<section id="adding-early-stopping">
<h2><span class="section-number">20.9. </span>Adding Early Stopping<a class="headerlink" href="#adding-early-stopping" title="Permalink to this headline">#</a></h2>
<p>In Keras, we include early stopping in our training through a callback. A <strong>callback</strong> is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks">a variety of useful callbacks</a> pre-defined, but you can <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback">define your own</a>, too.)</p>
<figure class="align-default" id="earlystopping-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/traintestoverfitting.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/traintestoverfitting.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.7 </span><span class="caption-text">Early stopping</span><a class="headerlink" href="#earlystopping-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="the-impact-of-the-value-of-lambda">
<h2><span class="section-number">20.10. </span>The impact of the value of <span class="math notranslate nohighlight">\(\lambda\)</span><a class="headerlink" href="#the-impact-of-the-value-of-lambda" title="Permalink to this headline">#</a></h2>
<p>We notice that the objective function contains not only the regularization term but also the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>The selection of the regularization parameter is an important part of regularization, and it needs to be fine-tuned during the model training process.</p>
<figure class="align-default" id="impact-of-lambda-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/lagrange-animation.gif" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/lagrange-animation.gif" />
<figcaption>
<p><span class="caption-number">Fig. 20.8 </span><span class="caption-text">The impact of the value of <span class="math notranslate nohighlight">\(\lambda\)</span></span><a class="headerlink" href="#impact-of-lambda-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The value of <span class="math notranslate nohighlight">\(\lambda\)</span> has a significant impact on weight regularization.</p>
<p>When <span class="math notranslate nohighlight">\(\lambda\)</span> is small, the effect of weight regularization is relatively minor. The network is more likely to learn complex patterns and structures, which can lead to overfitting. This means that the model may perform well on the training data but have poor generalization on new data.</p>
<p>When <span class="math notranslate nohighlight">\(\lambda\)</span> is large, the effect of weight regularization becomes more pronounced. The network is constrained to simpler patterns and structures, reducing the risk of overfitting. This can improve the model‚Äôs generalization on new data but may result in a slight decrease in performance on the training data.</p>
<p>Choosing the appropriate value of <span class="math notranslate nohighlight">\(\lambda\)</span> requires adjustment and optimization based on the specific problem and dataset. Typically, cross-validation or other evaluation methods can be used to select the optimal <span class="math notranslate nohighlight">\(\lambda\)</span> value, finding a balance between model complexity and generalization ability.</p>
<p>When using regularization during model training, its effect can be better understood. Let‚Äôs take the example of linear regression.</p>
<p>Suppose we have a dataset containing house area and prices, and we want to use a linear regression model to predict house prices. We can define a linear regression model that includes an intercept term and a coefficient for the house area.</p>
<p>Without regularization, the objective of the model is to minimize the mean squared error (MSE) on the training data. This means the model will try to find the best-fitting line in the training data to minimize the differences between the predicted values and the actual values.</p>
<p>However, if the training data contains noise or outliers, or if the training set is relatively small, the model may overfit the data, leading to a decrease in prediction performance on new data. In such cases, regularization can help control the complexity of the model and reduce the risk of overfitting.</p>
<p>By adding L2 regularization (Ridge regularization) to the linear regression model, we introduce the square of the L2 norm of the parameters as a penalty term in the loss function. This encourages the model to prefer smaller parameter values during training, preventing the parameters from becoming too large.</p>
<p>The effect of regularization is achieved by balancing the trade-off between minimizing the training error and minimizing the penalty term. A larger regularization parameter will penalize larger parameter values more strongly, making the model smoother and reducing the differences between parameters. This helps reduce the risk of overfitting and improves the model‚Äôs generalization ability on new data.</p>
<p>In summary, the role of regularization in linear regression models is to control the complexity of the model, reduce the risk of overfitting, and improve the model‚Äôs generalization ability on new data.</p>
<p>In this section, we primarily utilize learning curves to optimize the regularization parameter, also known as the learning curve.</p>
</section>
<section id="dropout">
<h2><span class="section-number">20.11. </span>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h2>
<p>Dropout is one of the most effective and most commonly used regularization techniques for neural network, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly ‚Äúdropping out‚Äù (i.e. set to zero) a number of output features of the layer during training. Let‚Äôs say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; aafter applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. The ‚Äòdropout rate‚Äô is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer‚Äôs output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.</p>
<p>In tf.keras you can introduce a dropout in a network via the Dropout layer, which gets applied to the output of layer right before.</p>
<figure class="align-default" id="dropout-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/dropoutgif.gif" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/dropoutgif.gif" />
<figcaption>
<p><span class="caption-number">Fig. 20.9 </span><span class="caption-text">Dropout</span><a class="headerlink" href="#dropout-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="prediction-after-dropout">
<h3><span class="section-number">20.11.1. </span>Prediction after dropout<a class="headerlink" href="#prediction-after-dropout" title="Permalink to this headline">#</a></h3>
<figure class="align-default" id="prediction-after-dropout-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/kUc8r.jpg" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/kUc8r.jpg" />
<figcaption>
<p><span class="caption-number">Fig. 20.10 </span><span class="caption-text">Prediction after dropout</span><a class="headerlink" href="#prediction-after-dropout-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>During training, p neuron activations (usually, p=0.5, so 50%) are dropped. Doing this at the testing stage is not our goal (the goal is to achieve a better generalization). From the other hand, keeping all activations will lead to an input that is unexpected to the network, more precisely, too high (50% higher) input activations for the following layer</p>
<p>Consider the neurons at the output layer. During training, each neuron usually get activations only from two neurons from the hidden layer (while being connected to four), due to dropout. Now, imagine we finished the training and remove dropout. Now activations of the output neurons will be computed based on four values from the hidden layer. This is likely to put the output neurons in unusual regime, so they will produce too large absolute values, being overexcited</p>
<p>To avoid this, the trick is to multiply the input connections‚Äô weights of the last layer by 1-p (so, by 0.5). Alternatively, one can multiply the outputs of the hidden layer by 1-p, which is basically the same</p>
</section>
</section>
<section id="conclusions">
<h2><span class="section-number">20.12. </span>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">#</a></h2>
<figure class="align-default" id="training-size-matters-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/ZahidHasan.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/ZahidHasan.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.11 </span><span class="caption-text">Training size matters</span><a class="headerlink" href="#training-size-matters-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="steps-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/steps.png" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/steps.png" />
<figcaption>
<p><span class="caption-number">Fig. 20.12 </span><span class="caption-text">How to choose a good model</span><a class="headerlink" href="#steps-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The above image illustrates well why we consider bias as an important aspect in model selection and even in machine learning. When our model understands the signal, its improvement is positive. However, once the model starts to understand the noise, the bias of the model starts to increase. This is where cross-validation, mentioned earlier, comes into play.</p>
<figure class="align-default" id="conclusion-ms">
<img alt="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/Bias-vs.webp" src="https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/Bias-vs.webp" />
<figcaption>
<p><span class="caption-number">Fig. 20.13 </span><span class="caption-text">Conclusion</span><a class="headerlink" href="#conclusion-ms" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The purpose of model selection is to choose the best model among multiple candidate models for a given machine learning problem. The best model refers to the one that performs well on the training data and has good generalization ability to unseen new data.</p>
<p>The importance of model selection lies in the fact that different models may have different adaptability to the nature of the data and the complexity of the problem. Selecting an appropriate model can improve the model‚Äôs prediction accuracy, robustness, and interpretability.</p>
</section>
<section id="your-turn">
<h2><span class="section-number">20.13. </span>Your turn! üöÄ<a class="headerlink" href="#your-turn" title="Permalink to this headline">#</a></h2>
<p>Machine learning model selection and dealing with overfitting and underfitting are crucial aspects of the machine learning pipeline. In this assignment, you‚Äôll have the opportunity to apply your understanding of these concepts and techniques.
Please complete the following tasks:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../assignments/ml-advanced/model-selection/model-selection-assignment-1.html"><span class="doc std std-doc">model-selection-assignment-1</span></a></p></li>
<li><p><a class="reference internal" href="../assignments/ml-advanced/model-selection/lasso-and-ridge-regression.html"><span class="doc std std-doc">lasso-and-ridge-regression</span></a></p></li>
<li><p><a class="reference internal" href="../assignments/ml-advanced/model-selection/dropout-and-batch-normalization.html"><span class="doc std std-doc">dropout-and-batch-normalization</span></a></p></li>
<li><p><a class="reference internal" href="../assignments/ml-advanced/model-selection/learning-curve-to-identify-overfit-underfit.html"><span class="doc std std-doc">learning-curve-to-identify-overfit-underfit</span></a></p></li>
<li><p><a class="reference internal" href="../assignments/ml-advanced/model-selection/regularized-linear-models.html"><span class="doc std std-doc">regularized-linear-models</span></a></p></li>
</ul>
</section>
<section id="self-study">
<h2><span class="section-number">20.14. </span>Self study<a class="headerlink" href="#self-study" title="Permalink to this headline">#</a></h2>
<p>Here are some recommended open-source and free model selection projects on GitHub, you can refer to them for further study:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/automl/auto-sklearn">An automated machine learning tool(AutoML), by aron-bram</a></p></li>
<li><p><a class="reference external" href="https://github.com/modelhub-ai/modelhub">An open platform ModelHub, by 9zelle9</a></p></li>
</ul>
</section>
<section id="acknowledgments">
<h2><span class="section-number">20.15. </span>Acknowledgments<a class="headerlink" href="#acknowledgments" title="Permalink to this headline">#</a></h2>
<p>Thanks to xyb for organizing the content related to model selection and for their suggestion to concretize abstract concepts.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "ocademy-ai/machine-learning",
            ref: "release",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ml-advanced"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
    <script type="text/javascript">
        function init() {
            WaveDrom.ProcessAll();
        }
        window.onload = init;
    </script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="unsupervised-learning-pca-and-clustering.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">19. </span>Unsupervised learning: PCA and clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../deep-learning/dl-overview.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Intro to Deep Learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Ocademy<br/>
  
      &copy; Copyright 2022-2023.<br/>
    <div class="extra_footer">
      <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> Text content of this work is licensed under the <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>