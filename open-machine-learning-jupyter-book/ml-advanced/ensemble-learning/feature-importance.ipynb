{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e865bf1-8eea-44ee-a6b3-0c9570faa8e2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "---\n",
    "jupytext:\n",
    "  cell_metadata_filter: -all\n",
    "  formats: md:myst\n",
    "  text_representation:\n",
    "    extension: .md\n",
    "    format_name: myst\n",
    "    format_version: 0.13\n",
    "    jupytext_version: 1.11.5\n",
    "kernelspec:\n",
    "  display_name: Python 3\n",
    "  language: python\n",
    "  name: python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831b6f8-79ef-4d1b-b963-47422c056731",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba051632",
   "metadata": {},
   "source": [
    "# Feature importance\n",
    "\n",
    "It's quite often that you want to make out the exact reasons of the algorithm outputting a particular answer. Or at the very least to find out which input features contributed most to the result. With Random Forest, you can obtain such information quite easily.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "From the picture below, it is intuitively clear that, in our credit scoring problem, *Age* is much more important than *Income*. This can be formally explained using the concept of *information gain*.\n",
    "\n",
    ":::{figure}https://habrastorage.org/webt/va/mm/9l/vamm9luoz1oqa4op74egpttaggy.png\n",
    "---\n",
    "name: 'example of credit scoring problem' \n",
    "width: 90%\n",
    "---\n",
    "Example of credit scoring problem\n",
    ":::\n",
    "\n",
    "In the case of many decision trees or a random forest, the closer the mean position of a feature over all the trees to the root, the more significant it is for a given classification or regression problem. Gains in the splitting criterion, such as the *Gini impurity*, obtained at each optimal split in every tree is a measure of importance that is directly associated with the splitting feature. The value of this score is distinct for each feature and accumulates over all the trees.\n",
    "\n",
    "Let's go a little deeper into the details. \n",
    "\n",
    "There exist a lot of methods to assess feature importances. Leo Breinman in his works suggested to evaluate the importance of a variable by measuring decrease of accuracy of the forest when the variable is randomly permuted or decrease of impurity of a nodes where the given variable is used for splitting. The former method is often called **permutation importance**. The latter method is used in `sklearn`.\n",
    "\n",
    "### Permutation importance\n",
    "\n",
    "Inspired by this [article](https://www.researchgate.net/publication/5231126_Conditional_Variable_Importance_for_Random_Forests).\n",
    "The average reduction in accuracy caused by a variable is determined during the calculation of the out-of-bag error. The greater the reduction in accuracy due to an exclusion or permutation of the variable, the higher its *importance score*. For this reason, variables with a greater average reduction in accuracy are generally more significant for classification.\n",
    "\n",
    "The rationale for calculating permutation importance is the following: By randomly permuting the predictor variable $X_j$, its original association with the response $Y$ is broken. When the permuted variable $X_j$, together with all the others non-permuted variables, is used the response for the out-of-bag observations, the prediction *accuracy* decreases substantially if the original $X_j$ was associated with response. Thus, as a measure of variable importance, the difference in prediction accuracy before and after permuting is used.\n",
    "\n",
    "More formally: denote $\\overline{\\mathfrak{B}}^{(t)}$ as the out-of-bag sample for a tree $t$, for $t\\in\\{1, ..., N\\}$ where $N$ is the number of trees in ensemble. Then the permutation importance of variable $X_j$ in tree $t$ is \n",
    "\n",
    "$${PI}^{(t)}\\left(X_j\\right)=\\frac{\\sum_{i\\in\\overline{\\mathfrak{B}}^{(t)}}I\\left(y_i=\\hat{y}_i^{(t)}\\right)}{\\left|\\overline{\\mathfrak{B}}^{(t)}\\right|}-\\frac{\\sum_{i\\in\\overline{\\mathfrak{B}}^{(t)}}I\\left(y_i=\\hat{y}_{i,\\pi_j}^{(t)}\\right)}{\\left|\\overline{\\mathfrak{B}}^{(t)}\\right|}$$ \n",
    "\n",
    "where $\\hat{y}_i^{(t)}=f^{(t)}(\\mathbf{x}_i)$ is the predicted class for observation $i$ before and $\\hat{y}_{i, \\pi_j}^{(t)}=f^{(t)}(\\mathbf{x}_{i,\\pi_j})$ is the predicted class for observation $i$ after permuting $X_j$, $\\mathbf{x}_{i,\\pi_j}=\\left(x_{i,1}, ..., x_{i,j-1},x_{\\pi_j(i),j},x_{i,j+1},...,x_{i,p}\\right)$\n",
    "\n",
    "Note that by definition ${PI}^{(t)}=0$, if variable $X_j$ isn't in tree $t$.\n",
    "\n",
    "Now, we can give the feature importance calculation for ensembles:\n",
    "* not normalized:\n",
    "\n",
    "$${PI}\\left(X_j\\right)=\\frac{\\sum_{t=1}^N {PI}^{(t)}(X_j)}{N}$$\n",
    "* normalized by the standard deviation of the differences:\n",
    "\n",
    "$$z_j=\\frac{{PI}\\left(X_j\\right)}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}$$\n",
    "\n",
    "## Illustrating permutation importance\n",
    "\n",
    "Let's assume that we have a toy dataset with 10 instances. Target variable can be either **'N'** or **'P'**.\n",
    "\n",
    "$$\\begin{array}{c|c|c|c|c|c|c|c|c|c}\n",
    "  \\text{Instances}, i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & \\\\ \n",
    "  \\hline\n",
    "  y_i & N & P & P & N & N & P & N & N & N & P \\\\\n",
    " \\end{array}$$\n",
    " \n",
    "We build an ensemble of 5 trees $t$, for $t\\in\\{1, ..., 5\\}$. For each tree we get out-of-bag sample (denoted $\\overline{\\mathfrak{B}}^{(t)}$ above). For example for the first tree out-of-bag sample consists of instances # 2, 4, 5, and 6.\n",
    "\n",
    "$$\\begin{array}{c|c|c|c|c|c|c|c|c|c}\n",
    "  \\text{Tree 1} & \\text{Bootstrap-sample 1} & 10 & 9 & 7 & 8 & 1 & 3 & 9 & 10 & 10 & 7\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 2} & \\text{Bootstrap-sample 2} & 4 & 8 & 5 & 8 & 3 & 9 & 2 & 6 & 1 & 6\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 3} & \\text{Bootstrap-sample 3} & 6 & 2 & 6 & 10 & 2 & 10 & 3 & 6 & 5 & 1\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 4} & \\text{Bootstrap-sample 4} & 6 & 7 & 8 & 10 & 6 & 10 & 9 & 10 & 8 & 2\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 5} & \\text{Bootstrap-sample 5} & 5 & 8 & 1 & 8 & 5 & 7 & 10 & 1 & 10 & 9\\\\\n",
    " \\end{array}$$\n",
    " \n",
    "Thus, out-of-bag samples for each tree $t$ are\n",
    "\n",
    "$$\\begin{array}{c|cccc}\n",
    "  \\text{Tree}, t & \\overline{\\mathfrak{B}}^{(t)} \\\\\n",
    "  \\hline\n",
    "  \\text{Tree 1} & 2 & 4 & 5 & 6\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 2} & 7 & 10\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 3} & 4 & 7 & 8 & 9\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 4} & 1 & 3 & 4 & 5\\\\\n",
    "  \\hline\n",
    "  \\text{Tree 5} & 2 & 3 & 4 & 6\\\\\n",
    "  \\hline\n",
    " \\end{array}$$\n",
    " \n",
    " Suppose that we have four features $X_j$, $j\\in\\{1, 2, 3, 4\\}$ and we'd like to compute _permutation importance_ for $X_2$. First, for each out-of-bag sample we compute _accuracy_ of the model before and after permutation of the values of $X_2$.\n",
    "\n",
    "For instance, before permutation for $\\overline{\\mathfrak{B}}^{(1)}$ we have\n",
    "\n",
    "$$\\begin{array}{c|cccc|cc|c}\n",
    "   & X_1 & \\color{red}{X_2} & X_3 & X_4 & y_i & \\hat{y}_i & I\\left(y_i=\\hat{y}_i\\right)\\\\\n",
    "  \\hline\n",
    "  \\textbf{2} & 1 & \\color{red}2 & 11 & 101 & \\textbf{P} & \\textbf{P} & 1\\\\\n",
    "  \\hline\n",
    "  \\textbf{4} & 2 & \\color{red}3 & 12 & 102 & \\textbf{N} & \\textbf{P} & 0\\\\\n",
    "  \\hline\n",
    "  \\textbf{5} & 3 & \\color{red}5 & 13 & 103 & \\textbf{N} & \\textbf{N} & 1\\\\\n",
    "  \\hline\n",
    "      \\textbf{6} & 4 & \\color{red}7 & 14 & 104 & \\textbf{P} & \\textbf{P} & 1\\\\\n",
    " \\end{array}$$\n",
    " \n",
    "Thus, the accuracy before permutation is $3/4=0.75$.\n",
    " \n",
    "After permutation for $\\overline{\\mathfrak{B}}^{(1)}$ we have\n",
    "\n",
    "$$\\begin{array}{c|cccc|cc|c}\n",
    "   & X_1 & \\color{red}{X_2} & X_3 & X_4 & y_i & \\hat{y}_i & I\\left(y_i=\\hat{y}_i\\right)\\\\\n",
    "  \\hline\n",
    "  \\textbf{2} & 1 & \\color{red}5 & 11 & 101 & \\textbf{P} & \\textbf{P} & 0\\\\\n",
    "  \\hline\n",
    "  \\textbf{4} & 2 & \\color{red}7 & 12 & 102 & \\textbf{N} & \\textbf{P} & 0\\\\\n",
    "  \\hline\n",
    "  \\textbf{5} & 3 & \\color{red}2 & 13 & 103 & \\textbf{N} & \\textbf{N} & 1\\\\\n",
    "  \\hline\n",
    "      \\textbf{6} & 4 & \\color{red}3 & 14 & 104 & \\textbf{P} & \\textbf{P} & 1\\\\\n",
    " \\end{array}$$\n",
    " \n",
    "The accuracy after permutation is $2/4=0.50$.\n",
    "\n",
    "Then the difference between accuracies is computed.\n",
    "\n",
    "The above mentioned steps are to be done for each out-of-bag sample $\\overline{\\mathfrak{B}}^{(t)}$. To get not normalized _permutation importance_ we sum all computed differences and divide by the number of trees. Normalization is done by dividing _not normalized permutation importance_ by standard error.\n",
    "\n",
    "## Sklearn Random Forest Feature Importance\n",
    "\n",
    "Inspired by [this](https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3) article.\n",
    "Sklearn library uses another approach to determine feature importance. The rationale for that method is that the more gain in information the node (with splitting feature $X_j$) provides, the higher its importance.\n",
    "\n",
    "The average reduction in the Gini impurity – or MSE for regression – represents the contribution of each feature to the homogeneity of nodes and leaves in the resulting Random Forest model. Each time a selected feature is used for splitting, the Gini impurity of the child nodes is calculated and compared with that of the original node.\n",
    "\n",
    "Gini impurity is a score of homogeneity with the range from  0  (homogeneous) to  1  (heterogeneous). The changes in the value of the splitting criterion are accumulated for each feature and normalized at the end of the calculation. A higher reduction in the Gini impurity signals that splitting results by this feature results in nodes with higher purity.\n",
    "\n",
    "The algorithm of obtaining feature importance may be represented with the following sequence of steps:\n",
    "\n",
    "1. For each tree $t$ in ensemble $t\\in\\{1,...,N\\}$:\n",
    "\n",
    "  1.1.  for each node $i$ calculate the reduction in impurity (or MSE, or entropy) as ${RI}_i^{(t)}=w_i^{(t)}\\cdot I_i^{(t)} - w_{LEFT_i}^{(t)}\\cdot I_{LEFT_i}^{(t)}-w_{RIGHT_i}^{(t)}\\cdot I_{RIGHT_i}^{(t)}$, where:\n",
    "      - $w_i^{(t)}$, $w_{LEFT_i}^{(t)}$, and $w_{RIGHT_i}^{(t)}$ are respectively weighted number of samples reaching   node $i$ in tree $t$, and its left $LEFT_i$ and right $RIGHT_i$ children\n",
    "      - $I_i^{(t)}$, $I_{LEFT_i}^{(t)}$,   $I_{RIGHT_i}^{(t)}$ are impurity of the nodes. For leaves ${RI}_i^{(t)}$ is equal to 0.\n",
    "\n",
    "  1.2.  for each feature $j$ calculate its importance in that particular tree as\n",
    "  \n",
    "$${FI}_j^{(t)}=\\frac{\\sum_{i:\\text{node }i\\text{ splits on feature } j}{RI}_i^{(t)}}{\\sum_{i\\in\\text{all nodes}}{RI}_i^{(t)}}$$\n",
    "\n",
    "   That means that in numerator we sum the reduction in impurity only in those nodes where feature $j$ is situated.\n",
    "2. Calculate the average feature importances over all trees in ensemble:\n",
    "\n",
    "$${FI}_j=\\frac{\\sum_{t=1}^N {FI}_j^{(t)}}{N}$$\n",
    "\n",
    "Those are pretty confusing formulas so let's demonstrate each step with the Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59dcfd",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "iris_data = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a56379",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "data = iris_data.iloc[:, :-1]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b600606",
   "metadata": {},
   "source": [
    "Since our aim is just to demonstrate the sequence of steps in calculating feature importances we'll transform the `target` variable as for classifying Iris Virginica One-To-All."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38d4c9",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "target = iris_data.iloc[:, -1]\n",
    "target = target.map({'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 1})\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0215eb",
   "metadata": {},
   "source": [
    "Creating Random Forest. For reproducibility, we set `random_state=17`. For the sake of simplicity we set the number of trees to 3 and limit the depth of trees in ensemble to be not greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35991c93",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=3, max_depth=3, random_state=17)\n",
    "rfc.fit(data, target);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eecaada",
   "metadata": {},
   "source": [
    "After fitting list of all the trees are stored in `estimators_` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0222f",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "tree_list = rfc.estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90532aa5",
   "metadata": {},
   "source": [
    "Visualizing trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96bebe",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "tree.plot_tree(\n",
    "    tree_list[0],\n",
    "    filled=True,\n",
    "    feature_names=data.iloc[0, :].tolist(),\n",
    "    class_names=[\"Y\", \"N\"],\n",
    "    node_ids=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f710d62",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "tree.plot_tree(\n",
    "    tree_list[1],\n",
    "    filled=True,\n",
    "    feature_names=data.iloc[0, :].tolist(),\n",
    "    class_names=[\"Y\", \"N\"],\n",
    "    node_ids=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030ff0d",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "tree.plot_tree(\n",
    "    tree_list[2],\n",
    "    filled=True,\n",
    "    feature_names=data.iloc[0, :].tolist(),\n",
    "    class_names=[\"Y\", \"N\"],\n",
    "    node_ids=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a0190",
   "metadata": {},
   "source": [
    "Let's start from the first tree and `Sepal length (cm)` feature. This feature is located in two nodes: the root (#0) and the rightmost node (#4). The reduction in impurity for these nodes are:\n",
    "\n",
    "$${RI}_{{SL}_1}^{(1)}=\\frac{150}{150}\\cdot 0.482578 - \\frac{63}{150}\\cdot 0.061476 - \\frac{87}{150}\\cdot 0.436517 = 0.203578$$\n",
    "\n",
    "$${RI}_{{SL}_2}^{(1)}=\\frac{56}{150}\\cdot 0.035077 - \\frac{7}{150}\\cdot 0.244898 - \\frac{49}{150}\\cdot 0 = 0.001667$$\n",
    "\n",
    "Note: The impurity for each node was recalculated to gain more accuracy than given in the picture.\n",
    "\n",
    "By doing the same calculations we get the following reduction in impurity for `Petal width (cm)`, and `Petal width (cm)` features:\n",
    "\n",
    "$${RI}_{PL}^{(1)}=0.035785$$\n",
    "\n",
    "$${RI}_{{PW}_1}^{(1)}=0.025820$$\n",
    "\n",
    "$${RI}_{{PW}_2}^{(1)}=0.193633$$\n",
    "\n",
    "Summarizing all numbers in table\n",
    "\n",
    "$$\\begin{array}{c|cc}\n",
    "  \\text{Feature}, j & \\text{Total }RI_j^{(1)} & {FI}_j^{(1)}\\\\ \n",
    "  \\hline\n",
    "  SL & 0.205244 & 0.445716\\\\\n",
    "  SW & 0.000000 & 0.000000\\\\\n",
    "  PL & 0.035785 & 0.077712\\\\\n",
    "  PW & 0.219453 & 0.476572\\\\\n",
    "  \\hline\n",
    "  \\sum & 0.460483\n",
    " \\end{array}$$\n",
    " \n",
    " After performing the same calculations for the second and third tree we average the results for features:\n",
    "\n",
    "$$\\begin{array}{c|ccc|c}\n",
    "  \\text{Feature}, j & {FI}_j^{(1)}& {FI}_j^{(2)}& {FI}_j^{(3)} & {FI}_j\\\\ \n",
    "  \\hline\n",
    "  SL & 0.445716 & 0.000000 & 0.000000 & 0.148572\\\\\n",
    "  SW & 0.000000 & 0.039738 & 0.000000 & 0.013246\\\\\n",
    "  PL & 0.077712 & 0.844925 & 0.162016 & 0.361551\\\\\n",
    "  PW & 0.476572 & 0.115337 & 0.837984 & 0.476631\\\\\n",
    " \\end{array}$$\n",
    " \n",
    " Let's compare our result with those stored in the `feature_importances_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names=data.iloc[0, :].tolist()\n",
    "print(feature_names)\n",
    "print(rfc.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6af4a",
   "metadata": {},
   "source": [
    "Voila!\n",
    "\n",
    "## Practical example\n",
    "\n",
    "Let's consider the results of a survey given to visitors of hostels listed on Booking.com and TripAdvisor.com. Our features here are the average ratings for different categories including service quality, room condition, value for money, etc. Our target variable is the hostel's overall rating on the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "hostel_data = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/hostel_factors.csv\")\n",
    "features = {\n",
    "    \"f1\": u\"Staff\",\n",
    "    \"f2\": u\"Hostel booking\",\n",
    "    \"f3\": u\"Check-in and check-out\",\n",
    "    \"f4\": u\"Room condition\",\n",
    "    \"f5\": u\"Shared kitchen condition\",\n",
    "    \"f6\": u\"Shared space condition\",\n",
    "    \"f7\": u\"Extra services\",\n",
    "    \"f8\": u\"General conditions & conveniences\",\n",
    "    \"f9\": u\"Value for money\",\n",
    "    \"f10\": u\"Customer Co-creation\",\n",
    "}\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=1000, max_features=10, random_state=0)\n",
    "\n",
    "forest.fit(hostel_data.drop([\"hostel\", \"rating\"], axis=1), hostel_data[\"rating\"])\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# Plot the feature importancies of the forest\n",
    "num_to_plot = 10\n",
    "feature_indices = [ind + 1 for ind in indices[:num_to_plot]]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(num_to_plot):\n",
    "    print(\n",
    "        \"%d. %s %f \"\n",
    "        % (f + 1, features[\"f\" + str(feature_indices[f])], importances[indices[f]])\n",
    "    )\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.title(u\"Feature Importance\")\n",
    "bars = plt.bar(\n",
    "    range(num_to_plot),\n",
    "    importances[indices[:num_to_plot]],\n",
    "    color=([str(i / float(num_to_plot + 1)) for i in range(num_to_plot)]),\n",
    "    align=\"center\",\n",
    ")\n",
    "ticks = plt.xticks(range(num_to_plot), feature_indices)\n",
    "plt.xlim([-1, num_to_plot])\n",
    "plt.legend(bars, [u\"\".join(features[\"f\" + str(i)]) for i in feature_indices]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b3c3c",
   "metadata": {},
   "source": [
    "The picture above shows that, more often than not, customers pay great attention to staff and the price-quality ratio. This couple of factors affects the resulting overall rating the most. The difference between these two features and other features is not very large, so we can conclude that exclusion of any of these features will lead to a reduction of model's accuracy. However, based on our analysis, we can recommend hostel owners to focus primarily on staff training and price-to-quality ratio.\n",
    "\n",
    "## Your turn! 🚀\n",
    "\n",
    "TBD\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Yury Kashnitsky](https://www.kaggle.com/kashnitsky) for creating the open-source content [Ensembles and random forest](https://www.kaggle.com/code/kashnitsky/topic-5-ensembles-part-3-feature-importance/notebook). They inspire the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
