{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f17e9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "license:\n",
    "    code: MIT\n",
    "    content: CC-BY-4.0\n",
    "github: https://github.com/ocademy-ai/machine-learning\n",
    "venue: By Ocademy\n",
    "open_access: true\n",
    "bibliography:\n",
    "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616662a5",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "## Over-fitting and under-fitting\n",
    "\n",
    "### Overview\n",
    "\n",
    "Remember that the main objective of any machine learning model is to generalize the learning based on training data, so that it will be able to do predictions accurately on unknown data. Here are a few concepts: the first is 'Hypothesis', the second is 'Truth'. When we obtain data and train it, we propose a hypothesis, and the process of forcing the hypothesis to be as close to the truth as possible is our training process. This process is called 'fitting', which means the model tries to learn the patterns, relationships, or rules in the data in order to make predictions or classifications on unknown data. Due to the existence of errors in the hypothesis, we introduce the concepts of generalization error and empirical error (training error). The generalization error represents the error in unknown samples when we fit the model to the truth. It is uncertain. On the other hand, the empirical error represents the error on the training set, and it can be determined. In order to reduce the error and approach the truth, we need model evaluation. However, due to the occurrence of overfitting, a smaller error does not necessarily indicate a better model.\n",
    "\n",
    "As you can notice the words 'Overfitting' and 'Underfitting' are kind of opposite of the term 'Generalization'. Overfitting and underfitting models don't generalize well and results in poor performance.\n",
    "\n",
    "These are the samples of over-fitting and under-fitting in regression:\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/under_over_justalright.png\n",
    "---\n",
    "name: Over-fitting-regression-ms\n",
    "---\n",
    "Over-fitting and under-fitting in regression\n",
    ":::\n",
    "\n",
    "During the fitting process, we have an important parameter called 'bias'. It refers to the deviation of the model from the true relationship when attempting to fit the data.\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "* Underfitting occurs when machine learning model don't fit the training data well enough. It is usually caused by simple function that cannot capture the underlying trend in the data.\n",
    "* Underfitting models have high error in training as well as test set. This behavior is called as 'Low Bias'\n",
    "* This usually happens when we try to fit linear function for non-linear data.\n",
    "* Since underfitting models don't perform well on training set, it's very easy to detect underfitting\n",
    "\n",
    "#### How To Avoid Underfitting?\n",
    "* Increasing the model complexity. e.g. If linear function under fit then try using polynomial features\n",
    "* Increase the number of features by performing the feature engineering\n",
    "\n",
    "### Overfitting\n",
    "* Overfitting occurs when machine learning model tries to fit the training data too well. It is usually caused by complicated function that creates lots of unnecessary curves and angles that are not related with data and end up capturing the noise in data.\n",
    "* Overfitting models have low error in training set but high error in test set. This behavior is called as 'High Variance'\n",
    "\n",
    "#### How To Avoid Overfitting?\n",
    "* Since overfitting algorithm captures the noise in data, reducing the number of features will help. We can manually select only important features or can use model selection algorithm for same\n",
    "* We can also use the 'Regularization' technique. It works well when we have lots of slightly useful features. Sklearn linear model(Ridge and LASSO) uses regularization parameter 'alpha' to control the size of the coefficients by imposing a penalty. Please refer below tutorials for more details.\n",
    "\n",
    "### Good Fitting \n",
    "* It is a sweet spot between Underfitting and Overfitting model\n",
    "* A good fitting model generalizes the learnings from training data and provide accurate predictions on new data\n",
    "* To get the good fitting model, keep training and testing the model till you get the minimum train and test error. Here important parameter is 'test error' because low train error may cause overfitting so always keep an eye on test error fluctuations. The sweet spot is just before the test error start to rise.\n",
    "\n",
    "In summary the goal of model selection is to find a model that fits the training data well and has low prediction error on new unknown data. If a model that is too simple is chosen, it may not fit the training data well, resulting in underfitting. On the other hand, if a model that is too complex is chosen, overfitting may occur, leading to a decrease in predictive performance on new data.\n",
    "Now let's take a look at another example, hoping it will be helpful for your understanding.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/classification.png\n",
    "---\n",
    "name: Over-fitting-classification-ms\n",
    "---\n",
    "Over-fitting and under-fitting in classification\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0cb84",
   "metadata": {},
   "source": [
    "\n",
    "### A simple example of linear regression \n",
    "\n",
    "This is a simple graphical representation of linear regression training. \n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-datapoints.jpg\n",
    "---\n",
    "name: Datapoints-ms\n",
    "---\n",
    "Training data points \n",
    ":::\n",
    "\n",
    "First we have some data points, then we're going to train it by linear regression.\n",
    "\n",
    "This shows how an over-fitting model fits the trainingset.  \n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-overfitting.jpg\n",
    "---\n",
    "name: Over-fitting-train-ms\n",
    "---\n",
    "Over-fitting model fits very well on training data\n",
    ":::\n",
    "\n",
    "Of course we are going to fit the model on the testset we determined.\n",
    "\n",
    "AS you can see, this over-fitting model can't fit well on the testset.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-overfitting-testdata.jpg\n",
    "---\n",
    "name: Over-fitting-test-ms\n",
    "---\n",
    "Over-fitting model fits poorly on test data \n",
    ":::\n",
    "\n",
    "Let's fit an unerfitting model on the trainingset.\n",
    "\n",
    "We can see the result very clearly on the picture that we'er 'under-fitting'.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-underfitting.jpg\n",
    "---\n",
    "name: Under-fitting-train-ms\n",
    "---\n",
    "Under-fitting model fits poorly on training data.\n",
    ":::\n",
    "\n",
    "But we can'tonly feel how it fits, we have to test it.\n",
    "\n",
    "Then let's test it on the testset.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-underfitting-test-data.jpg\n",
    "---\n",
    "name: Under-fitting-test-ms\n",
    "---\n",
    "Under-fitting model fits poorly on test data\n",
    ":::\n",
    "\n",
    "After seeing the under-fitting model and the over-fitting model we are eager to know what is a good-fitting model.\n",
    "\n",
    "Here we are\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-perfect-fit.jpg\n",
    "---\n",
    "name: Perfect-fitting-train-ms\n",
    "---\n",
    "Perfect-fitting model fits well on training data.\n",
    ":::\n",
    "\n",
    "Remember, we have to test it on the testset, and the result comes right here. It fits quit well on the testset.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/bias-variance-perfect-fit-test-data.jpg\n",
    "---\n",
    "name: Perfect-fitting-test-ms\n",
    "---\n",
    "Perfect-fitting model fits well on test data\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad081006",
   "metadata": {},
   "source": [
    "## Bias variance tradeoff\n",
    "\n",
    "In this section we talk about Bias Variance tradeoff \n",
    "\n",
    "So what is Bias and Variance? Or to say why they are so importent in model selection? \n",
    "\n",
    "Bias refers to the model's incorrect assumptions or simplifications about the problem. When a model has high bias, it may overlook some key features or patterns in the data, resulting in systematic errors in the predictions. In other words, a high-bias model tends to produce incorrect predictions.\n",
    "\n",
    "Variance refers to the sensitivity or volatility of the model to the training data. When a model has high variance, it is very sensitive to small perturbations in the training data and may overfit the noise and details in the training data, leading to poor generalization to new data. In other words, a high-variance model is more prone to the influence of randomness and produces larger prediction errors.\n",
    "\n",
    "Here are some illustrations showing the relationship between bias and variance in data fitting.\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/graphicalillustration.png\n",
    "---\n",
    "name: graphicalillustration-ms\n",
    "---\n",
    "Graphical illustration of variance and bias\n",
    ":::\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/total_error.png\n",
    "---\n",
    "name: Model-complexity-ms\n",
    "---\n",
    "Model complexity v.s. error\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23ec03",
   "metadata": {},
   "source": [
    "\n",
    "## Metrics\n",
    "\n",
    "Were there some ways that can be used to represent the bias and variance of a model?\n",
    "\n",
    "First, when we start training, how to evaluate the goodness of fit?\n",
    "\n",
    "The simplest way is to output some metrics that can substitute for bias and variance. Here are several metrics that can be used for calculation:\n",
    "\n",
    "Accuracy: Accuracy is a commonly used evaluation metric in classification models. It represents the proportion of correctly classified samples in the predictions made by the model. A higher accuracy indicates better performance. However, when there is class imbalance in the dataset, accuracy may underestimate the model's performance.\n",
    "\n",
    "Precision and Recall: Precision and recall are primarily used to evaluate the performance of binary classification models, especially in the presence of class imbalance. Precision represents the proportion of true positive samples among those predicted as positive, while recall represents the proportion of true positive samples among all actual positive samples. Precision and recall can help provide a comprehensive evaluation of the model's classification performance.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall, providing a balanced assessment of a model's accuracy and recall performance. A higher F1 score indicates better performance.\n",
    "\n",
    "Mean Squared Error (MSE): MSE is a commonly used evaluation metric in regression models. It represents the average of the squared differences between predicted values and true values. A smaller MSE indicates better performance.\n",
    "\n",
    "Log Loss: Log loss is commonly used in binary or multi-class probability prediction problems. It measures the difference between predicted probabilities and true labels. A lower log loss indicates better performance.\n",
    "\n",
    "These metrics are used to evaluate the performance of models in the model selection process. However, it's important to note that these metrics only reflect the fit of the model to a particular dataset and may not fully capture its generalization performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82087a9b",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd3833",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#This is a note of confusion matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create actual labels and predicted labels\n",
    "actual_labels = [0, 1, 0, 1, 1, 0, 0, 1]\n",
    "predicted_labels = [0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xticks([0, 1], ['Predicted 0', 'Predicted 1'])\n",
    "plt.yticks([0, 1], ['Actual 0', 'Actual 1'])\n",
    "\n",
    "# Display counts in each cell\n",
    "thresh = cm.max() / 2\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b2136",
   "metadata": {},
   "source": [
    "Above, we output a confusion matrix on actual_labels = [0, 1, 0, 1, 1, 0, 0, 1] and the predicted_labels = [0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "Of course, here we are just demonstrating how to output the confusion matrix to understand its meaning after obtaining these two sets of data. In the subsequent experiment, we will explain how to obtain the desired confusion matrix through code.\n",
    "\n",
    "There are four values in the matrix their meanings are as follows:\n",
    "True Positive (TP): The number of positive instances correctly predicted as positive by the model.\n",
    "False Negative (FN): The number of positive instances incorrectly predicted as negative by the model.\n",
    "False Positive (FP): The number of negative instances incorrectly predicted as positive by the model.\n",
    "True Negative (TN): The number of negative instances correctly predicted as negative by the model.\n",
    "\n",
    "As for the matrix we have above, TP is where we predicted as 1 and actually it is 1. FN is the acount that we predicted as 0 but actually it is 1. FP is predicted as 1 but actually it's 0. TN is we predicted as 0 and it's actually 0.\n",
    "\n",
    "After understanding the meaning of the matrix, we can use the following algorithms to calculate the desired metrics:\n",
    "\n",
    "Accuracy: The ratio of the number of correctly predicted samples to the total number of samples.\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: The proportion of true positive predictions among the predicted positive instances, measuring the prediction accuracy of the model.\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: The proportion of true positive predictions among the actual positive instances, measuring the model's ability to identify positives.\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score: The harmonic mean of precision and recall, considering both the accuracy and the identification ability of the model.\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "When evaluating the bias of a model, we usually consider metrics such as precision, accuracy, and F1 score. A lower F1 score may indicate that the model has issues in balancing accuracy and identification ability, but it cannot be simply equated to lower bias. By considering multiple metrics and the specific requirements of the application scenario, a more comprehensive assessment of the model's performance can be achieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130ca67",
   "metadata": {},
   "source": [
    "## Method \n",
    "\n",
    "Does a lower recall rate indicate better bias?\n",
    "\n",
    "**No**, a lower recall rate does not indicate better bias. In machine learning, recall rate is a metric that measures the model's ability to identify positive instances. A higher recall rate indicates that the model can better identify positive instances, while a lower recall rate means that the model may miss some true positive instances.\n",
    "\n",
    "Then does a lower F1 score indicate better bias?\n",
    "\n",
    "**No**, a lower F1 score does not indicate better bias. The F1 score is the harmonic mean of precision and recall, which considers both the accuracy and the identification ability of the model.Bias refers to the extent to which a model makes incorrect assumptions or oversimplifies the problem, and it is related to the model's prediction accuracy. A lower bias indicates that the model can better fit the training data and is closer to the true underlying relationship.\n",
    "\n",
    "The F1 score aims to consider both the precision and recall of the model. For certain applications, we are concerned with both the model's prediction accuracy (precision) and its ability to identify positive instances (recall). Therefore, a higher F1 score indicates that the model performs well in balancing prediction accuracy and identification ability.\n",
    "\n",
    "All these metrics are primarily used to measure the performance of a model on a specific dataset, while model bias typically refers to the systematic deviation of the model from the trends in the dataset, which may affect the model's ability to generalize.\n",
    "\n",
    "Then is there any way to indirectly indicate the bias of a model?\n",
    "\n",
    "Analyzing the difference between training error and validation error, Holdout Method,Cross-Validation, and Bootstrapping are all viable approaches.\n",
    "\n",
    "So what are these method?\n",
    "\n",
    "### Holdout Method\n",
    "\n",
    "Splitting the dataset into mutually exclusive training and testing sets, using the training set to train the model, and then evaluating the model's performance using the testing set. By comparing the performance on different models using the validation set, we can select the best-performing model. The sampling criteria require stratified sampling, which means dividing the data proportionally based on data types. \n",
    "\n",
    "However, since different partitioning methods yield different data samples, the results of model evaluation also differ. Typically, we choose a large portion of the dataset (70-80%) as the training set and the remaining portion as the testing set.\n",
    "By splitting the dataset, we can observe that the testing set only represents a small portion of the total dataset, which can lead to unstable evaluation results.\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "Splitting the dataset into K mutually exclusive subsets (K-fold cross-validation), using each subset as a validation set in turn and the remaining subsets as training sets to train the model and evaluate its performance. By averaging or aggregating the results from K validations, the best model can be selected.\n",
    "\n",
    "The stability and fidelity of the results in cross-validation evaluation method largely depend on the value of K. Additionally, when the sample size is small but can be clearly separated, leave-one-out method (LOOCV) can be used.\n",
    "\n",
    "Cross-validation provides high precision, but it can be time-consuming when dealing with large datasets.\n",
    "\n",
    "In general, using 10-fold cross-validation is sufficient to indirectly assess the generalization ability of a model.\n",
    "\n",
    "### Bootstrapping\n",
    "\n",
    "Bootstrapping, also known as resampling or sampling with replacement, is a technique where each time a copy of a sample is selected from a dataset containing m samples and added to the resulting dataset. This process is repeated m times, resulting in a dataset with m samples. (Some samples may appear multiple times in the resulting dataset.) This resulting dataset is then used as the training set.\n",
    "\n",
    "Since the sampling is conducted independently, the probability that a specific sample is never selected in m iterations of sampling is [(1-1/m)^m]. As m approaches infinity, i.e., m‚Üí‚àû, the limit of this probability is 1/e, where e is the base of the natural logarithm and approximately equal to 2.71828. Therefore, when m is sufficiently large, the probability that a specific sample is never selected in m iterations of sampling is close to 1/e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e5323",
   "metadata": {},
   "source": [
    "## Interpreting the Learning Curves\n",
    "\n",
    "You might think about the information in the training data as being of two kinds: *signal* and *noise*. The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is *only* true of the training data; the noise is all of the random fluctuation that comes from data in the real-world or all of the incidental, non-informative patterns that can't actually help the model make predictions. The noise is the part might look useful but really isn't.\n",
    "\n",
    "We train a model by choosing weights or parameters that minimize the loss on a training set. You might know, however, that to accurately assess a model's performance, we need to evaluate it on a new set of data, the *validation* data. \n",
    "\n",
    "When we train a model we've been plotting the loss on the training set epoch by epoch. To this we'll add a plot the validation data too. These plots we call the **learning curves**. To train deep learning models effectively, we need to be able to interpret them.\n",
    "\n",
    "Now, the training loss will go down either when the model learns signal or when it learns noise. But the validation loss will go down only when the model learns signal. (Whatever noise the model learned from the training set won't generalize to new data.) So, when a model learns signal both curves go down, but when it learns noise a *gap* is created in the curves. The size of the gap tells you how much noise the model has learned.\n",
    "\n",
    "Ideally, we would create models that learn all of the signal and none of the noise. This will practically never happen. Instead we make a trade. We can get the model to learn more signal at the cost of learning more noise. So long as the trade is in our favor, the validation loss will continue to decrease. After a certain point, however, the trade can turn against us, the cost exceeds the benefit, and the validation loss begins to rise.\n",
    "\n",
    "This trade-off indicates that there can be two problems that occur when training a model: not enough signal or too much noise. Underfitting the training set is when the loss is not as low as it could be because the model hasn't learned enough *signal*. Overfitting the training set is when the loss is not as low as it could be because the model learned too much *noise*. The trick to training deep learning models is finding the best balance between the two.\n",
    "\n",
    "We'll look at a couple ways of getting more signal out of the training data while reducing the amount of noise later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63830061",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#This is a note of a learning curve by using the iris dataset in sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the range of training set sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Generate learning curve data using the learning_curve function\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, train_sizes=train_sizes, cv=5)\n",
    "\n",
    "# Calculate the average accuracy for the training and test sets\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "\n",
    "# Plot the accuracy curves for the training and test sets\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Accuracy\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa3d14",
   "metadata": {},
   "source": [
    "First of all, let's take a look at a plot, this is a simple learning curve using an iris dataset in sklearn.dataset. We can simply notice the two curve we plot fells far apart when we have less examples, and when we enlarge the training examples we can see the two lines are approaching convergence.\n",
    "\n",
    "This is how we can see the fitting process using learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338d800",
   "metadata": {},
   "source": [
    "\n",
    "## Capacity\n",
    "\n",
    "A model's **capacity** refers to the size and complexity of the patterns it is able to learn. For neural networks, this will largely be determined by how many neurons it has and how they are connected together. If it appears that your network is underfitting the data, you should try increasing its capacity.\n",
    "\n",
    "You can increase the capacity of a network either by making it *wider* (more units to existing layers) or by making it *deeper* (adding more layers). Wider networks have an easier time learning more linear relationships, while deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n",
    "\n",
    "You'll explore how the capacity of a network can affect its performance in the exercise.\n",
    "\n",
    "Determining an appropriate model capacity is a crucial task in model selection. Here are some common methods and guidelines to help determine the right model capacity:\n",
    "\n",
    "Rule of thumb: In general, if the dataset is small or the task is relatively simple, choosing a lower-capacity model may be more suitable to avoid overfitting. For larger datasets or complex tasks, a higher-capacity model may be able to better fit the data.\n",
    "\n",
    "Cross-validation: This method has been mentioned earlier in the previous text, and it is an extremely important approach in model selection. Therefore, it is necessary to mention this method multiple times and gain a deeper understanding of it.\n",
    "\n",
    "Learning curves: Learning curves can help determine if the model capacity is appropriate. By plotting the performance of the model on the training set and the validation set as the number of training samples increases, one can observe the model's fitting and generalization abilities. If the model performs poorly on both the training set and the validation set, it may be underfitting due to low capacity. If the model performs well on the training set but poorly on the validation set, it may be overfitting due to high capacity. Adjustments to the model capacity can be made based on the trend of the learning curve.\n",
    "\n",
    "Regularization: Adjusting the model capacity through regularization techniques (which we will also mention in the text later). Increasing the regularization parameter can reduce model capacity and decrease the risk of overfitting. Decreasing the regularization parameter can increase model capacity and improve fitting ability. By evaluating the model performance on the validation set with different regularization parameters, an appropriate regularization parameter value can be chosen.\n",
    "\n",
    "Model comparison experiments: Train and evaluate models with different capacities and compare their performance on the validation set. By comparing the generalization performance of different-capacity models, select the model capacity with the best performance.\n",
    "\n",
    "Considering the above methods and guidelines, selecting an appropriate model capacity requires a balance between theory and practice and decision-making based on the specific problem and available resources. The ultimate goal is to choose a model that performs well on both the training data and new data, achieving good generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037aa612",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization\n",
    "\n",
    "You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the 'simplest' one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simple models are less likely to overfit than complex ones.\n",
    "\n",
    "A 'simple model' in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parmeters altogether, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weeights only to  take small values, which makes the distribution of weight values more 'regular'. This is called 'weight regularization', and it is done by adding to the loss function of the network a cost associated with having large weights. \n",
    "\n",
    "Let's consider a target function with a regularization term, which can be represented as:\n",
    "\n",
    "J(Œ∏) = L(Œ∏) + ŒªR(Œ∏)\n",
    "\n",
    "Here, J(Œ∏) is the target function, Œ∏ represents the model's parameters, L(Œ∏) is the loss function (typically the model's error on the training data), R(Œ∏) is the regularization term, and Œª is the regularization parameter.\n",
    "\n",
    "The loss function L(Œ∏) measures how well the model fits the training data, and our goal is to minimize it. The regularization term R(Œ∏) constrains or penalizes the values of the model's parameters, and it controls the complexity of the model.\n",
    "\n",
    "The regularization parameter Œª determines the weight of the regularization term in the target function. When Œª approaches 0, the impact of the regularization term becomes negligible, and the model's objective is primarily to minimize the loss function. On the other hand, when Œª approaches infinity, the regularization term's impact becomes significant, and the model's objective is to minimize the regularization term as much as possible, leading to parameter values tending towards zero.\n",
    "\n",
    "There are two forms of this cost: L1 regularization (also known as Lasso regression) with the regularization term R(Œ∏) represented as the sum of the absolute values of the parameters Œ∏: R(Œ∏) = ||Œ∏||‚ÇÅ. L1 regularization can induce certain parameters of the model to become zero, thereby achieving feature selection and sparsity.\n",
    "\n",
    "L2 regularization (also known as Ridge regression) with the regularization term R(Œ∏) represented as the square root of the sum of the squares of the parameters Œ∏: R(Œ∏) = ||Œ∏||‚ÇÇ. L2 regularization encourages the parameter values of the model to gradually approach zero but not exactly become zero, hence it does not possess the ability for feature selection.\n",
    "\n",
    "In `tf.keras`, weight regularization is added by passing weight regularizer instances to layers as keyword arguments. Let's add L2 weight regularization now.\n",
    "\n",
    "$$L2\\ Loss = Loss + \\textcolor{red}{\\lambda}\\sum_{i} w_i^2$$\n",
    "\n",
    "$$L1\\ Loss = Loss + \\textcolor{red}{\\lambda}\\sum_{i} \\lvert w \\rvert$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/circlesquare.png\n",
    "---\n",
    "name: circlesquare-ms\n",
    "---\n",
    "L1 and L2 regularization\n",
    ":::\n",
    "\n",
    "Both are very common regularization techniques, but they are suitable for different scenarios. L1 regularization is suitable for situations that require feature selection or demand model interpretability. On the other hand, L2 regularization is more general and applicable in most cases to prevent overfitting and improve model generalization ability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb9c25",
   "metadata": {},
   "source": [
    "\n",
    "## Early Stopping\n",
    "\n",
    "We mentioned that when a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn't decreasing anymore. Interrupting the training this way is called **early stopping**.\n",
    "\n",
    "Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won't continue to learn noise and overfit the data.\n",
    "\n",
    "Training with early stopping also means we're in less danger of stopping the training too early, before the network has finished learning signal. So besides preventing overfitting from training too long, early stopping can also prevent *underfitting* from not training long enough. Just set your training epochs to some large number (more than you'll need), and early stopping will take care of the rest.\n",
    "\n",
    "## Adding Early Stopping\n",
    "\n",
    "In Keras, we include early stopping in our training through a callback. A **callback** is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch. (Keras has [a variety of useful callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) pre-defined, but you can [define your own](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback), too.)\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/traintestoverfitting.png\n",
    "---\n",
    "name: EarlyStopping-ms\n",
    "---\n",
    "Early stopping\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585321db",
   "metadata": {},
   "source": [
    "## The impact of the value of $\\lambda$ \n",
    "\n",
    "We notice that the objective function contains not only the regularization term but also the regularization parameter $\\lambda$.\n",
    "\n",
    "The selection of the regularization parameter is an important part of regularization, and it needs to be fine-tuned during the model training process.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/lagrange-animation.gif\n",
    "---\n",
    "name: impact-of-lambda-ms\n",
    "---\n",
    "The impact of the value of $\\lambda$  \n",
    ":::\n",
    "\n",
    "The value of $\\lambda$ has a significant impact on weight regularization.\n",
    "\n",
    "When $\\lambda$ is small, the effect of weight regularization is relatively minor. The network is more likely to learn complex patterns and structures, which can lead to overfitting. This means that the model may perform well on the training data but have poor generalization on new data.\n",
    "\n",
    "When $\\lambda$ is large, the effect of weight regularization becomes more pronounced. The network is constrained to simpler patterns and structures, reducing the risk of overfitting. This can improve the model's generalization on new data but may result in a slight decrease in performance on the training data.\n",
    "\n",
    "Choosing the appropriate value of $\\lambda$ requires adjustment and optimization based on the specific problem and dataset. Typically, cross-validation or other evaluation methods can be used to select the optimal $\\lambda$ value, finding a balance between model complexity and generalization ability.\n",
    "\n",
    "When using regularization during model training, its effect can be better understood. Let's take the example of linear regression.\n",
    "\n",
    "Suppose we have a dataset containing house area and prices, and we want to use a linear regression model to predict house prices. We can define a linear regression model that includes an intercept term and a coefficient for the house area.\n",
    "\n",
    "Without regularization, the objective of the model is to minimize the mean squared error (MSE) on the training data. This means the model will try to find the best-fitting line in the training data to minimize the differences between the predicted values and the actual values.\n",
    "\n",
    "However, if the training data contains noise or outliers, or if the training set is relatively small, the model may overfit the data, leading to a decrease in prediction performance on new data. In such cases, regularization can help control the complexity of the model and reduce the risk of overfitting.\n",
    "\n",
    "By adding L2 regularization (Ridge regularization) to the linear regression model, we introduce the square of the L2 norm of the parameters as a penalty term in the loss function. This encourages the model to prefer smaller parameter values during training, preventing the parameters from becoming too large.\n",
    "\n",
    "The effect of regularization is achieved by balancing the trade-off between minimizing the training error and minimizing the penalty term. A larger regularization parameter will penalize larger parameter values more strongly, making the model smoother and reducing the differences between parameters. This helps reduce the risk of overfitting and improves the model's generalization ability on new data.\n",
    "\n",
    "In summary, the role of regularization in linear regression models is to control the complexity of the model, reduce the risk of overfitting, and improve the model's generalization ability on new data.\n",
    "\n",
    "In this section, we primarily utilize learning curves to optimize the regularization parameter, also known as the learning curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03100d74",
   "metadata": {},
   "source": [
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is one of the most effective and most commonly used regularization techniques for neural network, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. set to zero) a number of output features of the layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; aafter applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. The 'dropout rate' is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\n",
    "\n",
    "In tf.keras you can introduce a dropout in a network via the Dropout layer, which gets applied to the output of layer right before.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/dropoutgif.gif\n",
    "---\n",
    "name: Dropout-ms\n",
    "---\n",
    "Dropout \n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "### Prediction after dropout\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/kUc8r.jpg\n",
    "---\n",
    "name: Prediction-after-dropout-ms\n",
    "---\n",
    "Prediction after dropout \n",
    ":::\n",
    "\n",
    "\n",
    "\n",
    "During training, p neuron activations (usually, p=0.5, so 50%) are dropped. Doing this at the testing stage is not our goal (the goal is to achieve a better generalization). From the other hand, keeping all activations will lead to an input that is unexpected to the network, more precisely, too high (50% higher) input activations for the following layer \n",
    "\n",
    "Consider the neurons at the output layer. During training, each neuron usually get activations only from two neurons from the hidden layer (while being connected to four), due to dropout. Now, imagine we finished the training and remove dropout. Now activations of the output neurons will be computed based on four values from the hidden layer. This is likely to put the output neurons in unusual regime, so they will produce too large absolute values, being overexcited \n",
    "\n",
    "To avoid this, the trick is to multiply the input connections' weights of the last layer by 1-p (so, by 0.5). Alternatively, one can multiply the outputs of the hidden layer by 1-p, which is basically the same "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eed48",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusions\n",
    "\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/ZahidHasan.png\n",
    "---\n",
    "name: Training-size-matters-ms\n",
    "---\n",
    "Training size matters\n",
    ":::\n",
    "\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/steps.png\n",
    "---\n",
    "name: Steps-ms\n",
    "---\n",
    "How to choose a good model\n",
    ":::\n",
    "\n",
    "The above image illustrates well why we consider bias as an important aspect in model selection and even in machine learning. When our model understands the signal, its improvement is positive. However, once the model starts to understand the noise, the bias of the model starts to increase. This is where cross-validation, mentioned earlier, comes into play.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/model-selection/Bias-vs.webp\n",
    "---\n",
    "name: Conclusion-ms\n",
    "---\n",
    "Conclusion \n",
    ":::\n",
    "\n",
    "The purpose of model selection is to choose the best model among multiple candidate models for a given machine learning problem. The best model refers to the one that performs well on the training data and has good generalization ability to unseen new data.\n",
    "\n",
    "The importance of model selection lies in the fact that different models may have different adaptability to the nature of the data and the complexity of the problem. Selecting an appropriate model can improve the model's prediction accuracy, robustness, and interpretability.\n",
    "\n",
    "## Your turn! üöÄ\n",
    "\n",
    "Machine learning model selection and dealing with overfitting and underfitting are crucial aspects of the machine learning pipeline. In this assignment, you'll have the opportunity to apply your understanding of these concepts and techniques. Please complete the following tasks:\n",
    "[assignment](../assignments/ml-advanced/model-selection/model-selection-assignment-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
