{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650f74c8",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "Usually, we regard Deep Q Network as DQN, and it can also be called Reinforcement Learning (RL) whose aim is to achieve the desired behavior of an agent that learns from its mistakes and improves its performance. \n",
    "\n",
    "RL is a type of machine learning that allows us to create AI agents that learn from the environment by interacting with it to maximize its cumulative reward. Here is an image showing the basic RL operation principle:\n",
    "\n",
    ":::{figure} 02_pipline\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/02_pipline.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Pipline of DQN\n",
    ":::\n",
    "\n",
    "From this image, we can see that at each step $k$, the agent picks an action $u_k$, receives an observation $y_k$ and receives a reward $r_k$, and the environment receives an action $u_k$, emits an observation $y_{k+1}$ and emits a reward $r_{k+1}$. Later, the time increments k ← k + 1. A one step time delay is assumed between executing the action and receiving the observation as well as reward. We assume that the resulting time interval $∆t = t_k − t_{k+1}$ is constant.\n",
    "\n",
    ":::{note}\n",
    "Key characteristics of RL:\n",
    "- No supervisor.\n",
    "- Data-driven.\n",
    "- Discrete time steps.\n",
    "- Sequential data stream (not independent and identically distributed data).\n",
    "- Agent actions affect subsequent data (sequential decision making).\n",
    ":::\n",
    "\n",
    "##  Basic terminology\n",
    "\n",
    "### Reward\n",
    "\n",
    "A reward is a scalar random variable $R_k$ with realizations $r_k$:\n",
    "\n",
    "- Often it is considered a real-number $r_k \\in \\mathbb{R}$ or an integer $r_k \\in \\mathbb{Z}$.\n",
    "- The reward function (interpreter) may be naturally given or is a design degree of freedom (i.e., can be manipulated).\n",
    "- It fully indicates how well an RL agent is doing at step $k$.\n",
    "- The agent’s task is to maximize its reward over time.\n",
    "\n",
    ":::{note}\n",
    "If we want the machine to flip a pancake:\n",
    "- Pos. reward: catching the 180◦ rotated pancake\n",
    "- Neg. reward: droping the pancake on the floor\n",
    ":::\n",
    "\n",
    "Rewards can have many different flavors and are highly dependent on the given problem:\n",
    "\n",
    "- Actions may have short and/or long term consequences.\n",
    "- The reward for a certain action may be delayed.\n",
    "- Examples: Stock trading, strategic board games,...\n",
    "- Rewards can be positive and negative real values.\n",
    "- Certain situations (e.g. car hits wall) might lead to a negative reward.\n",
    "- Exogenous impacts might introduce stochastic reward components.\n",
    "- Example: A wind gust pushes the helicopter into a tree.\n",
    "\n",
    "Besides, the RL agent’s learning process is heavily linked with the reward distribution over time. Designing expedient rewards functions is therefore crucially important for successfully applying RL. And often there is no predefined way on how to design the “best reward function”.\n",
    "\n",
    "### Task-dependent return definitions\n",
    "\n",
    "#### Episodic tasks\n",
    "\n",
    "Episodic tasks can naturally break into subsequences (finite horizon), for examples: most games, maze,... And the return becomes a finite sum: $g_k = r_{k+1} + r_{k+2} + ... + r_{N}$. Episodes end at their terminal step $k = N$.\n",
    "\n",
    "#### Continuing tasks\n",
    "\n",
    "Continuing tasks lack a natural end (infinite horizon), for example: process control task, and the return should be discounted to prevent infinite numbers: $g_k = r_{k+1} + \\gamma r_{k+2} + \\gamma^2 r_{k+3} + ... = \\sum_{i=1}^{\\infty} \\gamma^{i} r_{k+i+1}$. Here, $\\gamma ∈ {\\mathbb{R}|0 ≤ \\gamma ≤ 1}$ is the discount rate.\n",
    "\n",
    ":::{note}\n",
    "From numeric viewpoint:\n",
    "If $\\gamma$ = 1 and $r_k$ > 0 for $k → \\infty $, $g_k$ gets infinite.\n",
    "If $\\gamma$ < 1 and $r_k$ is bounded for $k → \\infty$, $g_k$ is bounded.\n",
    "\n",
    "From strategic viewpoint:\n",
    "If $\\gamma$ ≈ 1: agent is farsighted.\n",
    "If $\\gamma$ ≈ 0: agent is shortsighted (only interested in immediate reward).\n",
    ":::\n",
    "\n",
    "### State\n",
    "\n",
    "#### Environment state\n",
    "\n",
    "Random variable $X_k^{e}$ with realizations $x_k^{e}$:\n",
    "\n",
    "- Internal status representation of the environment, e.g.physical states (car velocity or motor current), game states (current chess board situation). financial states (stock market status).\n",
    "- Fully, limited or not at all visible by the agent:sometimes even ’foggy’ or uncertain, but in general: $Y_k = f(X_k)$ as the measurable outputs of the environment.\n",
    "- Continuous or discrete quantity.\n",
    "\n",
    "#### Agent state\n",
    "\n",
    "Random variable $X_k^{a}$ with realizations $x_k^{a}$:\n",
    "\n",
    "- Internal status representation of the agent.\n",
    "- In general: $x_k^{a} \\neq x_k^{e}$, e.g., due to measurement noise or an additional agent’s memory.\n",
    "- Agent’s condensed information relevant for next action.\n",
    "- Dependent on internal knowledge / policy representation of the agent.\n",
    "- Continuous or discrete quantity.\n",
    "\n",
    "### Action\n",
    "\n",
    "An action is the agent’s degree of freedom in order to maximize its reward. The major distinctions are: \n",
    "- Finite action set (FAS): $u_k ∈ {u_{k,1},u_{k,2}, ...} ∈ \\mathbb{R}_m$.\n",
    "- Continuous action set (CAS): Infinite number of actions: $u_k ∈ \\mathbb{R}_m$.\n",
    "- Deterministic $u_k$ or random Uk variable.\n",
    "- Often state-dependent and potentially constrained: $u_k ∈ U(x_k) ⊆ \\mathbb{R}_m$.\n",
    "\n",
    ":::{note}\n",
    "Evaluating the state and action spaces (e.g., finite vs. continuous) of a new RL problem should be always the first steps in order to choose appropriate solution algorithms.\n",
    ":::\n",
    "\n",
    "### Policy\n",
    "\n",
    "A policy $\\pi$ is the agent’s internal strategy on picking actions.\n",
    "- Deterministic policies: maps state and action directly: $u_k = \\pi (x_k)$. \n",
    "- Stochastic policies: maps a probability of the action given a state: $\\pi(U_k|X_k) = \\mathbb{P} [Uk|Xk]$ .\n",
    "- RL is all about changing $\\pi$ over time in order to maximize the expected return.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Here is a deterministic policy example: find optimal gains ${K_p, K_i, K_d}$ given the reward $r_k = −e^2_k$\n",
    "- Agent’s behavior is explicitly determined by ${K_p, K_i, K_d}$.\n",
    "- Reference value is part of the environment state: $x_k =[y_k y^∗_k]^T$.\n",
    "- Control output is the agent’s action: $u_k = \\pi(x_k|K_p, K_i, K_d)$.\n",
    "\n",
    ":::{figure} 03_policy_example\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/03_policy_example.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Classical PID control loop with scalar quantities\n",
    ":::\n",
    "\n",
    "### Value functions\n",
    "\n",
    "The state-value function is the expected return being in state $x_k$ following a policy $\\pi:v_{\\pi}(x_k)$.\n",
    "\n",
    "Assuming an MDP problem structure the state-value function is $v_{\\pi}(x_k) = \\mathbb{E}_{\\pi} [G_k | X_k = x_k] = \\mathbb{E}_{\\pi}[\\sum_{i=0}^{\\infty} \\gamma^i R_{k+i+1} | x_k]$.\n",
    "\n",
    "The action-value function is the expected return being in state $x_k$ taken an action $u_k$ and, thereafter, following a policy $\\pi: q_{\\pi}(x_k,u_k)$.\n",
    "\n",
    "Assuming an MDP problem structure the action-value function is $q_{\\pi}(x_k, u_k) = \\mathbb{E}_{\\pi} [G_k | X_k=x_k, U_k=u_k] = \\mathbb{E}_{\\pi} [\\sum_{i=0}^{\\infty} \\gamma^i R_{k+i+1} | x_k,u_k]$.\n",
    "\n",
    "A key task in RL is to estimate $v_{\\pi}(x_k)$ and $q_{\\pi}(x_k,u_k)$ based on sampled data.\n",
    "\n",
    "### Model\n",
    "\n",
    "A model predicts what will happen inside an environment.\n",
    "\n",
    "That could be a state model $\\mathcal{P}$: $\\mathcal{P} = \\mathbb{P}[X_{k+1}=x_{k+1}|X_k=x_k, U_k=u_k]$. Or a reward model $\\mathcal{R}$: $\\mathcal{R} = \\mathbb{P}[R_{k+1}=r_{k+1}|X_k=x_k, U_k=u_k]$. In general, those models could be stochastic (as denoted above) but in some problems relax to a deterministic form. Using data in order to fit a model is a learning problem of its own and often called system identification.\n",
    "\n",
    "### Exploration and exploitation\n",
    "\n",
    "In RL the environment is initially unknown. How to act optimal?\n",
    "- Exploration: find out more about the environment.\n",
    "- Exploitation: maximize current reward using limited information. \n",
    "\n",
    ":::{note}\n",
    "Trade-off problem: what’s the best split between both strategies? \n",
    ":::\n",
    "\n",
    "## Main algorithms\n",
    "\n",
    "In this section, we will take maze as an example. The problem statement is:\n",
    "\n",
    "- Reward: $r_k = −1$.\n",
    "- At goal: episode termination.\n",
    "- Actions: $u_k \\in {N, E, S, W}$.\n",
    "- State: agent’s location.\n",
    "- Deterministic problem (no stochastic influences).\n",
    "\n",
    ":::{figure} 04_maze\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/04_maze.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Maze setup statement\n",
    ":::\n",
    "\n",
    "### RL-solution by policy\n",
    "\n",
    ":::{figure-md} 05_maze_policy\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/05_maze_policy.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Maze solved by policy\n",
    ":::\n",
    "\n",
    "Key characteristics:\n",
    "- For any state there is a direct action command.\n",
    "- The policy is explicitly available.\n",
    "\n",
    "### RL-solution by value function\n",
    "\n",
    ":::{figure-md} 06_maze_valuefunc\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/06_maze_valuefunc.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Maze solved by value function\n",
    ":::\n",
    "\n",
    "Key characteristics:\n",
    "- The agent evaluates neighboring maze positions by their value.\n",
    "- The policy is only implicitly available.\n",
    "\n",
    "### RL-solution by model evaluation\n",
    "\n",
    ":::{figure-md} 07_maze_modeleval\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/07_maze_modeleval.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Maze solved by model evaluation\n",
    ":::\n",
    "\n",
    "Key characteristics:\n",
    "- Agent uses internal model of the environment.\n",
    "- The model is only an estimate (inaccurate, incomplete).\n",
    "- The agent interacts with the model before.\n",
    "\n",
    "### RL agent taxonomy\n",
    "\n",
    ":::{figure-md} 07_taxonomy\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/DQN/07_taxonomy.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Main categories of reinforcement learning algorithms\n",
    ":::\n",
    "\n",
    "## Code\n",
    "\n",
    "This is an example of DQN discrete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f59b21",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random as r\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455539f",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, maze_type):\n",
    "        # Use the maze types from maze_collection.py\n",
    "        [self.maze,\n",
    "         [self.startX, self.startY],\n",
    "         [self.goalX, self.goalY]] = maze_type\n",
    "        self.x = self.startX\n",
    "        self.y = self.startY\n",
    "\n",
    "        self.won = False\n",
    "\n",
    "        self.win = None\n",
    "        self.squares = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = self.startX\n",
    "        self.y = self.startY\n",
    "\n",
    "        self.won = False\n",
    "        if self.win is not None:\n",
    "            self.display()\n",
    "\n",
    "    def display_commandline(self):\n",
    "        to_print = \"\"\n",
    "        for y_pos, y in enumerate(self.maze):\n",
    "            for x_pos, x in enumerate(y):\n",
    "                if self.x == x_pos and self.y == y_pos:\n",
    "                    to_print += \" O \"\n",
    "                elif self.goalX == x_pos and self.goalY == y_pos:\n",
    "                    to_print += \" X \"\n",
    "                elif x == 0:\n",
    "                    to_print += \"   \"\n",
    "                elif x == 1:\n",
    "                    to_print += \" # \"\n",
    "            to_print += \"\\n\"\n",
    "        print(to_print)\n",
    "\n",
    "    # def initialise_graphics(self):\n",
    "    #     self.win = g.GraphWin(\"Maze\", 200 + (50 * len(self.maze[0])), 200 + (50 * len(self.maze)))\n",
    "    #     self.squares = []\n",
    "    #     for i in range(len(self.maze)):\n",
    "    #         self.squares.append([])\n",
    "    #         for j in range(len(self.maze[i])):\n",
    "    #             self.squares[i].append(\n",
    "    #                 g.Rectangle(g.Point(100 + (j * 50), 100 + (i * 50)), g.Point(150 + (j * 50), 150 + (i * 50))))\n",
    "    #             self.squares[i][j].draw(self.win)\n",
    "    #     self.display()\n",
    "\n",
    "    def display(self):\n",
    "        for y_pos, y in enumerate(self.maze):\n",
    "            for x_pos, x in enumerate(y):\n",
    "                if self.x == x_pos and self.y == y_pos:\n",
    "                    self.squares[y_pos][x_pos].setFill(\"orangered\")\n",
    "                elif self.goalX == x_pos and self.goalY == y_pos:\n",
    "                    self.squares[y_pos][x_pos].setFill(\"green\")\n",
    "                elif x == 0:\n",
    "                    self.squares[y_pos][x_pos].setFill(\"white\")\n",
    "                elif x == 1:\n",
    "                    self.squares[y_pos][x_pos].setFill(\"black\")\n",
    "\n",
    "    def check_win_condition(self):\n",
    "        if self.x == self.goalX and self.y == self.goalY:\n",
    "            self.won = True\n",
    "\n",
    "    def move_up(self):\n",
    "        if self.y - 1 >= 0 and self.maze[self.y - 1][self.x] != 1:\n",
    "            self.y -= 1\n",
    "        self.check_win_condition()\n",
    "\n",
    "    def move_down(self):\n",
    "        if self.y + 1 < len(self.maze) and self.maze[self.y + 1][self.x] != 1:\n",
    "            self.y += 1\n",
    "        self.check_win_condition()\n",
    "\n",
    "    def move_left(self):\n",
    "        if self.x - 1 >= 0 and self.maze[self.y][self.x - 1] != 1:\n",
    "            self.x -= 1\n",
    "        self.check_win_condition()\n",
    "\n",
    "    def move_right(self):\n",
    "        if self.x + 1 < len(self.maze[0]) and self.maze[self.y][self.x + 1] != 1:\n",
    "            self.x += 1\n",
    "        self.check_win_condition()\n",
    "\n",
    "    def distance_up(self):\n",
    "        for i in range(self.y, -1, -1):\n",
    "            if i - 1 < 0 or self.maze[i - 1][self.x] == 1:\n",
    "                return self.y - i\n",
    "\n",
    "    def distance_down(self):\n",
    "        for i in range(self.y, len(self.maze), 1):\n",
    "            if i + 1 >= len(self.maze) or self.maze[i + 1][self.x] == 1:\n",
    "                return i - self.y\n",
    "\n",
    "    def distance_left(self):\n",
    "        for i in range(self.x, -1, -1):\n",
    "            if i - 1 < 0 or self.maze[self.y][i - 1] == 1:\n",
    "                return self.x - i\n",
    "\n",
    "    def distance_right(self):\n",
    "        for i in range(self.x, len(self.maze[0]), 1):\n",
    "            if i + 1 >= len(self.maze) or self.maze[self.y][i + 1] == 1:\n",
    "                return i - self.x\n",
    "\n",
    "    def normal_x(self):\n",
    "        return self.x / (len(self.maze[0]) - 1)\n",
    "\n",
    "    def normal_y(self):\n",
    "        return self.y / (len(self.maze) - 1)\n",
    "\n",
    "    def normal_goal_x(self):\n",
    "        return self.goalX / (len(self.maze[0]) - 1)\n",
    "\n",
    "    def normal_goal_y(self):\n",
    "        return self.goalY / (len(self.maze) - 1)\n",
    "\n",
    "T_maze = [\n",
    "    [  # Layout\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 1],\n",
    "        [1, 0, 1]\n",
    "    ],\n",
    "    [1, 2],  # Start coordinates\n",
    "    [0, 0]   # Goal coordinates\n",
    "]\n",
    "\n",
    "def main(output_file_name):\n",
    "    # housekeeping\n",
    "    m = Maze(T_maze)\n",
    "    learning_rate = 0.0001\n",
    "    num_memory_units = 4\n",
    "    graphical = True\n",
    "    file_output = True\n",
    "\n",
    "    if file_output is True:\n",
    "        # output to file (this is set to overwrite!)\n",
    "        file = open(output_file_name + \".txt\", \"w\")\n",
    "        file.write(\"Iter\\tWon?\\tSteps\\tAll steps\\n\")\n",
    "\n",
    "    # weight update operation\n",
    "    # ph_delta_weights_list = [tf.placeholder(tf.float32, w.get_shape()) for w in weights_list]\n",
    "    # ph_delta_weights_list = [w.get_shape() for w in weights_list]\n",
    "    # update_weights = [tf.compat.v1.assign(weights_list[i], weights_list[i] + ph_delta_weights_list[i])\n",
    "    #                   for i in range(len(weights_list))]\n",
    "\n",
    "    # training setup\n",
    "    maxSteps = 20\n",
    "    iteration = 0\n",
    "    maxIterations = 10000\n",
    "\n",
    "    steps_taken = np.zeros(maxIterations)\n",
    "\n",
    "    # Plot display -----------------------------------------------------------------------------------------------------\n",
    "    if graphical is True:\n",
    "        spread = 50\n",
    "\n",
    "        plt.ion()\n",
    "        fig = plt.figure(\"Maze solver\")\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.axis([0, maxIterations/spread + 1, 0, maxSteps + 1])\n",
    "        plt.ylabel(\"Steps taken\")\n",
    "        plt.xlabel(\"Iterations ({})\".format(spread))\n",
    "        ax.plot([0], [0])\n",
    "        ax.grid()\n",
    "\n",
    "        iterations = []\n",
    "        duration_history = []\n",
    "\n",
    "    # Looping through iterations\n",
    "    while iteration < maxIterations:\n",
    "        # Current step\n",
    "        step = 0\n",
    "\n",
    "        # # All outputs and dp_dthetas for this iteration\n",
    "        # probabilities = np.zeros(maxSteps)\n",
    "        # dp_dthetas = list()\n",
    "        #\n",
    "        # memory = np.zeros(num_memory_units)\n",
    "        #\n",
    "        # movements = \"\"\n",
    "\n",
    "        while m.won is False and step < maxSteps:\n",
    "            # All outputs and dp_dthetas for this iteration\n",
    "            probabilities = np.zeros(maxSteps)\n",
    "            dp_dthetas = list()\n",
    "\n",
    "            memory = np.zeros(num_memory_units)\n",
    "\n",
    "            movements = \"\"\n",
    "\n",
    "            # Defining neural network input\n",
    "            input_values = np.array([m.normal_x(), m.normal_y()])\n",
    "            input_values = np.append(input_values, memory)\n",
    "\n",
    "            # Running input through the neural network\n",
    "            # [output, dp0dtheta, dp1dtheta, dp2dtheta, dp3dtheta, output_memory] =\\\n",
    "            #     sess.run([y, dprobability0_dweights, dprobability1_dweights, dprobability2_dweights,\n",
    "            #               dprobability3_dweights, memory_units],\n",
    "            #              feed_dict={x: [input_values]})\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                x = tf.cast(input_values, tf.float32)\n",
    "\n",
    "                W1 = tf.Variable(tf.random.truncated_normal([2+num_memory_units, 6]))\n",
    "                b1 = tf.Variable(tf.random.truncated_normal([1, 6]))\n",
    "                W2 = tf.Variable(tf.random.truncated_normal([6, 6]))\n",
    "                b2 = tf.Variable(tf.random.truncated_normal([1, 6]))\n",
    "                W3 = tf.Variable(tf.random.truncated_normal([6, 4+num_memory_units]))\n",
    "                b3 = tf.Variable(tf.random.truncated_normal([1, 4+num_memory_units]))\n",
    "\n",
    "                h1 = tf.sigmoid(tf.matmul([x], W1) + b1)\n",
    "                h2 = tf.sigmoid(tf.matmul(h1, W2) + b2)\n",
    "\n",
    "                output_final_layer_before_activation_function = tf.matmul(h2, W3) + b3\n",
    "                left_output = output_final_layer_before_activation_function[:, 0:4]\n",
    "                right_output = output_final_layer_before_activation_function[:, 4:]\n",
    "                y = tf.nn.softmax(left_output)\n",
    "                memory_units = tf.sigmoid(right_output)\n",
    "\n",
    "                weights_list = [W1, b1, W2, b2, W3, b3]\n",
    "            dprobability0_dweights = tape.gradient(y[:, 0], weights_list)\n",
    "            # print(dprobability0_dweights)\n",
    "            # print(\"gradient\")\n",
    "            dprobability1_dweights = tape.gradient(y[:, 1], weights_list)\n",
    "            dprobability2_dweights = tape.gradient(y[:, 2], weights_list)\n",
    "            dprobability3_dweights = tape.gradient(y[:, 3], weights_list)\n",
    "\n",
    "            ph_delta_weights_list = [w.get_shape() for w in weights_list]\n",
    "            # update_weights = [tf.compat.v1.assign(weights_list[i], weights_list[i] + ph_delta_weights_list[i])\n",
    "            #           for i in range(len(weights_list))]\n",
    "\n",
    "            [output, dp0dtheta, dp1dtheta, dp2dtheta, dp3dtheta, output_memory] =\\\n",
    "                [y, dprobability0_dweights, dprobability1_dweights, dprobability2_dweights,\n",
    "                          dprobability3_dweights, memory_units]\n",
    "\n",
    "\n",
    "            # Random value between 0 and 1, inclusive on both sides\n",
    "            result = r.uniform(0, 1)\n",
    "\n",
    "            if result <= output[0][0]:\n",
    "                # Up\n",
    "                m.move_up()\n",
    "                probabilities[step] = output[0][0]\n",
    "                dp_dthetas.append(dp0dtheta)\n",
    "                print(dp0dtheta)\n",
    "                print(\"=========================================\")\n",
    "                movements += \"U\"\n",
    "            elif result <= output[0][0] + output[0][1]:\n",
    "                # Right\n",
    "                m.move_right()\n",
    "                probabilities[step] = output[0][1]\n",
    "                dp_dthetas.append(dp1dtheta)\n",
    "                movements += \"R\"\n",
    "            elif result <= output[0][0] + output[0][1] + output[0][2]:\n",
    "                # Down\n",
    "                m.move_down()\n",
    "                probabilities[step] = output[0][2]\n",
    "                dp_dthetas.append(dp2dtheta)\n",
    "                movements += \"D\"\n",
    "            elif result <= output[0][0] + output[0][1] + output[0][2] + output[0][3]:\n",
    "                # Left\n",
    "                m.move_left()\n",
    "                probabilities[step] = output[0][3]\n",
    "                dp_dthetas.append(dp3dtheta)\n",
    "                movements += \"L\"\n",
    "\n",
    "            memory = output_memory[0]\n",
    "            step += 1\n",
    "\n",
    "        print(\"Iteration #{:05d}\\tWon: {}\\tSteps taken: {:04d}\\tSteps: {}\".format(iteration, m.won,\n",
    "                                                                                  step, movements))\n",
    "        if file_output is True:\n",
    "            file.write(\"{:05d}\\t{}\\t{:04d}\\t{}\\n\".format(iteration, m.won, step, movements))\n",
    "\n",
    "        # Assigning a reward\n",
    "        reward = maxSteps - (2 * step)  # linear reward function\n",
    "        #reward = maxSteps - pow(step, 2)  # power reward function\n",
    "\n",
    "        # Applying weight change for every step taken, based on the reward given at the end\n",
    "        for i in range(step):\n",
    "            # print(dp_dthetas[0][0])\n",
    "            # print('===================================')\n",
    "            # print((1 / probabilities[i]) * reward)\n",
    "            deltaTheta = [(learning_rate * (1 / probabilities[i]) * reward) * dp_dthetas[i][j]\n",
    "                          for j in range(len(weights_list))]\n",
    "\n",
    "            # sess.run(update_weights, feed_dict=dict(zip(ph_delta_weights_list, deltaTheta)))\n",
    "            update_weights = [tf.compat.v1.assign(weights_list[i], weights_list[i] + ph_delta_weights_list[i])\n",
    "                      for i in range(len(dict(zip(ph_delta_weights_list, deltaTheta))))]\n",
    "\n",
    "        steps_taken[iteration] = step\n",
    "        if graphical is True and iteration % spread == 0:\n",
    "            steps_mean = np.mean(steps_taken[iteration-spread:iteration+1])\n",
    "            iterations = iterations+[iteration/spread]\n",
    "            duration_history = duration_history+[steps_mean]\n",
    "            del ax.lines[0]\n",
    "            ax.plot(iterations, duration_history, 'b-', label='Traj1')\n",
    "            plt.draw()\n",
    "            plt.pause(0.001)\n",
    "\n",
    "        m.reset()\n",
    "\n",
    "        iteration += 1\n",
    "    if file_output is True:\n",
    "        file.close()\n",
    "    if graphical is True:\n",
    "        if file_output is True:\n",
    "            plt.savefig(output_file_name + \".png\")\n",
    "        else:\n",
    "            plt.show()\n",
    "        #input(\"Press [enter] to continue.\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb2770",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    for run in range(11, 25):\n",
    "        number = \"{:05d}\".format(run)\n",
    "        os.mkdir(\"T_fixed-memory-linear_reward_\" + number)\n",
    "        main(\"T_fixed-memory-linear_reward_\" + number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa1a40",
   "metadata": {},
   "source": [
    "## Your turn! 🚀\n",
    "\n",
    " TBD.\n",
    "\n",
    "## Self study\n",
    "\n",
    "You can refer to this website for further study:\n",
    "\n",
    "- [Introduction to Reinforcement Learning](https://pylessons.com/CartPole-reinforcement-learning)\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Paderborn University - LEA](https://github.com/upb-lea) for creating the open-source course [reinforcement_learning_course_materials](https://github.com/upb-lea/reinforcement_learning_course_materials) and [Gergely Pacsuta](https://github.com/pacsuta) for creating the open-source project [tf-nn-maze-solver](https://github.com/pacsuta/tf-nn-maze-solver). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    ":::{bibliography}\n",
    ":filter: docname in docnames\n",
    ":::"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
