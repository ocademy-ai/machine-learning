{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys \n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython imageio scikit-image requests\n",
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "license:\n",
    "    code: MIT\n",
    "    content: CC-BY-4.0\n",
    "github: https://github.com/ocademy-ai/machine-learning\n",
    "venue: By Ocademy\n",
    "open_access: true\n",
    "bibliography:\n",
    "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are responsible for the latest major breakthroughs in image recognition in the past few years.\n",
    "\n",
    "In mathematics, a convolution is a function that is applied over the output of another function. In our case, we will consider applying a matrix multiplication (filter) across an image. See the below diagram for an example of how this may work.\n",
    "\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/01_intro_cnn.png\" width=\"90%\" class=\"bg-white mb-1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "display(HTML(\"\"\"\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/html/conv-demo/index.html\" width=\"105%\" height=\"800px;\"\n",
    "style=\"border:none;\" scrolling=\"auto\"></iframe>\n",
    "A demo of convolution function. <a\n",
    "href=\"https://cs231n.github.io/convolutional-networks/\"> [source]</a>\n",
    "</p>\n",
    "\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs generally follow a structure. The main convolutional setup is (input array) -> (convolutional filter layer) -> (Pooling) -> (Activation layer). The above diagram depicts how a convolutional layer may create one feature. Generally, filters are multidimensional and end up creating many features. It is also common to have a completely separate filter-feature creator of different sizes acting on the same layer. After this convolutional filter, it is common to apply a pooling layer. This pooling may be a max-pooling or an average pooling or another aggregation. One of the key concepts here is that the pooling layer has no parameters while decreasing the layer size. See the below diagram for an example of max-pooling.\n",
    "\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/01_intro_cnn2.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "After the max pooling, there is generally an activation layer. One of the more common activation layers is the ReLU (Rectified Linear Unit) {cite}`reluwiki`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST handwritten digits\n",
    "\n",
    "Here we illustrate how to use a simple CNN with three convolutional units to predict the MNIST handwritten digits. \n",
    "\n",
    "```{note}\n",
    "There is good reason why this dataset is used like the 'hello world' of image recognition, it is fairly compact while having a decent amount of training, test, and validation data. It only has one channel (black and white) and only ten possible outputs (0-9).\n",
    "```\n",
    "\n",
    "When the script is done training the model, you should see similar output to the following graphs.\n",
    "\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/02_cnn1_loss_acc.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Training and test loss (left) and test batch accuracy (right).\n",
    "\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/02_cnn1_mnist_output.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "A random set of 6 digits with actual and predicted labels. You can see a prediction failure in the lower right box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "display(HTML(\"\"\"\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/html/cnn-vis/cnn.html\" width=\"105%\" height=\"800px;\"\n",
    "style=\"border:none;\" scrolling=\"auto\"></iframe>\n",
    "A demo of CNN. <a\n",
    "href=\"https://adamharley.com/nn_vis/cnn/3d.html\"> [source]</a>\n",
    "</p>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "display(HTML(\"\"\"\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/html/cnn-vis-2/index.html\" width=\"105%\" height=\"600px;\"\n",
    "style=\"border:none;\" scrolling=\"auto\"></iframe>\n",
    "A demo of CNN. <a\n",
    "href=\"https://poloclub.github.io/cnn-explainer/\"> [source]</a>\n",
    "</p>\n",
    "\"\"\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Data preprocessing\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiler model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Test model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Visualizing the training process\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Convert images to 4D tensors (batch_size, height, width, channels)\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "# Set model parameters\n",
    "batch_size = 100\n",
    "learning_rate = 0.005\n",
    "evaluation_size = 500\n",
    "image_width = train_images.shape[1]\n",
    "image_height = train_images.shape[2]\n",
    "target_size = np.max(train_labels) + 1\n",
    "num_channels = 1  # greyscale = 1 channel\n",
    "generations = 500\n",
    "eval_every = 5\n",
    "conv1_features = 25\n",
    "conv2_features = 50\n",
    "max_pool_size1 = 2  # NxN window for 1st max pool layer\n",
    "max_pool_size2 = 2  # NxN window for 2nd max pool layer\n",
    "fully_connected_size1 = 100\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(conv1_features, (4, 4), activation='relu', input_shape=(image_width, image_height, num_channels)),\n",
    "    tf.keras.layers.MaxPooling2D((max_pool_size1, max_pool_size1)),\n",
    "    tf.keras.layers.Conv2D(conv2_features, (4, 4), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((max_pool_size2, max_pool_size2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(fully_connected_size1, activation='relu'),\n",
    "    tf.keras.layers.Dense(target_size)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "for i in range(generations):\n",
    "    rand_index = np.random.choice(len(train_images), size=batch_size)\n",
    "    rand_x = train_images[rand_index]\n",
    "    rand_y = train_labels[rand_index]\n",
    "    \n",
    "    history = model.train_on_batch(rand_x, rand_y)\n",
    "    temp_train_loss, temp_train_acc = history[0], history[1]\n",
    "    \n",
    "    if (i+1) % eval_every == 0:\n",
    "        eval_index = np.random.choice(len(test_images), size=evaluation_size)\n",
    "        eval_x = test_images[eval_index]\n",
    "        eval_y = test_labels[eval_index]\n",
    "        \n",
    "        test_loss, temp_test_acc = model.evaluate(eval_x, eval_y, verbose=0)\n",
    "        \n",
    "        # Record and print results\n",
    "        train_loss.append(temp_train_loss)\n",
    "        train_acc.append(temp_train_acc)\n",
    "        test_acc.append(temp_test_acc)\n",
    "        acc_and_loss = [(i+1), temp_train_loss, temp_train_acc * 100, temp_test_acc * 100]\n",
    "        acc_and_loss = [np.round(x, 2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss: {:.2f}. Train Acc (Test Acc): {:.2f}% ({:.2f}%)'.format(*acc_and_loss))\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(range(0, generations, eval_every), train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot train and test accuracy\n",
    "plt.plot(range(0, generations, eval_every), train_acc, 'k-', label='Train Set Accuracy')\n",
    "plt.plot(range(0, generations, eval_every), test_acc, 'r--', label='Test Set Accuracy')\n",
    "plt.title('Train and Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot some samples\n",
    "# Plot the 6 of the last batch results:\n",
    "predictions = model.predict(train_images[:6])\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "images = np.squeeze(train_images[:6])\n",
    "\n",
    "Nrows = 2\n",
    "Ncols = 3\n",
    "for i in range(6):\n",
    "    plt.subplot(Nrows, Ncols, i+1)\n",
    "    plt.imshow(np.reshape(images[i], [28, 28]), cmap='Greys_r')\n",
    "    plt.title('Pred: ' + str(predictions[i]), fontsize=10)\n",
    "    frame = plt.gca()\n",
    "    frame.axes.get_xaxis().set_visible(False)\n",
    "    frame.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10\n",
    "\n",
    "Here we will build a convolutional neural network to predict the `CIFAR-10` data.\n",
    "\n",
    "The script provided will download and unzip the `CIFAR-10` data. Then it will start training a CNN from scratch. You should see similar output at the end of the following two graphs.\n",
    "\n",
    "<img src=\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/03_cnn2_loss_acc.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Here we see the training loss (left) and the test batch accuracy (right)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "# Set model parameters\n",
    "batch_size = 128\n",
    "data_dir = 'temp'\n",
    "output_every = 50\n",
    "generations = 200\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "extract_folder = 'cifar-10-batches-bin'\n",
    "\n",
    "# Load data\n",
    "data_dir = 'temp'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "# Check if file exists, otherwise download it\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if os.path.isfile(data_file):\n",
    "    pass\n",
    "else:\n",
    "    # Download file\n",
    "    def progress(block_num, block_size, total_size):\n",
    "        progress_info = [cifar10_url, float(block_num * block_size) / float(total_size) * 100.0]\n",
    "        print('\\r Downloading {} - {:.2f}%'.format(*progress_info), end=\"\")\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file, progress)\n",
    "    # Extract file\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Crop images\n",
    "train_images = tf.image.crop_to_bounding_box(train_images, 4, 4, 24, 24)\n",
    "test_images = tf.image.crop_to_bounding_box(test_images, 4, 4, 24, 24)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers\n",
    "train_labels = train_labels.flatten()\n",
    "test_labels = test_labels.flatten()\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), activation='relu', input_shape=(crop_height, crop_width, num_channels)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(384, activation='relu'),\n",
    "    tf.keras.layers.Dense(192, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_targets)\n",
    "])\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Create accuracy metric\n",
    "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[accuracy_metric])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, batch_size=batch_size, epochs=generations, \n",
    "                    validation_data=(test_images, test_labels), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# Print loss and accuracy\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "# Plot loss over time\n",
    "plt.plot(history.history['loss'], 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy over time\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], 'k-')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to fine-tune current CNN architectures?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the script provided in this section is to download the CIFAR-10 data and sort it out in the proper folder structure for running it through the TensorFlow fine-tuning tutorial. The script should create the following folder structure.\n",
    "\n",
    "-train_dir\n",
    "  |--airplane\n",
    "  |--automobile\n",
    "  |--bird\n",
    "  |--cat\n",
    "  |--deer\n",
    "  |--dog\n",
    "  |--frog\n",
    "  |--horse\n",
    "  |--ship\n",
    "  |--truck\n",
    "-validation_dir\n",
    "  |--airplane\n",
    "  |--automobile\n",
    "  |--bird\n",
    "  |--cat\n",
    "  |--deer\n",
    "  |--dog\n",
    "  |--frog\n",
    "  |--horse\n",
    "  |--ship\n",
    "  |--truck\n",
    "\n",
    "  ### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this script, we download the CIFAR-10 images and\n",
    "# transform/save them in the Inception Retraining Format\n",
    "#\n",
    "# The end purpose of the files is for re-training the\n",
    "# Google Inception tensorflow model to work on the CIFAR-10.\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import pickle as cPickle\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import imageio\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "cifar_link = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "data_dir = 'temp'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Download tar file\n",
    "target_file = os.path.join(data_dir, 'cifar-10-python.tar.gz')\n",
    "if not os.path.isfile(target_file):\n",
    "    print('CIFAR-10 file not found. Downloading CIFAR data (Size = 163MB)')\n",
    "    print('This may take a few minutes, please wait.')\n",
    "    filename, headers = urllib.request.urlretrieve(cifar_link, target_file)\n",
    "\n",
    "# Extract into memory\n",
    "tar = tarfile.open(target_file)\n",
    "tar.extractall(path=data_dir)\n",
    "tar.close()\n",
    "objects = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create train image folders\n",
    "train_folder = 'train_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, train_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, train_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "# Create test image folders\n",
    "test_folder = 'validation_dir'\n",
    "if not os.path.isdir(os.path.join(data_dir, test_folder)):\n",
    "    for i in range(10):\n",
    "        folder = os.path.join(data_dir, test_folder, objects[i])\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Extract images accordingly\n",
    "data_location = os.path.join(data_dir, 'cifar-10-batches-py')\n",
    "train_names = ['data_batch_' + str(x) for x in range(1,6)]\n",
    "test_names = ['test_batch']\n",
    "\n",
    "\n",
    "def load_batch_from_file(file):\n",
    "    file_conn = open(file, 'rb')\n",
    "    image_dictionary = cPickle.load(file_conn, encoding='latin1')\n",
    "    file_conn.close()\n",
    "    return image_dictionary\n",
    "\n",
    "\n",
    "def save_images_from_dict(image_dict, folder='data_dir'):\n",
    "    # image_dict.keys() = 'labels', 'filenames', 'data', 'batch_label'\n",
    "    for ix, label in enumerate(image_dict['labels']):\n",
    "        folder_path = os.path.join(data_dir, folder, objects[label])\n",
    "        filename = image_dict['filenames'][ix]\n",
    "        # Transform image data\n",
    "        image_array = image_dict['data'][ix]\n",
    "        image_array.resize([3, 32, 32])\n",
    "        # Save image using imageio\n",
    "        output_location = os.path.join(folder_path, filename)\n",
    "        # Ensure the pixel values are in the range [0, 255]\n",
    "        image_array = np.clip(image_array, 0, 255).astype(np.uint8)\n",
    "        imageio.imwrite(output_location, image_array.transpose(1, 2, 0))\n",
    "\n",
    "# Sort train images\n",
    "for file in train_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=train_folder)\n",
    "\n",
    "# Sort test images\n",
    "for file in test_names:\n",
    "    print('Saving images from file: {}'.format(file))\n",
    "    file_location = os.path.join(data_dir, 'cifar-10-batches-py', file)\n",
    "    image_dict = load_batch_from_file(file_location)\n",
    "    save_images_from_dict(image_dict, folder=test_folder)\n",
    "    \n",
    "# Create labels file\n",
    "cifar_labels_file = os.path.join(data_dir,'cifar10_labels.txt')\n",
    "print('Writing labels file, {}'.format(cifar_labels_file))\n",
    "with open(cifar_labels_file, 'w') as labels_file:\n",
    "    for item in objects:\n",
    "        labels_file.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylenet / Neural-Style\n",
    "\n",
    "The purpose of this script is to illustrate how to do stylenet in TensorFlow. We reference the following [paper](https://arxiv.org/abs/1508.06576) for this algorithm.\n",
    "\n",
    "But there is some prerequisites,\n",
    "\n",
    "- Download the `VGG-verydeep-19.mat` file.\n",
    "- You must download two images, a style image and a content image for the algorithm to blend.\n",
    "\n",
    "The style image is\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/starry_night.jpg\n",
    "---\n",
    "name: Style image starry night\n",
    "---\n",
    "starry night\n",
    ":::\n",
    "\n",
    "The context image is below.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/book_cover.jpg\n",
    "---\n",
    "name: Content image book cover\n",
    "---\n",
    "book cover\n",
    ":::\n",
    "\n",
    "The final result looks like\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/05_stylenet_ex.png\n",
    "---\n",
    "name: stylenet final result\n",
    "---\n",
    "stylenet\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use two images, an original image and a style image\n",
    "# and try to make the original image in the style of the style image.\n",
    "#\n",
    "# Reference paper:\n",
    "# https://arxiv.org/abs/1508.06576\n",
    "#\n",
    "# Need to download the model 'imagenet-vgg-verydee-19.mat' from:\n",
    "#   http://www.vlfeat.org/matconvnet/models/beta16/imagenet-vgg-verydeep-19.mat\n",
    "\n",
    "import os\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import imageio\n",
    "from skimage.transform import resize\n",
    "from operator import mul\n",
    "from functools import reduce\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# URLs\n",
    "original_image_url = 'https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/book_cover.jpg'\n",
    "style_image_url = 'https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/starry_night.jpg'\n",
    "vgg_url = 'https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/cnn/imagenet-vgg-verydeep-19.mat'\n",
    "\n",
    "# Local directories\n",
    "data_dir = 'temp'\n",
    "vgg_dir = os.path.join(data_dir, 'VGG')\n",
    "if not os.path.exists(vgg_dir):\n",
    "    os.makedirs(vgg_dir)\n",
    "\n",
    "# Function to download and save a file\n",
    "def download_file(url, directory):\n",
    "    response = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    return filepath\n",
    "\n",
    "# Download images and VGG Network\n",
    "original_image_path = download_file(original_image_url, data_dir)\n",
    "style_image_path = download_file(style_image_url, data_dir)\n",
    "vgg_path = download_file(vgg_url, vgg_dir)\n",
    "\n",
    "# Load images using PIL and convert to NumPy arrays\n",
    "original_image = Image.open(original_image_path)\n",
    "style_image = Image.open(style_image_path)\n",
    "original_image = np.array(original_image)\n",
    "style_image = np.array(style_image)\n",
    "\n",
    "# Default Arguments\n",
    "original_image_weight = 5.0\n",
    "style_image_weight = 500.0\n",
    "regularization_weight = 100\n",
    "learning_rate = 10\n",
    "generations = 100\n",
    "output_generations = 25\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# Get shape of target and make the style image the same\n",
    "target_shape = original_image.shape\n",
    "style_image = resize(style_image, target_shape)\n",
    "\n",
    "# VGG-19 Layer Setup\n",
    "# From paper\n",
    "vgg_layers = ['conv1_1', 'relu1_1',\n",
    "              'conv1_2', 'relu1_2', 'pool1',\n",
    "              'conv2_1', 'relu2_1',\n",
    "              'conv2_2', 'relu2_2', 'pool2',\n",
    "              'conv3_1', 'relu3_1',\n",
    "              'conv3_2', 'relu3_2',\n",
    "              'conv3_3', 'relu3_3',\n",
    "              'conv3_4', 'relu3_4', 'pool3',\n",
    "              'conv4_1', 'relu4_1',\n",
    "              'conv4_2', 'relu4_2',\n",
    "              'conv4_3', 'relu4_3',\n",
    "              'conv4_4', 'relu4_4', 'pool4',\n",
    "              'conv5_1', 'relu5_1',\n",
    "              'conv5_2', 'relu5_2',\n",
    "              'conv5_3', 'relu5_3',\n",
    "              'conv5_4', 'relu5_4']\n",
    "\n",
    "\n",
    "# Extract weights and matrix means\n",
    "def extract_net_info(path_to_params):\n",
    "    vgg_data = scipy.io.loadmat(path_to_params)\n",
    "    normalization_matrix = vgg_data['normalization'][0][0][0]\n",
    "    mat_mean = np.mean(normalization_matrix, axis=(0, 1))\n",
    "    network_weights = vgg_data['layers'][0]\n",
    "    return mat_mean, network_weights\n",
    "    \n",
    "\n",
    "# Create the VGG-19 Network\n",
    "def vgg_network(network_weights, init_image):\n",
    "    network = {}\n",
    "    image = init_image\n",
    "\n",
    "    for i, layer in enumerate(vgg_layers):\n",
    "        if layer[0] == 'c':\n",
    "            weights, bias = network_weights[i][0][0][0][0]\n",
    "            weights = np.transpose(weights, (1, 0, 2, 3))\n",
    "            bias = bias.reshape(-1)\n",
    "            conv_layer = tf.nn.conv2d(image, tf.constant(weights), (1, 1, 1, 1), 'SAME')\n",
    "            image = tf.nn.bias_add(conv_layer, bias)\n",
    "        elif layer[0] == 'r':\n",
    "            image = tf.nn.relu(image)\n",
    "        else:  # pooling\n",
    "            image = tf.nn.max_pool(image, (1, 2, 2, 1), (1, 2, 2, 1), 'SAME')\n",
    "        network[layer] = image\n",
    "    return network\n",
    "\n",
    "# Here we define which layers apply to the original or style image\n",
    "original_layers = ['relu4_2', 'relu5_2']\n",
    "style_layers = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "\n",
    "# Get network parameters\n",
    "normalization_mean, network_weights = extract_net_info(vgg_path)\n",
    "\n",
    "shape = (1,) + original_image.shape\n",
    "style_shape = (1,) + style_image.shape\n",
    "original_features = {}\n",
    "style_features = {}\n",
    "\n",
    "# Set style weights\n",
    "style_weights = {l: 1./(len(style_layers)) for l in style_layers}\n",
    "\n",
    "# Computer feature layers with original image\n",
    "g_original = tf.Graph()\n",
    "with g_original.as_default(), tf.Session() as sess1:\n",
    "    image = tf.placeholder('float', shape=shape)\n",
    "    vgg_net = vgg_network(network_weights, image)\n",
    "    original_minus_mean = original_image - normalization_mean\n",
    "    original_norm = np.array([original_minus_mean])\n",
    "    for layer in original_layers:\n",
    "        original_features[layer] = vgg_net[layer].eval(feed_dict={image: original_norm})\n",
    "\n",
    "# Get style image network\n",
    "g_style = tf.Graph()\n",
    "with g_style.as_default(), tf.Session() as sess2:\n",
    "    image = tf.placeholder('float', shape=style_shape)\n",
    "    vgg_net = vgg_network(network_weights, image)\n",
    "    style_minus_mean = style_image - normalization_mean\n",
    "    style_norm = np.array([style_minus_mean])\n",
    "    for layer in style_layers:\n",
    "        features = vgg_net[layer].eval(feed_dict={image: style_norm})\n",
    "        features = np.reshape(features, (-1, features.shape[3]))\n",
    "        gram = np.matmul(features.T, features) / features.size\n",
    "        style_features[layer] = gram\n",
    "\n",
    "# Make Combined Image via loss function\n",
    "with tf.Graph().as_default():\n",
    "    # Get network parameters\n",
    "    initial = tf.random_normal(shape) * 0.256\n",
    "    init_image = tf.Variable(initial)\n",
    "    vgg_net = vgg_network(network_weights, init_image)\n",
    "\n",
    "    # Loss from Original Image\n",
    "    original_layers_w = {'relu4_2': 0.5, 'relu5_2': 0.5}\n",
    "    original_loss = 0\n",
    "    for o_layer in original_layers:\n",
    "        temp_original_loss = original_layers_w[o_layer] * original_image_weight *\\\n",
    "                             (2 * tf.nn.l2_loss(vgg_net[o_layer] - original_features[o_layer]))\n",
    "        original_loss += (temp_original_loss / original_features[o_layer].size)\n",
    "\n",
    "    # Loss from Style Image\n",
    "    style_loss = 0\n",
    "    style_losses = []\n",
    "    for style_layer in style_layers:\n",
    "        layer = vgg_net[style_layer]\n",
    "        feats, height, width, channels = [x.value for x in layer.get_shape()]\n",
    "        size = height * width * channels\n",
    "        features = tf.reshape(layer, (-1, channels))\n",
    "        style_gram_matrix = tf.matmul(tf.transpose(features), features) / size\n",
    "        style_expected = style_features[style_layer]\n",
    "        style_losses.append(style_weights[style_layer] * 2 *\n",
    "                            tf.nn.l2_loss(style_gram_matrix - style_expected) /\n",
    "                            style_expected.size)\n",
    "    style_loss += style_image_weight * tf.reduce_sum(style_losses)\n",
    "\n",
    "    # To Smooth the results, we add in total variation loss\n",
    "    total_var_x = reduce(mul, init_image[:, 1:, :, :].get_shape().as_list(), 1)\n",
    "    total_var_y = reduce(mul, init_image[:, :, 1:, :].get_shape().as_list(), 1)\n",
    "    first_term = regularization_weight * 2\n",
    "    second_term_numerator = tf.nn.l2_loss(init_image[:, 1:, :, :] - init_image[:, :shape[1]-1, :, :])\n",
    "    second_term = second_term_numerator / total_var_y\n",
    "    third_term = (tf.nn.l2_loss(init_image[:, :, 1:, :] - init_image[:, :, :shape[2]-1, :]) / total_var_x)\n",
    "    total_variation_loss = first_term * (second_term + third_term)\n",
    "\n",
    "    # Combined Loss\n",
    "    loss = original_loss + style_loss + total_variation_loss\n",
    "\n",
    "    # Declare Optimization Algorithm\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1, beta2)\n",
    "    train_step = optimizer.minimize(loss)\n",
    "\n",
    "    # Initialize variables and start training\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for i in range(generations):\n",
    "\n",
    "            train_step.run()\n",
    "\n",
    "            # Print update and save temporary output\n",
    "            if (i+1) % output_generations == 0:\n",
    "                print('Generation {} out of {}, loss: {}'.format(i + 1, generations,sess.run(loss)))\n",
    "\n",
    "                image_eval = init_image.eval(session=sess)\n",
    "                image_eval = image_eval.reshape(shape[1:])  # Á°Æ‰øùÂΩ¢Áä∂Ê≠£Á°Æ\n",
    "                image_eval += normalization_mean  # Âä†‰∏äÂùáÂÄº\n",
    "                image_eval = np.clip(image_eval, 0, 255)  # Á°Æ‰øùÂÄºÂú®0Âà∞255‰πãÈó¥\n",
    "                image_eval = image_eval.astype(np.uint8)  # ËΩ¨Êç¢‰∏∫uint8\n",
    "\n",
    "# ‰øùÂ≠òÂõæÂÉè\n",
    "output_file = 'temp_output_{}.jpg'.format(i)\n",
    "imageio.imwrite(output_file, image_eval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepdream in TensorFlow\n",
    "Note: There is no new code in this script. It originates from the TensorFlow tutorial located here. However, this code is modified slightly to run on Python 3. The code is also commented very heavily to explain, line-by-line, what occurs in the deepdream demo.\n",
    "\n",
    "Here are some potential outputs.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/CNN/06_deepdream_ex.png\n",
    "---\n",
    "name: Deepdream outputs\n",
    "---\n",
    "deepdream\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TensorFlow for Deep Dream\n",
    "#---------------------------------------\n",
    "# From: Alexander Mordvintsev\n",
    "#      --https://www.tensorflow.org/tutorials/generative/deepdream\n",
    "#\n",
    "# And as this code use Tensorflow 2.x, you may need to restart the kernel to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import IPython.display as display\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image and read it into a NumPy array.\n",
    "def download(url, max_dim=None):\n",
    "  name = url.split('/')[-1]\n",
    "  image_path = tf.keras.utils.get_file(name, origin=url)\n",
    "  img = PIL.Image.open(image_path)\n",
    "  if max_dim:\n",
    "    img.thumbnail((max_dim, max_dim))\n",
    "  return np.array(img)\n",
    "\n",
    "# Normalize an image\n",
    "def deprocess(img):\n",
    "  img = 255*(img + 1.0)/2.0\n",
    "  return tf.cast(img, tf.uint8)\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "  display.display(PIL.Image.fromarray(np.array(img)))\n",
    "\n",
    "\n",
    "# Downsizing the image makes it easier to work with.\n",
    "original_img = download(url, max_dim=500)\n",
    "show(original_img)\n",
    "display.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximize the activations of these layers\n",
    "names = ['mixed3', 'mixed5']\n",
    "layers = [base_model.get_layer(name).output for name in names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "dream_model = tf.keras.Model(inputs=base_model.input, outputs=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(img, model):\n",
    "  # Pass forward the image through the model to retrieve the activations.\n",
    "  # Converts the image into a batch of size 1.\n",
    "  img_batch = tf.expand_dims(img, axis=0)\n",
    "  layer_activations = model(img_batch)\n",
    "  if len(layer_activations) == 1:\n",
    "    layer_activations = [layer_activations]\n",
    "\n",
    "  losses = []\n",
    "  for act in layer_activations:\n",
    "    loss = tf.math.reduce_mean(act)\n",
    "    losses.append(loss)\n",
    "\n",
    "  return  tf.reduce_sum(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepDream(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(\n",
    "      input_signature=(\n",
    "        tf.TensorSpec(shape=[None,None,3], dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=[], dtype=tf.float32),)\n",
    "  )\n",
    "  def __call__(self, img, steps, step_size):\n",
    "      print(\"Tracing\")\n",
    "      loss = tf.constant(0.0)\n",
    "      for n in tf.range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "          # This needs gradients relative to `img`\n",
    "          # `GradientTape` only watches `tf.Variable`s by default\n",
    "          tape.watch(img)\n",
    "          loss = calc_loss(img, self.model)\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the pixels of the input image.\n",
    "        gradients = tape.gradient(loss, img)\n",
    "\n",
    "        # Normalize the gradients.\n",
    "        gradients /= tf.math.reduce_std(gradients) + 1e-8 \n",
    "        \n",
    "        # In gradient ascent, the \"loss\" is maximized so that the input image increasingly \"excites\" the layers.\n",
    "        # You can update the image by directly adding the gradients (because they're the same shape!)\n",
    "        img = img + gradients*step_size\n",
    "        img = tf.clip_by_value(img, -1, 1)\n",
    "\n",
    "      return loss, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepdream = DeepDream(dream_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_dream_simple(img, steps=100, step_size=0.01):\n",
    "  # Convert from uint8 to the range expected by the model.\n",
    "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "  img = tf.convert_to_tensor(img)\n",
    "  step_size = tf.convert_to_tensor(step_size)\n",
    "  steps_remaining = steps\n",
    "  step = 0\n",
    "  while steps_remaining:\n",
    "    if steps_remaining>100:\n",
    "      run_steps = tf.constant(100)\n",
    "    else:\n",
    "      run_steps = tf.constant(steps_remaining)\n",
    "    steps_remaining -= run_steps\n",
    "    step += run_steps\n",
    "\n",
    "    loss, img = deepdream(img, run_steps, tf.constant(step_size))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    show(deprocess(img))\n",
    "    print (\"Step {}, loss {}\".format(step, loss))\n",
    "\n",
    "\n",
    "  result = deprocess(img)\n",
    "  display.clear_output(wait=True)\n",
    "  show(result)\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dream_img = run_deep_dream_simple(img=original_img, \n",
    "                                  steps=100, step_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "OCTAVE_SCALE = 1.30\n",
    "\n",
    "img = tf.constant(np.array(original_img))\n",
    "base_shape = tf.shape(img)[:-1]\n",
    "float_base_shape = tf.cast(base_shape, tf.float32)\n",
    "\n",
    "for n in range(-2, 3):\n",
    "  new_shape = tf.cast(float_base_shape*(OCTAVE_SCALE**n), tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape).numpy()\n",
    "\n",
    "  img = run_deep_dream_simple(img=img, steps=50, step_size=0.01)\n",
    "\n",
    "display.clear_output(wait=True)\n",
    "img = tf.image.resize(img, base_shape)\n",
    "img = tf.image.convert_image_dtype(img/255.0, dtype=tf.uint8)\n",
    "show(img)\n",
    "\n",
    "end = time.time()\n",
    "end-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn! üöÄ\n",
    "\n",
    "TBD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self study\n",
    "\n",
    "You can refer to those YouTube videos for further study:\n",
    "\n",
    "- [Convolutional Neural Networks (CNNs) explained, by deeplizard](https://www.youtube.com/watch?v=YRhxdVk_sIs)\n",
    "- [Convolutional Neural Networks Explained (CNN Visualized), by Futurology](https://www.youtube.com/watch?v=pj9-rr1wDhM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research trend\n",
    "\n",
    "State of the Art Convolutional Neural Networks (CNNs) Explained | Deep Learning in 2020:\n",
    "\n",
    "<div class=\"yt-container\">\n",
    "   <iframe src=\"https://www.youtube.com/embed/YUyec4eCEiY\" allowfullscreen></iframe>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Nick](https://github.com/nfmcclure) for creating the open-source course [tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook). And thanks to [TensorFlow](https://www.tensorflow.org/) for creating the open source project [DeepDream](https://www.tensorflow.org/tutorials/generative/deepdream) It inspires the majority of the content in this chapter.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
