{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "license:\n",
    "    code: MIT\n",
    "    content: CC-BY-4.0\n",
    "github: https://github.com/ocademy-ai/machine-learning\n",
    "venue: By Ocademy\n",
    "open_access: true\n",
    "bibliography:\n",
    "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339a744",
   "metadata": {},
   "source": [
    "\n",
    "# Generative adversarial networks \n",
    "\n",
    "The original purpose of Generative adversarial networks(GAN) is to generate new data. It classically generates new images, but is applicable to wide range of domains. It learns the training set distribution and can generate new images that have never been seen before. In contrast to e.g., autoregressive models or RNNs (generating one word at a time), GANs generate the whole output all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d0fb6",
   "metadata": {
    "attributes": {
     "classes": [
      "seealso"
     ],
     "id": ""
    }
   },
   "source": [
    "GAN is proposed in 2005, the paper is Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. \"Generative Adversarial Networks\", arxiv:1406.2661."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b87585",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# A bunch of utility functions\n",
    "\n",
    "def show_images(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n",
    "    param_count = np.sum([np.prod(p.shape) for p in model.weights])\n",
    "    return param_count\n",
    "url = 'https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/Gan/gan-checks-tf.npz'\n",
    "response = requests.get(url)\n",
    "with open('gan-checks-tf.npz', 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Now you can use np.load on the downloaded file\n",
    "answers = np.load('gan-checks-tf.npz')\n",
    "\n",
    "NOISE_DIM = 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c555b",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd987e4",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "class MNIST(object):\n",
    "    def __init__(self, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct an iterator object over the MNIST data\n",
    "        \n",
    "        Inputs:\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        train, _ = tf.keras.datasets.mnist.load_data()\n",
    "        X, y = train\n",
    "        X = X.astype(np.float32)/255\n",
    "        X = X.reshape((X.shape[0], -1))\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B)) \n",
    "mnist = MNIST(batch_size=25) \n",
    "show_images(mnist.X[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c3002",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.01):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    # TODO: implement leaky ReLU\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    x = tf.nn.leaky_relu(x,alpha)\n",
    "    return x\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"Generate random uniform noise from -1 to 1.\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size of noise to generate\n",
    "    - dim: integer giving the dimension of the noise to generate\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor containing uniform noise in [-1, 1] with shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # TODO: sample and return noise\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    noise = tf.random.uniform([batch_size,dim],minval = -1,maxval = 1)\n",
    "    return noise\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"Compute discriminator score for a batch of input images.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor of flattened input images, shape [batch_size, 784]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with shape [batch_size, 1], containing the score \n",
    "    for an image being real for each input image.\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        tf.keras.layers.InputLayer(784),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.LeakyReLU(0.01),\n",
    "        tf.keras.layers.Dense(256),\n",
    "        tf.keras.layers.LeakyReLU(0.01),\n",
    "        tf.keras.layers.Dense(1)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ])\n",
    "    return model\n",
    "def test_discriminator(true_count=267009):\n",
    "    model = discriminator()\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Incorrect number of parameters in discriminator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Correct number of parameters in discriminator.')\n",
    "        \n",
    "test_discriminator()\n",
    "\n",
    "def generator(noise_dim=NOISE_DIM):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "    \n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 784].\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # TODO: implement architecture\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        tf.keras.layers.InputLayer(noise_dim),\n",
    "        tf.keras.layers.Dense(1024),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(1024),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.Dense(784,activation = tf.nn.tanh)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ])\n",
    "    return model\n",
    "def test_generator(true_count=1858320):\n",
    "    model = generator(4)\n",
    "    cur_count = count_params(model)\n",
    "    if cur_count != true_count:\n",
    "        print('Incorrect number of parameters in generator. {0} instead of {1}. Check your achitecture.'.format(cur_count,true_count))\n",
    "    else:\n",
    "        print('Correct number of parameters in generator.')\n",
    "        \n",
    "test_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(logits_real, logits_fake):\n",
    "\n",
    "    loss = None\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    real_loss = cross_entropy(tf.ones_like(logits_real), logits_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(logits_fake), logits_fake)\n",
    "    loss = real_loss + fake_loss\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss\n",
    "\n",
    "def generator_loss(logits_fake):\n",
    "    \"\"\"\n",
    "    Computes the generator loss described above.\n",
    "\n",
    "    Inputs:\n",
    "    - logits_fake: PyTorch Tensor of shape (N,) giving scores for the fake data.\n",
    "    \n",
    "    Returns:\n",
    "    - loss: PyTorch Tensor containing the (scalar) loss for the generator.\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    loss = cross_entropy(tf.ones_like(logits_fake), logits_fake)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return loss\n",
    "def test_discriminator_loss(logits_real, logits_fake, d_loss_true):\n",
    "    d_loss = discriminator_loss(tf.constant(logits_real),\n",
    "                                tf.constant(logits_fake))\n",
    "    print(\"Maximum error in d_loss: %g\"%rel_error(d_loss_true, d_loss))\n",
    "\n",
    "test_discriminator_loss(answers['logits_real'], answers['logits_fake'],\n",
    "                        answers['d_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb70ce8",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_generator_loss(logits_fake, g_loss_true):\n",
    "    g_loss = generator_loss(tf.constant(logits_fake))\n",
    "    print(\"Maximum error in g_loss: %g\"%rel_error(g_loss_true, g_loss))\n",
    "\n",
    "test_generator_loss(answers['logits_fake'], answers['g_loss_true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d946dfd",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_solvers(learning_rate=1e-3, beta1=0.5):\n",
    "    \"\"\"Create solvers for GAN training.\n",
    "    \n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "    \n",
    "    Returns:\n",
    "    - D_solver: instance of tf.optimizers.Adam with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.optimizers.Adam with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "    D_solver = None\n",
    "    G_solver = None\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    D_solver = tf.optimizers.Adam(learning_rate,beta1)\n",
    "    G_solver = tf.optimizers.Adam(learning_rate,beta1)\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return D_solver, G_solver\n",
    "# a giant helper function\n",
    "def run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss,\\\n",
    "              show_every=20, print_every=20, batch_size=128, num_epochs=10, noise_size=96):\n",
    "    \"\"\"Train a GAN for a certain number of epochs.\n",
    "    \n",
    "    Inputs:\n",
    "    - D: Discriminator model\n",
    "    - G: Generator model\n",
    "    - D_solver: an Optimizer for Discriminator\n",
    "    - G_solver: an Optimizer for Generator\n",
    "    - generator_loss: Generator loss\n",
    "    - discriminator_loss: Discriminator loss\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    mnist = MNIST(batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    iter_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for (x, _) in mnist:\n",
    "            with tf.GradientTape() as tape:\n",
    "                real_data = x\n",
    "                logits_real = D(preprocess_img(real_data))\n",
    "\n",
    "                g_fake_seed = sample_noise(batch_size, noise_size)\n",
    "                fake_images = G(g_fake_seed)\n",
    "                logits_fake = D(tf.reshape(fake_images, [batch_size, 784]))\n",
    "                d_total_error = discriminator_loss(logits_real, logits_fake)\n",
    "                d_gradients = tape.gradient(d_total_error, D.trainable_variables)      \n",
    "                D_solver.apply_gradients(zip(d_gradients, D.trainable_variables))\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                g_fake_seed = sample_noise(batch_size, noise_size)\n",
    "                fake_images = G(g_fake_seed)\n",
    "\n",
    "                gen_logits_fake = D(tf.reshape(fake_images, [batch_size, 784]))\n",
    "                g_error = generator_loss(gen_logits_fake)\n",
    "                g_gradients = tape.gradient(g_error, G.trainable_variables)      \n",
    "                G_solver.apply_gradients(zip(g_gradients, G.trainable_variables))\n",
    "\n",
    "            if (iter_count % show_every == 0):\n",
    "                print('Epoch: {}, Iter: {}, D: {:.4}, G:{:.4}'.format(epoch, iter_count,d_total_error,g_error))\n",
    "                imgs_numpy = fake_images.cpu().numpy()\n",
    "                show_images(imgs_numpy[0:16])\n",
    "                plt.show()\n",
    "            iter_count += 1\n",
    "    \n",
    "    # random noise fed into our generator\n",
    "    z = sample_noise(batch_size, noise_size)\n",
    "    # generated images\n",
    "    G_sample = G(z)\n",
    "    print('Final images')\n",
    "    show_images(G_sample[:16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e38e7",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make the discriminator\n",
    "D = discriminator()\n",
    "\n",
    "# Make the generator\n",
    "G = generator()\n",
    "\n",
    "# Use the function you wrote earlier to get optimizers for the Discriminator and the Generator\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "# Run it!\n",
    "run_a_gan(D, G, D_solver, G_solver, discriminator_loss, generator_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774af8c",
   "metadata": {},
   "source": [
    "## Your turn! ðŸš€\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Sebastian Raschka](https://github.com/rasbt) for creating the open-source project [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20) and [Diego Gomez](https://github.com/diegoalejogm) for creating the open-source project [gans](https://github.com/diegoalejogm/gans). They inspire the majority of the content in this chapter.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utseusgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
