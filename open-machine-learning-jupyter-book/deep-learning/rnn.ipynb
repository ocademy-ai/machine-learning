{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f92eda8",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Install the necessary dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a27ae",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "---\n",
    "license:\n",
    "    code: MIT\n",
    "    content: CC-BY-4.0\n",
    "github: https://github.com/ocademy-ai/machine-learning\n",
    "venue: By Ocademy\n",
    "open_access: true\n",
    "bibliography:\n",
    "  - https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42911db",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "> The nervous system contains many circular paths, whose activity so regenerates the excitation of any participant neuron that reference to time past becomes indefinite, although it still implies that afferent activity has realized one of a certain class of configurations over time. Precise specification of these implications by means of recursive functions, and determination of those that can be embodied in the activity of nervous nets, completes the theory.\n",
    "> \n",
    "> -- Warren McCulloch and Walter Pitts, 1943\n",
    "\n",
    "A recurrent neural network (RNN) is a type of artificial neural network that uses sequential data or time series data and it is mainly used for Natural Language Processing. Now let us see what it looks like.\n",
    "\n",
    "Sequential data is not [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables).\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/sequential_data.png\n",
    "---\n",
    "name: 'sequential data'\n",
    "width: 90%\n",
    "---\n",
    "sequential data\n",
    ":::\n",
    "\n",
    "And the RNNs use recurrent edge to update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6e8c",
   "metadata": {},
   "source": [
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/rnn1.png\n",
    "---\n",
    "name: 'RNN1'\n",
    "width: 90%\n",
    "---\n",
    "RNN1\n",
    ":::\n",
    "\n",
    "If unroll over a sequence $(x_0,x_1,x_2)$.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/rnn2.png\n",
    "---\n",
    "name: 'RNN2'\n",
    "width: 90%\n",
    "---\n",
    "RNN2\n",
    ":::\n",
    "\n",
    "Then, the input (w0,w1,...,wt) sequence of words ( 1-hot encoded ) and the output (w1,w2,...,wt+1) shifted sequence of words ( 1-hot encoded ) have the following relation.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/rnn3.png\n",
    "---\n",
    "name: 'RNN3'\n",
    "width: 90%\n",
    "---\n",
    "RNN3\n",
    ":::\n",
    "\n",
    "The input projection is $x_t = Emb(\\omega_t) = E\\omega_t$, the recurrent connection is $h_t = g(W^h h_t + x_t + b^h)$, and the output projection should be $y = softmax(W^o h_t + b^o)$.\n",
    "The backpropagation of RNN is in this way:\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/rnn4.png\n",
    "---\n",
    "name: 'RNN4'\n",
    "width: 90%\n",
    "---\n",
    "RNN4\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b3000",
   "metadata": {},
   "source": [
    "Let's make the backpropagation process more clearly.\n",
    "\n",
    "First, we unfold a single-hidden layer RNN, and we can see the weight matrices $W_h$ in it.\n",
    "\n",
    ":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/RNN/bp_rnn1.png\n",
    "---\n",
    "name: 'backpropagation for RNN'\n",
    "width: 90%\n",
    "---\n",
    "backpropagation for RNN\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214e042",
   "metadata": {},
   "source": [
    "Through the image, we can get the output:\n",
    "\n",
    "- Net input: $z_h^{<t>} = W_hx x^{<t>} + W_hh h^{<t-1>} + b_h$\n",
    "- Activation: $h^{<t>} = \\sigma (z_h^{<t>})$\n",
    "- Output: $z_y^<t> = W_yh h^{<t>} + b_y$, $y^{<t>} = \\sigma(z_y^{<t>})$\n",
    "\n",
    "After that, the loss is computed as the sum over all time steps: $L = \\sum_{t=1}^T L^{<t>}$\n",
    "\n",
    ":::{note}\n",
    "There are some key points:\n",
    "\n",
    "- Similar as training very deep networks with tied parameters.\n",
    "- Example between $x_0$ and $y_2$: Wh is used twice.\n",
    "- Usually truncate the backprop after $T$ timesteps.\n",
    "- Difficulties to train long-term dependencies.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b47ea",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52f47d",
   "metadata": {},
   "source": [
    "A text classifier implemented in TensorFlow to classify SMS spam messages.\n",
    "Code first downloads and processes the SMS Spam Collection dataset from the UCI Machine Learning Repository and then builds a basic Recurrent neural network (RNN) for text classification using TensorFlow.\n",
    "The code first cleans and preprocesses the text, then splits it into training and test sets, followed by tokenizing and padding the training set. Next, the code uses an embedding layer to convert the tokenized text into a vector representation, which is then fed into a recurrent neural network and finally classified using a Softmax loss function.\n",
    "The output of the # code is the accuracy of the classifier along with some statistics\n",
    "We implement an RNN in TensorFlow to predict spam/ham from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8241ee1",
   "metadata": {
    "attributes": {
     "classes": [
      "code-cell"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fa5d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download or open data\n",
    "data_dir = \"tmp\"\n",
    "data_file = \"text_data.txt\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    zip_url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read(\"SMSSpamCollection\")\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode(\"ascii\", errors=\"ignore\")\n",
    "    text_data = text_data.decode().split(\"\\n\")\n",
    "\n",
    "    # Save data to text file\n",
    "    with open(os.path.join(data_dir, data_file), \"w\") as file_conn:\n",
    "        for text in text_data:\n",
    "            file_conn.write(\"{}\\n\".format(text))\n",
    "else:\n",
    "    # Open data from text file\n",
    "    text_data = []\n",
    "    with open(os.path.join(data_dir, data_file), \"r\") as file_conn:\n",
    "        for row in file_conn:\n",
    "            text_data.append(row)\n",
    "    text_data = text_data[:-1]\n",
    "\n",
    "text_data = [x.split(\"\\t\") for x in text_data if len(x) >= 1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2d94450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r\"([^\\s\\w]|_|[0-9])+\", \"\", text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c01c5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat', 'ok lar joking wif u oni', 'free entry in a wkly comp to win fa cup final tkts st may text fa to to receive entry questionstd txt ratetcs apply overs', 'u dun say so early hor u c already then say', 'nah i dont think he goes to usf he lives around here though']\n",
      "(5574, 25)\n"
     ]
    }
   ],
   "source": [
    "# Clean texts\n",
    "text_data_train = [clean_text(x) for x in text_data_train]\n",
    "print(text_data_train[:5])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data_train)\n",
    "text_processed = tokenizer.texts_to_sequences(text_data_train)\n",
    "max_sequence_length = 25\n",
    "text_processed = pad_sequences(\n",
    "    text_processed, maxlen=max_sequence_length, padding=\"post\"\n",
    ")\n",
    "print(text_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d94dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 8630\n",
      "80-20 Train Test split: 4459 -- 1115\n",
      "Epoch 1/20\n",
      "15/15 [==============================] - 2s 41ms/step - loss: 0.6142 - accuracy: 0.7062 - val_loss: 0.4936 - val_accuracy: 0.8879\n",
      "Epoch 2/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.4652 - accuracy: 0.8623 - val_loss: 0.3963 - val_accuracy: 0.9215\n",
      "Epoch 3/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3839 - accuracy: 0.8980 - val_loss: 0.3211 - val_accuracy: 0.9361\n",
      "Epoch 4/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.3174 - accuracy: 0.9322 - val_loss: 0.2614 - val_accuracy: 0.9585\n",
      "Epoch 5/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.2671 - accuracy: 0.9448 - val_loss: 0.2177 - val_accuracy: 0.9630\n",
      "Epoch 6/20\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.2230 - accuracy: 0.9619 - val_loss: 0.1832 - val_accuracy: 0.9686\n",
      "Epoch 7/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1839 - accuracy: 0.9770 - val_loss: 0.1638 - val_accuracy: 0.9641\n",
      "Epoch 8/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1537 - accuracy: 0.9849 - val_loss: 0.1778 - val_accuracy: 0.9462\n",
      "Epoch 9/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1351 - accuracy: 0.9885 - val_loss: 0.1349 - val_accuracy: 0.9630\n",
      "Epoch 10/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1164 - accuracy: 0.9885 - val_loss: 0.1393 - val_accuracy: 0.9540\n",
      "Epoch 11/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.1019 - accuracy: 0.9936 - val_loss: 0.1193 - val_accuracy: 0.9574\n",
      "Epoch 12/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0907 - accuracy: 0.9924 - val_loss: 0.1223 - val_accuracy: 0.9596\n",
      "Epoch 13/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0807 - accuracy: 0.9947 - val_loss: 0.1254 - val_accuracy: 0.9574\n",
      "Epoch 14/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0712 - accuracy: 0.9952 - val_loss: 0.1198 - val_accuracy: 0.9563\n",
      "Epoch 15/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0657 - accuracy: 0.9952 - val_loss: 0.1182 - val_accuracy: 0.9608\n",
      "Epoch 16/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0618 - accuracy: 0.9961 - val_loss: 0.1213 - val_accuracy: 0.9596\n",
      "Epoch 17/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0541 - accuracy: 0.9972 - val_loss: 0.1225 - val_accuracy: 0.9596\n",
      "Epoch 18/20\n",
      "15/15 [==============================] - 0s 15ms/step - loss: 0.0467 - accuracy: 0.9978 - val_loss: 0.1207 - val_accuracy: 0.9552\n",
      "Epoch 19/20\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.0455 - accuracy: 0.9972 - val_loss: 0.1106 - val_accuracy: 0.9563\n",
      "Epoch 20/20\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.0407 - accuracy: 0.9975 - val_loss: 0.1170 - val_accuracy: 0.9552\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Shuffle and split data\n",
    "text_processed = np.array(text_processed)\n",
    "text_data_target = np.array([1 if x == \"ham\" else 0 for x in text_data_target])\n",
    "shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\n",
    "x_shuffled = text_processed[shuffled_ix]\n",
    "y_shuffled = text_data_target[shuffled_ix]\n",
    "\n",
    "# Split train/test set\n",
    "ix_cutoff = int(len(y_shuffled) * 0.80)\n",
    "x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "print(\"80-20 Train Test split: {:d} -- {:d}\".format(len(y_train), len(y_test)))\n",
    "\n",
    "# Create the model using the Sequential API\n",
    "embedding_size = 50\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            input_length=max_sequence_length,\n",
    "        ),\n",
    "        tf.keras.layers.SimpleRNN(units=10),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=2, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0005),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "epochs = 20\n",
    "batch_size = 250\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2\n",
    ")\n",
    "\n",
    "# Plot loss and accuracy over time\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Set\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Set\")\n",
    "plt.title(\"Softmax Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Softmax Loss\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Set\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Set\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc90f40",
   "metadata": {},
   "source": [
    "## Your turn! ðŸš€\n",
    "\n",
    "Practice the Recurrent Neural Networks by following this TBD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d18f5",
   "metadata": {},
   "source": [
    "## Self study\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0ab70",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Nick](https://github.com/nfmcclure) for creating the open-source course [tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook) and [Sebastian Raschka](https://github.com/rasbt) for creating the open-sourse [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20). It inspires the majority of the content in this chapter.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
