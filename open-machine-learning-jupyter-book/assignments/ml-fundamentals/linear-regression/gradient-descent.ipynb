{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Objective\n",
    "\n",
    "In previous sessions, we've delved into the application of Linear Regression and Logistic Regression models. You may find the code relatively straightforward and intuitive at this point. However, you might be pondering questions like:\n",
    "\n",
    "- What exactly occurs when we invoke the `.fit()` function?\n",
    "- Why does the execution of the `.fit()` function sometimes take a significant amount of time?\n",
    "\n",
    "This session is designed to provide insight into the functionality of the `.fit()` method, which is responsible for training machine learning models and fine-tuning model parameters. The underlying technique at play here is known as \"Gradient Descent.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Explore and Gain Intuition\n",
    "\n",
    "To further enhance your understanding and gain a playful insight into Gradient Descent, you can explore the following resources:\n",
    "\n",
    "\n",
    "- [Gradient Descent Visualization](https://github.com/lilipads/gradient_descent_viz): This GitHub repository offers a visualization of the Gradient Descent algorithm, which can be a valuable resource for understanding the optimization process.\n",
    "\n",
    "- [Optimization Algorithms Visualization](https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87): Explore this visualization to see how different optimization algorithms, including Gradient Descent, work and how they converge to find optimal solutions.\n",
    "\n",
    "These resources will help you build an intuitive grasp of Gradient Descent and its role in training machine learning models. Enjoy your exploration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math (feel free to skip if you find it difficult)\n",
    "\n",
    "The fundamental concept behind gradient descent is rather straightforward: it involves the gradual adjustment of parameters, such as the slope ($m$) and the intercept ($b$) in our regression equation $y = mx + b, with the aim of minimizing a cost function. This cost function is typically a metric that quantifies the disparity between our model's predicted results and the actual values. In regression scenarios, the widely employed cost function is the `mean squared error` (MSE). When dealing with classification problems, a different set of parameters must be fine-tuned.\n",
    "\n",
    "The MSE (Mean Squared Error) is mathematically expressed as:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "Here, $y_i$ represents the actual data points, $\\hat{y_i}$ signifies the predictions made by our model ($mx_i + b$), and $n$ denotes the total number of data points.\n",
    "\n",
    "Our primary challenge is to determine the optimal adjustments to parameters $m$ and $b\" to minimize the MSE effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives \n",
    "\n",
    "In our pursuit of minimizing the Mean Squared Error (MSE), we turn to partial derivatives to understand how each individual parameter influences the MSE. The term \"partial\" signifies that we are taking derivatives with respect to individual parameters, in this case, $m$ and $b, separately.\n",
    "\n",
    "Consider the following formula, which closely resembles the MSE, but now we've introduced the function $f(m, b)$ into it. The addition of this function doesn't significantly alter the essence of the calculation, but it allows us to input specific values for $m$ and $b$ to compute the result.\n",
    "\n",
    "$$f(m, b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (mx_i+b))^2$$\n",
    "\n",
    "For the purposes of calculating partial derivatives, we can temporarily disregard the summation and the terms preceding it, focusing solely on the expression $y - (mx + b)^2$. This expression serves as a better starting point for the subsequent partial derivative calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivative with Respect to $m$\n",
    "\n",
    "When we calculate the partial derivative with respect to the parameter $m,\" we isolate the parameter $m\" and treat $b$ as a constant (effectively setting it to 0 for differentiation purposes). To compute this derivative, we utilize the chain rule, which is a fundamental concept in calculus.\n",
    "\n",
    "The chain rule is expressed as follows:\n",
    "\n",
    "$$ [f(g(x))]' = f'(g(x)) * g(x)' \\quad - \\textrm{chain rule} $$\n",
    "\n",
    "The chain rule is applicable when one function is nested inside another. In this context, the square operation, $()^2$, is the outer function, while $y - (mx + b)$ is the inner function. Following the chain rule, we differentiate the outer function, maintain the inner function as it is, and then multiply it by the derivative of the inner function. Let's break down the steps:\n",
    "\n",
    "$$ (y - (mx + b))^2 $$\n",
    "\n",
    "1. The derivative of $()^2$ is $2()$, just like $x^2$ becomes $2x$.\n",
    "2. We leave the inner function, $y - (mx + b)$, unaltered.\n",
    "3. The derivative of $y - (mx + b)$ with respect to **_m_** is $(0 - x)$, which simplifies to $-x$. This is because both **_y_** and **_b_** are treated as constants (their derivatives are zero), and the derivative of **_mx_** is simply **_x_**.\n",
    "\n",
    "Now, let's combine these components:\n",
    "\n",
    "$$ 2 \\cdot (y - (mx+b)) \\cdot (-x) $$\n",
    "\n",
    "For clarity, we can rearrange this expression by moving the factor of $-x$ to the left:\n",
    "\n",
    "$$ -2x \\cdot (y-(mx+b)) $$\n",
    "\n",
    "This is the final version of our derivative with respect to $m$:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial m} = \\frac{1}{n}\\sum_{i=1}^{n} -2x_i(y_i - (mx_i+b)) $$\n",
    "\n",
    "Here, $\\frac{df}{dm}$ signifies the partial derivative of function $f$ (as previously defined) with respect to the parameter $m$. We can now insert this derivative into our summation to complete the calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivative with Respect to $b$\n",
    "\n",
    "The process for computing the partial derivative with respect to the parameter $b\" is analogous to our previous derivation with respect to $m. We still apply the same rules and utilize the chain rule:\n",
    "\n",
    "1. The derivative of $()^2$ is $2()$, which corresponds to how $x^2$ becomes $2x$.\n",
    "2. We leave the inner function, $y - (mx + b)$, unaltered.\n",
    "3. For the derivative of $y - (mx + b)$ with respect to **_b_**, it becomes $(0 - 1)$ or simply $-1.\" This is because both **_y_** and **_mx_** are treated as constants (their derivatives are zero), and the derivative of **_b_** is 1.\n",
    "\n",
    "Now, let's consolidate these components:\n",
    "\n",
    "$$ 2 \\cdot (y - (mx+b)) \\cdot (-1) $$\n",
    "\n",
    "Simplifying this expression:\n",
    "\n",
    "$$ -2 \\cdot (y-(mx+b)) $$\n",
    "\n",
    "This is the final version of our derivative with respect to $b$:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial b} = \\frac{1}{n}\\sum_{i=1}^{n} -2(y_i - (mx_i+b)) $$\n",
    "\n",
    "Similarly to the previous case, $\\frac{df}{db}$ represents the partial derivative of function $f$ with respect to the parameter $b\". Inserting this derivative into our summation concludes the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Final Function\n",
    "\n",
    "Before delving into the code, there are a few essential details to address:\n",
    "\n",
    "1. Gradient descent is an iterative process, and with each iteration (referred to as an \"epoch\"), we incrementally reduce the Mean Squared Error (MSE). At each iteration, we apply our derived functions to update the values of parameters $m$ and $b$.\n",
    "\n",
    "2. Because gradient descent is iterative, we must determine how many iterations to perform, or devise a mechanism to stop the algorithm when it approaches the minimum of the MSE. In essence, we continue iterations until the algorithm no longer improves the MSE, signifying that it has reached a minimum.\n",
    "\n",
    "3. An important parameter in gradient descent is the learning rate ($lr$). The learning rate governs the pace at which the algorithm moves toward the minimum of the MSE. A smaller learning rate results in slower but more precise convergence, while a larger learning rate may lead to faster convergence but may overshoot the minimum.\n",
    "\n",
    "In summary, gradient descent primarily involves the process of taking derivatives and applying them iteratively to minimize a function. These derivatives guide us toward optimizing the parameters $m$ and $b\" in order to minimize the Mean Squared Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. One-dimensional gradient descent\n",
    "One-dimensional gradient descent is a simple optimization algorithm used to find the minimum (or maximum) of a univariate function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main steps:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 1: Choose an initial point on the function curve. For example, we choose an initial x value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 2: Compute the derivative of the function at the current point. In this case, the derivative of the function is computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 2 * x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 3: Update the position along the function curve using the gradient and the learning rate. The learning rate determines the size of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "x -= learning_rate * df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 4: Repeat the process for a specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "for i in range(max_iter):\n",
    "    gradient = df(x)\n",
    "    x -= learning_rate * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 5: Output the result, which is the minimum value of x that corresponds to the minimum of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_min:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target function\n",
    "def f(x):\n",
    "    return x ** 2 + 5 * x + 10\n",
    "\n",
    "# Define the guidance of the target function\n",
    "def df(x):\n",
    "    return 2 * x + 5\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = f(x)\n",
    "plt.plot(x, y, label='f(x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient descent function\n",
    "def gradient_descent(learning_rate, max_iter, x_init):\n",
    "    # step 1:start with an initial value for the input variable, denoted as x_int.\n",
    "    x = x_init\n",
    "    x_history = [x]\n",
    "    for i in range(max_iter):\n",
    "        # step 2:compute the derivative of the function at the current point.\n",
    "        gradient = df(x)\n",
    "        x -= learning_rate * gradient\n",
    "        # step 3:update the position along the function curve.\n",
    "        x_history.append(x)\n",
    "    # step 4:repeat the process.\n",
    "    return x, x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and initial value\n",
    "        # learning rate: the size of the step for gradient descent.\n",
    "        # max_iter: number of gradient descent iterations.\n",
    "learning_rate = 0.3\n",
    "max_iter = 100\n",
    "x_init = 10\n",
    "\n",
    "# Run gradient descent function\n",
    "x_min, x_history = gradient_descent(learning_rate, max_iter, x_init)\n",
    "\n",
    "# Draw the target function and the process of gradient descent\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = f(x)\n",
    "y_history = f(np.array(x_history))\n",
    "plt.plot(x, y, label='f(x)')\n",
    "plt.plot(x_history, y_history, 'ro-', label='Gradient Descent')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend()\n",
    "plt.title('One-Dimensional Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "print('x_min:', x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can change `learning rate` and  `max_iter` to see different images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Two-dimensional gradient descent\n",
    "Two-dimensional gradient descent is an optimization algorithm used to find the minimum (or maximum) of a function with two input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main steps:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 1: Choose an initial point on the function surface by specifying initial values for both x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = -5\n",
    "y_init = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 2: Compute the partial derivatives of the function at the current point. These partial derivatives represent the rate of change of the function with respect to x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfx(x, y):\n",
    "    return 2 * x + 5\n",
    "\n",
    "def dfy(x, y):\n",
    "    return 2 * y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 3: Update the positions along the function surface for both x and y using their respective partial derivatives and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "x -= learning_rate * dfx(x, y)\n",
    "y -= learning_rate * dfy(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 4: Repeat the process for a specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 100\n",
    "for i in range(max_iter):\n",
    "    gradient_x = dfx(x, y)\n",
    "    gradient_y = dfy(x, y)\n",
    "    x -= learning_rate * gradient_x\n",
    "    y -= learning_rate * gradient_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> step 5: Output the results, which are the minimum values of x and y that correspond to the minimum of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_min:', x)\n",
    "print('y_min:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target function\n",
    "def f(x, y):\n",
    "    return x ** 2 + y ** 2 + 5 * x + 2 * y + 10\n",
    "\n",
    "# Take the partial derivative of the objective function with respect to x\n",
    "def dfx(x, y):\n",
    "    return 2 * x + 5\n",
    "\n",
    "# Take the partial derivative of the objective function with respect to y\n",
    "def dfy(x, y):\n",
    "    return 2 * y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the two-dimensional gradient descent function\n",
    "def gradient_descent(learning_rate, max_iter, x_init, y_init):\n",
    "    # step 1:start with an initial value for the two input variables, denoted as (x_init, y_init).\n",
    "    x = x_init\n",
    "    y = y_init\n",
    "    x_history = [x]\n",
    "    y_history = [y]\n",
    "    for i in range(max_iter):\n",
    "        # step 2:compute the partial derivatives of the function at the current point.\n",
    "        gradient_x = dfx(x, y)\n",
    "        gradient_y = dfy(x, y)\n",
    "        x -= learning_rate * gradient_x\n",
    "        y -= learning_rate * gradient_y\n",
    "        # step 3:update the positions along the function surface.\n",
    "        x_history.append(x)\n",
    "        y_history.append(y)\n",
    "    # step 4:repeat the process.\n",
    "    return x, y, x_history, y_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the learning rate and initial value\n",
    "learning_rate = 0.1\n",
    "max_iter = 100\n",
    "x_init = -5\n",
    "y_init = 5\n",
    "\n",
    "# Run two-dimensional gradient descent function\n",
    "x_min, y_min, x_history, y_history = gradient_descent(learning_rate, max_iter, x_init, y_init)\n",
    "\n",
    "# Draw the target function and the process of gradient descent\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
    "ax.plot(x_history, y_history, f(np.array(x_history), np.array(y_history)), 'ro-', label='Gradient Descent')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('f(x, y)')\n",
    "ax.legend()\n",
    "ax.set_title('Two-Dimensional Gradient Descent')\n",
    "plt.show()\n",
    "\n",
    "print('x_min:', x_min)\n",
    "print('y_min:', y_min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
