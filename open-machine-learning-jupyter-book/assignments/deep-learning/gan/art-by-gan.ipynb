{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Art by gan"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this Notebook, we will build a Generative Adversarial Network  (GAN) to illustrate the workings of a Generative Adversarial Network and to generate images. Generative modelling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data. As GANs work by identifying the patterns in the data, we will be using oil painted portraits. However, glancing over the dataset gives me an idea that it is going to be a long shot. The orientation and poses in the dataset vary vastly. Keeping that in mind we are still willing to give it a try. Only because portraits are our jam. We basically love oil painted portraits. "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# Importing Libraries\n","import random\n","import math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import PIL\n","from PIL import Image\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Reshape,\n","    UpSampling2D,\n","    Conv2D,\n","    BatchNormalization,\n",")\n","from tensorflow.keras.layers import (\n","    LeakyReLU,\n","    Dropout,\n","    ZeroPadding2D,\n","    Flatten,\n","    Activation,\n",")\n","from tensorflow.keras.optimizers import Adam\n","import tqdm\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","# Settings\n","sns.set(rc={\"axes.facecolor\": \"#EDE9DE\", \"figure.facecolor\": \"#D8CA7E\"})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data loading & Prepreprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For this project, We are using .jpg files of images of portraits. The dataset includes various artists. We are loading data as TensorFlow.Dataset, with a batch size of 64. We have reduced the image size to (64,64), presuming, it will be computationally less taxing on the GPU."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Loading the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","import zipfile\n","directory = \"./tmp/Portraits\"  \n","zip_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/Gan/Portraits.zip\"\n","os.makedirs(directory, exist_ok=True)\n","\n","response = requests.get(zip_url)\n","zip_filename = os.path.join(directory, \"Portraits.zip\")\n","\n","with open(zip_filename, \"wb\") as file:\n","    file.write(response.content)\n","print(\"ZIP File successfully downloaded\")\n","\n","with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n","    zip_ref.extractall(directory)\n","\n","print(\"ZIP File successfully unzipped\")\n","os.remove(zip_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# Importing data\n","data_path = \"./tmp/Portraits\"\n","batch_s = 64\n","# Import as tf.Dataset\n","data = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_path, label_mode=None, image_size=(64, 64), batch_size=16\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now that we have the dataset loaded, let us have a look at a few images."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true},"outputs":[],"source":["# Defing a function to see images\n","def Show_Img(data):\n","    plt.figure(figsize=(15, 15))\n","    for images in data.take(1):\n","        for i in range(18):\n","            ax = plt.subplot(6, 6, i + 1)\n","            ax.imshow(images[i].numpy().astype(\"uint8\"))\n","            ax.axis(\"off\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Most of the images are portraits. A portrait is a painting representation of a person, The face is predominantly depicted portraits along with expressions and postures. To represent the personality of the subject. Since our model is relative a smaller GAN we have reduced the size of the image. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preprocessing the data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Normalization:** For the data normalization, we will convert the data in the range between 0 to 1. This helps in fast convergence and makes it easy for the computer to do calculations faster. \n","Each of the three RGB channels in the image can take pixel values ranging from 0 to 256. Dividing it by 255 converts it to a range between 0 to 1."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Preprocessing the dataset for model\n","data = data.map(lambda x: x / 255.0)\n","data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now the data is up as a tensorflow Dataset object and is Prepocessed. Next up is building the GAN. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Building GAN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["GANs employs deep learning methods. It is a dexterous way of posing the problem as a supervised learning problem. It is composed of two models namely Generator and a Discriminator.\n","\n","Two models are trained simultaneously by an adversarial process. A generator (\"the artist\") learns to create images that look like the dataset while a discriminator (\"the art critic\") learns to tell real images apart from fakes.\n","\n","During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.\n","\n","**In this section we will be:**\n","* Building a Generator\n","* Building a Discriminator"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## The generator\n","\n","The Generator is a neural network that generates the images. It takes in a random noise as seed and outputs sample data. As the GAN's training progresses the Generator output becomes more and more like the training set, as the Generator tries to improve the output so that the discrimination passes the output as a real image. \n","\n","**Following steps are involved in the models building**\n","\n","* Initialising the Model\n","* Defining by adding layers"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Building a generator"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["latent_dim = 100\n","g_resolution = 2\n","\n","# Building a Generator\n","generator = Sequential()\n","generator.add(Dense(4 * 4 * 256, activation=\"relu\", input_dim=latent_dim))\n","generator.add(Reshape((4, 4, 256)))\n","generator.add(UpSampling2D())\n","generator.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","generator.add(BatchNormalization(momentum=0.8))\n","generator.add(Activation(\"relu\"))\n","generator.add(UpSampling2D())\n","generator.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n","generator.add(BatchNormalization(momentum=0.8))\n","generator.add(Activation(\"relu\"))\n","generator.add(UpSampling2D())\n","generator.add(Conv2D(256, kernel_size=3, padding=\"same\"))  #\n","generator.add(BatchNormalization(momentum=0.8))\n","generator.add(Activation(\"relu\"))\n","generator.add(UpSampling2D())\n","generator.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","generator.add(BatchNormalization(momentum=0.8))\n","generator.add(Activation(\"relu\"))\n","generator.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n","generator.add(Activation(\"tanh\"))\n","\n","generator.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now that the Generator is framed, let us see what random output our untrained Generator produces to get an idea of the process. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["seed = tf.random.normal([1, latent_dim])\n","Generated_Portrait = generator(seed, training=False)\n","Generated_Portrait = np.squeeze(Generated_Portrait, axis=0)\n","\n","\n","Generated_Portrait = np.clip(Generated_Portrait, 0, 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Clearly, the output is a random seed containing noise as the Generator is not trained yet. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## The discriminator"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In GANs the Generator works along with the Discriminator. \n","\n","The Discriminator network decided whether the data is fake aka created by the Generator or real i.e. from the original input data. To do so it applies a binary classification method using a sigmoid function to get an output in the range of 0 to 1."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Building a discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["discriminator = Sequential()\n","discriminator.add(\n","    Conv2D(32, kernel_size=3, strides=2, input_shape=(64, 64, 3), padding=\"same\")\n",")\n","discriminator.add(LeakyReLU(alpha=0.2))\n","discriminator.add(Dropout(0.25))\n","discriminator.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","discriminator.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n","discriminator.add(BatchNormalization(momentum=0.8))\n","discriminator.add(LeakyReLU(alpha=0.2))\n","discriminator.add(Dropout(0.25))\n","discriminator.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","discriminator.add(BatchNormalization(momentum=0.8))\n","discriminator.add(LeakyReLU(alpha=0.2))\n","discriminator.add(Dropout(0.25))\n","discriminator.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","discriminator.add(BatchNormalization(momentum=0.8))\n","discriminator.add(LeakyReLU(alpha=0.2))\n","discriminator.add(Dropout(0.25))\n","discriminator.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n","discriminator.add(BatchNormalization(momentum=0.8))\n","discriminator.add(LeakyReLU(alpha=0.2))\n","discriminator.add(Dropout(0.25))\n","discriminator.add(Flatten())\n","discriminator.add(Dense(1, activation=\"sigmoid\"))\n","\n","discriminator.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now with this discriminator(untrained), let us see what verdict it has for the preiously generated image with random noise. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# for the random image generated\n","Generated_Portrait = np.expand_dims(Generated_Portrait, axis=0)\n","Discriminator_Verdict = discriminator(Generated_Portrait)\n","print(Discriminator_Verdict)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The output of the discriminator i.e. The Verdict, Says that there is almost a fifty-fifty chance of the image being real. This is so because the Discriminator is not yet trained. So basically, An untrained Generarator generated some pixel-noise and the untrained Discriminator classified it as \"can't tell\". So far we are on a right track. \n","\n","Let us proceed and build the GAN architecture to train."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## GAN compilation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["GAN training has two sections:\n","\n","**Section 1**: The Discriminator is trained while the Generator is idle. \n","The discriminator is trained real images and random noise (from an untrained generator). This trains it to tell between fake and real. This accommodates the discriminator to predict as fakes.\n","\n","**Section 2**: The Generator is trained while the Discriminator is idle.  In this section, the generator is trained.  After training the Discriminator, this step uses the predictions from the discriminator. Grants the generator to adjust the weights to try to deceive the discriminator. \n","\n","The above method is repeated for a few epochs.  \n","\n","The next section defines the GAN training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class GAN(tf.keras.Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","        self.d_loss_metric = tf.keras.metrics.Mean(name=\"d_loss\")\n","        self.g_loss_metric = tf.keras.metrics.Mean(name=\"g_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.d_loss_metric, self.g_loss_metric]\n","\n","    def train_step(self, real_images):\n","        # Sample random points in the latent space\n","        batch_size = tf.shape(real_images)[0]\n","        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n","        # Decode them to fake images\n","        generated_images = self.generator(seed)\n","        # Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","        # Assemble labels discriminating real from fake images\n","        labels = tf.concat(\n","            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n","        )\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(combined_images)\n","            d_loss = self.loss_fn(labels, predictions)\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        self.d_optimizer.apply_gradients(\n","            zip(grads, self.discriminator.trainable_weights)\n","        )\n","\n","        # Sample random points in the latent space\n","        seed = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Assemble labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator (note that we should *not* update the weights of the discriminator)!\n","        with tf.GradientTape() as tape:\n","            predictions = self.discriminator(self.generator(seed))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","\n","        # Update metrics\n","        self.d_loss_metric.update_state(d_loss)\n","        self.g_loss_metric.update_state(g_loss)\n","        return {\n","            \"d_loss\": self.d_loss_metric.result(),\n","            \"g_loss\": self.g_loss_metric.result(),\n","        }"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Training the model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Calling the above created GAN function trains the generator and discriminator simultaneously. \n","To implement the GAN we must define:\n","* Number of epochs\n","* The optimizers for Generator and Discriminator\n","* The cross-entropy loss\n","After defing optimizers and numbers of epochs, We will define, compile and fit the model. "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# Defining the number of epochs\n","epochs = 100\n","# The optimizers for Generator and Discriminator\n","discriminator_opt = tf.keras.optimizers.Adamax(1.5e-4, 0.5)\n","generator_opt = tf.keras.optimizers.Adamax(1.5e-4, 0.5)\n","# To compute cross entropy loss\n","loss_fn = tf.keras.losses.BinaryCrossentropy()\n","\n","# Defining GAN Model\n","model = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n","\n","# Compiling GAN Model\n","model.compile(d_optimizer=discriminator_opt, g_optimizer=generator_opt, loss_fn=loss_fn)\n","\n","# Fitting the GAN\n","history = model.fit(data, epochs=epochs)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Ploting the Learning Curves"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","pal = [\"#994F5F\", \"#E2AB30\"]\n","# Plotting the learning curve\n","history_df = pd.DataFrame(history.history)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This looks alright-ish! \n","\n","Let us get some portraits done by the GAN and appreciate the art created by this AI. \n","To get the art output we will create a function that saves the output portraits generated. We will be plotting the generated Portraits"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## AI makes artwork"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Number of images to be generate\n","num_img = 18\n","\n","# A function to generate and save images\n","def Potrait_Generator():\n","    Generated_Paintings = []\n","    seed = tf.random.normal([num_img, latent_dim])\n","    generated_image = generator(seed)\n","    generated_image *= 255\n","    generated_image = generated_image.numpy()\n","    for i in range(num_img):\n","        img = tf.keras.preprocessing.image.array_to_img(generated_image[i])\n","        Generated_Paintings.append(img)\n","        img.save(\"Potraits{:02d}.png\".format(i))\n","    return\n","\n","# Generating images\n","Images = Potrait_Generator()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["# Loading generated images\n","Generated_path = \"../../../images/deep-learning/GAN\"\n","Potraits_generated = tf.keras.preprocessing.image_dataset_from_directory(\n","    Generated_path, label_mode=None\n",")\n","# Plotting generated images\n","Show_Img(Potraits_generated)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Conculsion"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In the evaluation of the model: We can see that the GAN picked up the patterns in the portraits. It worked quite well. For further improvement,  as GANs are notorious for being data-hungry, I would consider increasing the dataset. There are many inconsistencies in the data which is rather complicated for the GAN to learn. Cleaning the data with some consistencies in the portrait styles would certainly help. Training it longer i.e. for more epochs would also help. Lastly, one can always strive to make a  more robust architecture for the Neural Networks."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Acknowledgments\n","\n","Thanks to [Karnika Kapoor](https://www.kaggle.com/karnikakapoor) for creating [art-by-gan](https://www.kaggle.com/code/karnikakapoor/art-by-gan). It inspires the majority of the content in this chapter."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.17"}},"nbformat":4,"nbformat_minor":4}
