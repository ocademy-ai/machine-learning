{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN On Foreign Exchange Market\n",
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from plotly import tools\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl\n",
    "from collections import deque\n",
    "\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags for the debuging purposes are presented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this flag is set, each step of the environment's state will be printed\n",
    "ENVIRONMENT_DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\n",
    "    \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/dqn/USD_TRY%20Gemi%20Verileri.csv\"\n",
    ")\n",
    "ds[\"Tarih\"] = pd.to_datetime(ds[\"Tarih\"], errors=\"coerce\")\n",
    "ds[\"Şimdi\"] = pd.to_numeric(ds[\"Şimdi\"].str.replace(\",\", \".\"), errors=\"coerce\")\n",
    "ds[\"Fark %\"] = ds[\"Fark %\"].str.replace(\"%\", \"\")\n",
    "ds[\"Fark %\"] = ds[\"Fark %\"].str.replace(\",\", \".\")\n",
    "\n",
    "ds.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse version of the data will be used. It is expected to make the learning stronger since from 2002 to this date, usd is increasing with respect to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[\"Şimdi\"]\n",
    "Y = ds[\"Tarih\"]\n",
    "X = np.array(X).reshape((len(X), 1))\n",
    "Y = np.array(Y).reshape((len(Y), 1))\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot_date(Y, X, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test data\n",
    "\n",
    "date_split = 4000\n",
    "train = ds[:date_split]\n",
    "test = ds[date_split:]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define envireonment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, data, tl, history_t=10):\n",
    "        self.data = data\n",
    "        self.history_t = history_t\n",
    "        self.tl_start = tl\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.tl = self.tl_start\n",
    "        self.usd = 0\n",
    "        self.done = False\n",
    "        self.profits = 0\n",
    "        self.current_position = \"none\"\n",
    "        self.history = [self.data.iloc[x, :][\"Fark %\"] for x in range(self.history_t)]\n",
    "        self.t = self.history_t\n",
    "        self.last_tl = 0\n",
    "        return self.history\n",
    "\n",
    "    def step(self, act):\n",
    "        reward = 0\n",
    "        if act == 0:  # Hold\n",
    "            self.current_position = self.current_position\n",
    "        elif act == 1:  # Buy\n",
    "            if self.current_position == \"none\":\n",
    "                self.current_position = \"long\"\n",
    "                # Buy usd\n",
    "\n",
    "                self.last_tl = self.tl\n",
    "                self.usd = self.tl / (self.data.iloc[self.t, :][\"Şimdi\"])\n",
    "                self.tl = 0\n",
    "            else:\n",
    "                self.current_position = self.current_position\n",
    "\n",
    "        else:  # sell\n",
    "            if self.current_position == \"long\":\n",
    "                self.current_position = \"none\"\n",
    "                # Sell usd\n",
    "\n",
    "                self.tl = self.usd * (self.data.iloc[self.t, :][\"Şimdi\"])\n",
    "                self.usd = 0\n",
    "                self.profits = self.profits + (self.tl - self.last_tl)\n",
    "\n",
    "                if (self.tl - self.last_tl) > 0:\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    reward = -1\n",
    "            else:\n",
    "                self.current_position = self.current_position\n",
    "\n",
    "        # et next time\n",
    "        self.t += 1\n",
    "        # print(\"history before: \",self.history)\n",
    "        self.history.pop(0)\n",
    "        self.history.append(self.data.iloc[self.t, :][\"Fark %\"])\n",
    "        # print(\"history after: \",self.history)\n",
    "\n",
    "        # print(\"reward: \",reward)\n",
    "\n",
    "        if ENVIRONMENT_DEBUG == True:\n",
    "            print(\n",
    "                \"t: \",\n",
    "                (self.t - self.history_t),\n",
    "                \" reward: \",\n",
    "                reward,\n",
    "                \" profits: \",\n",
    "                self.profits,\n",
    "                \" current position: \",\n",
    "                self.current_position,\n",
    "                \" done: \",\n",
    "                self.done,\n",
    "            )\n",
    "\n",
    "        if self.t == (len(self.data) - 1):\n",
    "            self.done = True\n",
    "            print(\n",
    "                \"Total steps: \",\n",
    "                (self.t - self.history_t),\n",
    "                \" TotalProfit: \",\n",
    "                self.profits,\n",
    "                \" done: \",\n",
    "                self.done,\n",
    "            )\n",
    "\n",
    "        return self.history, reward, self.done, self.profits  # obs, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q-learning Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, hidden_layer_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Dense(self.hidden_layer_size, input_dim=self.state_size, activation=\"relu\")\n",
    "        )\n",
    "        model.add(Dense(self.hidden_layer_size, activation=\"relu\"))\n",
    "        model.add(Dense(self.action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.array(state).astype(float)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def act_greedy(self, state):\n",
    "        state = np.array(state).astype(float)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(\n",
    "                    self.model.predict(next_state)[0]\n",
    "                )\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # macros\n",
    "    EPISODES = 50\n",
    "    STATE_SIZE = 90\n",
    "\n",
    "    # profits list\n",
    "    total_profits = []\n",
    "\n",
    "    # initialize environment and the agent\n",
    "    env = Environment(train, 100, STATE_SIZE)  # 100tl, 60 history\n",
    "    agent = DQNAgent(STATE_SIZE, 3, 100)\n",
    "\n",
    "    # Iterate the game\n",
    "    for e in range(EPISODES):\n",
    "        # check if the buy and sell actions are taken\n",
    "        actions_count = 0\n",
    "\n",
    "        # reset state in the beginning of each game\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, STATE_SIZE])\n",
    "\n",
    "        # time_t represents each frame of the game\n",
    "        # the more time_t the more score\n",
    "        for time_t in range(5000):\n",
    "            # Decide action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            if (action == 1) or (action == 2):\n",
    "                actions_count = actions_count + 1\n",
    "\n",
    "            # Advance the game to the next frame based on the action.\n",
    "            next_state, reward, done, profits = env.step(action)\n",
    "            next_state = np.array(next_state, dtype=np.float32)\n",
    "            next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "\n",
    "            # make rewards = profits (EXPERIMENTAL)\n",
    "            reward = profits\n",
    "\n",
    "            # Remember the previous state, action, reward, and done\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # make next_state the new current state for the next frame.\n",
    "            state = next_state\n",
    "\n",
    "            # done becomes True when the game ends\n",
    "            if done:\n",
    "                total_profits.append(profits)\n",
    "                # print the score and break out of the loop\n",
    "                # print(\"number of actions taken other than hold in this iteration is \",actions_count,\"\\n\")\n",
    "                # print(\"episode: {}/{}, score: {}\".format(e, EPISODES, time_t))\n",
    "                break\n",
    "        # train the agent with the experience of the episode\n",
    "        agent.replay(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_profits(total_profits):\n",
    "    epoch_count = range(1, len(total_profits) + 1)\n",
    "    fig = plt.figure(figsize=(30, 10))\n",
    "    plt.plot(epoch_count, total_profits, \"b-\")\n",
    "    plt.legend(\"Total Profits\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Total Profits\")\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_profits(total_profits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the agent with real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.iloc[::-1]\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test = Environment(test, 100, STATE_SIZE)  # 100tl, 60 history\n",
    "\n",
    "# Iterate the game\n",
    "for e in range(1):\n",
    "    # check if the buy and sell actions are taken\n",
    "    actions_count = 0\n",
    "\n",
    "    # reset state in the beginning of each game\n",
    "    state = env_test.reset()\n",
    "    state = np.reshape(state, [1, STATE_SIZE])\n",
    "\n",
    "    # time_t represents each frame of the game\n",
    "    # the more time_t the more score\n",
    "    for time_t in range(5000):\n",
    "        # Decide action\n",
    "        action = agent.act_greedy(state)\n",
    "\n",
    "        if (action == 1) or (action == 2):\n",
    "            actions_count = actions_count + 1\n",
    "\n",
    "        # Advance the game to the next frame based on the action.\n",
    "        next_state, reward, done, profits = env_test.step(action)\n",
    "\n",
    "        next_state = np.reshape(next_state, [1, STATE_SIZE])\n",
    "\n",
    "        # make rewards = profits (EXPERIMENTAL)\n",
    "        reward = profits\n",
    "\n",
    "        # Remember the previous state, action, reward, and done\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # make next_state the new current state for the next frame.\n",
    "        state = next_state\n",
    "\n",
    "        # done becomes True when the game ends\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "Thanks to [emrebulbul23](https://www.kaggle.com/emrebulbul23) for creating [DQN on foreign exchange market](https://www.kaggle.com/code/emrebulbul23/dqn-on-foreign-exchange-market). It inspired the majority of the content in this article."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
