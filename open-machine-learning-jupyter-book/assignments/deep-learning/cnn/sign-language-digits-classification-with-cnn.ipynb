{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Sign Language Digits Classification with CNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, MaxPool2D, Conv2D, Flatten\n","from keras.optimizers import Adam\n","from keras.preprocessing.image import ImageDataGenerator"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load data from numpy file"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["X = np.load(\n","    \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/cnn/sign-language-digits-classification-with-cnn/X.npy\"\n",")\n","y = np.load(\n","    \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/cnn/sign-language-digits-classification-with-cnn/Y.npy\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# reshape X\n","X = X.reshape(-1, 64, 64, 1)\n","print(\"X Shape:\", X.shape)\n","print(\"Y Shape:\", y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 6))\n","for i, j in enumerate([0, 205, 411, 617, 823, 1030, 1237, 1444, 1650, 1858]):\n","    plt.subplot(2, 5, i + 1)\n","    plt.subplots_adjust(top=2, bottom=1)\n","    plt.imshow(X[j].reshape(64, 64))\n","    plt.title(np.argmax(y[j]))\n","    plt.axis(\"off\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* As you can see, labels and images don't match correctly. So first of all we will re-organize them.\n","* Image size is 64x64\n","* There are 2062 images in dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["list_y = []\n","list_y = [np.where(i == 1)[0][0] for i in y]\n","count = pd.Series(list_y).value_counts()\n","print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","sns.countplot(np.array(list_y))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* We have a balanced dataset."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preparing Data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We will re-organize data to match labels and images correctly.\n","* 204-409   => 0\n","* 822-1028  => 1\n","* 1649-1855 => 2\n","* 1443-1649 => 3\n","* 1236-1443 => 4\n","* 1855-2062 => 5\n","* 615-822   => 6\n","* 409-615   => 7\n","* 1028-1236 => 8\n","* 0-204     => 9"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X_organized = np.concatenate(\n","    (\n","        X[204:409, :],\n","        X[822:1028, :],\n","        X[1649:1855, :],\n","        X[1443:1649, :],\n","        X[1236:1443, :],\n","        X[1855:2062, :],\n","        X[615:822, :],\n","        X[409:615, :],\n","        X[1028:1236, :],\n","        X[0:204, :],\n","    ),\n","    axis=0,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 6))\n","for i, j in enumerate([0, 205, 411, 617, 823, 1030, 1237, 1444, 1650, 1858]):\n","    plt.subplot(2, 5, i + 1)\n","    plt.subplots_adjust(top=2, bottom=1)\n","    plt.imshow(X_organized[j].reshape(64, 64))\n","    plt.title(np.argmax(y[j]))\n","    plt.axis(\"off\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* Now labels and images are matched correctly."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train Test Split"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(\n","    X_organized, y, test_size=0.2, random_state=42\n",")\n","\n","print(\"x_train shape:\", x_train.shape)\n","print(\"x_test shape:\", x_test.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"y_test shape:\", y_test.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* Now our test and train datasets are ready. We can start to create CNN model."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Implementation of CNN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Data Augmentation With Keras API\n","\n","Data augmentation is a technique which generates new training samples without changing labels of images. To generate new samples, some features of images are changed like brightness, rotation or zoom level. To apply it, ImageDataGenerator class is used in KERAS API. This class refers parameters and changes images. After complete the changing process, it returns new samples. This is important! ImageDataGenerator returns only new images. It means that out training dataset consists of different from original dataset. It provides more generalizaton for model anf of course it is desirable.\n","\n","So, in implementation of CNN part, we will use data augmentation and we will change rotation and zoom level of images. we chose these parameters with a simple logic. Think of test data that we might encounter in real life. we don't always hold our hand at 90 degrees. So it is quite possible that we have a rotational change when using sign language. Likewise, the zoom level of the photo to be taken may also change. So we thought we could train my model better by creating a more general data set with these two parameters.  Let's take a closer look at these parameters.\n","\n","* **rotation_range:** Rotation augmentation randomly rotates the image clockwise by a given number between 0 and 360.\n","* **zoom_range:** The percentage of the zoom can be a single float or a range as an array or tuple. If a float is specified, then the range for the zoom will be [1-value, 1+value].\n","\n","We will apply data augmentation with this parameters.\n","* rotation = 45\n","* zoom_range = 0.5\n","\n","Before continue to CNN implementation, let's look some samples to see effects of data augmentation on dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_new_samples(new_images):\n","    plt.figure(figsize=(20, 6))\n","    for i in range(10):\n","        plt.subplot(2, 5, i + 1)\n","        image = new_images.next()\n","        plt.imshow(image[0].reshape(64, 64))\n","        plt.axis(\"off\")\n","\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Changin zoom level"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(zoom_range=0.5)\n","new_images = datagen.flow(x_train, batch_size=250)\n","show_new_samples(new_images)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Changing rotaion"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(rotation_range=45)\n","new_images = datagen.flow(x_train, batch_size=250)\n","show_new_samples(new_images)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Changing rotaion, zoom"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["datagen = ImageDataGenerator(zoom_range=0.5, rotation_range=45)\n","new_images = datagen.flow(x_train, batch_size=1)\n","show_new_samples(new_images)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = Sequential()\n","\n","model.add(\n","    Conv2D(\n","        filters=32,\n","        kernel_size=(9, 9),\n","        padding=\"Same\",\n","        activation=\"relu\",\n","        input_shape=(64, 64, 1),\n","    )\n",")\n","model.add(MaxPool2D(pool_size=(5, 5)))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(filters=64, kernel_size=(7, 7), padding=\"Same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(4, 4), strides=(3, 3)))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(filters=128, kernel_size=(5, 5), padding=\"Same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(3, 3), strides=(2, 2)))\n","model.add(Dropout(0.2))\n","\n","model.add(Flatten())\n","model.add(Dropout(0.2))\n","model.add(Dense(256, activation=\"relu\"))\n","model.add(Dense(10, activation=\"softmax\"))\n","\n","optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n","\n","model.compile(\n","    optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","\n","datagen = ImageDataGenerator(zoom_range=0.5, rotation_range=45)\n","datagen.fit(x_train)\n","\n","history = model.fit(\n","    datagen.flow(x_train, y_train, batch_size=250),\n","    epochs=100,\n","    validation_data=(x_test, y_test),\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10, 5))\n","plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation loss\")\n","plt.title(\"Test Loss\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_predict = model.predict(x_test)\n","y_predict_classes = np.argmax(y_predict, axis=1)\n","y_true = np.argmax(y_test, axis=1)\n","confusion_mtx = confusion_matrix(y_true, y_predict_classes)\n","plt.figure(figsize=(10, 10))\n","sns.heatmap(confusion_mtx, annot=True, fmt=\".1f\")\n","plt.xlabel(\"Predicted Label\")\n","plt.ylabel(\"True Label\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Acknowledgments\n","\n","Thanks to [Görkem Günay](https://www.kaggle.com/gorkemgunay) for creating [sign-language-digits-classification-with-cnn](https://www.kaggle.com/code/gorkemgunay/sign-language-digits-classification-with-cnn). It inspires the majority of the content in this chapter."]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
