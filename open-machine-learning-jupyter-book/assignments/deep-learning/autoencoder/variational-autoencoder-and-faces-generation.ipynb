{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with Variational Autoencoders\n",
    "\n",
    "This is a starter kernel to use **Labelled Faces in the Wild (LFW) Dataset** in order to maintain knowledge about main Autoencoder principles. PyTorch will be used for modelling.\n",
    "\n",
    "## Fork it and give it an upvote.\n",
    "\n",
    "![architecture](https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/architecture.png)\n",
    "\n",
    "Useful links:\n",
    "\n",
    "* [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "* [Conditional VAE (Russian)](https://habr.com/ru/post/331664/)\n",
    "* [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908)\n",
    "* [Introducing Variational Autoencoders (in Prose and Code)](https://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html)\n",
    "* [How Autoencoders work - Understanding the math and implementation (Notebook)](https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases)\n",
    "* [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit of theory\n",
    "\n",
    "\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n",
    "\n",
    "1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n",
    "\n",
    "2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n",
    "\n",
    "3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n",
    "\n",
    "source: https://blog.keras.io/building-autoencoders-in-keras.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import base64\n",
    "import imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "import requests\n",
    "import zipfile\n",
    "from copy import deepcopy\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Training on',DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/lfw-deepfunneled.zip\"\n",
    "attributes_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/lfw_attributes.txt\"\n",
    "model_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/variational-autoencoder-and-faces-generation.pth\"\n",
    "model_vae_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/vae_model.pth\"\n",
    "notebook_path = os.getcwd()\n",
    "\n",
    "tmp_folder_path = os.path.join(notebook_path, \"tmp\")\n",
    "\n",
    "if not os.path.exists(tmp_folder_path):\n",
    "    os.makedirs(tmp_folder_path)\n",
    "\n",
    "file_path = os.path.join(tmp_folder_path,\"autoencoder\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "zip_store_path = os.path.join(file_path, \"zip-store\")\n",
    "\n",
    "if not os.path.exists(zip_store_path):\n",
    "    os.makedirs(zip_store_path)\n",
    "    \n",
    "datasets_response = requests.get(datasets_url)\n",
    "attributes_response = requests.get(attributes_url)\n",
    "model_response = requests.get(model_url)\n",
    "model_vae_response = requests.get(model_url)\n",
    "\n",
    "datasets_name = os.path.basename(datasets_url)\n",
    "attributes_name = os.path.basename(attributes_url)\n",
    "model_name = os.path.basename(model_url)\n",
    "model_vae_name = os.path.basename(model_url)\n",
    "\n",
    "datasets_save_path = os.path.join(zip_store_path, datasets_name)\n",
    "attributes_save_path = os.path.join(file_path, attributes_name)\n",
    "model_save_path = os.path.join(file_path, model_name)\n",
    "model_vae_save_path = os.path.join(file_path, model_name)\n",
    "\n",
    "with open(datasets_save_path, \"wb\") as file:\n",
    "    file.write(datasets_response.content)\n",
    "\n",
    "with open(attributes_save_path, \"wb\") as file:\n",
    "    file.write(attributes_response.content)\n",
    "\n",
    "with open(model_save_path, \"wb\") as file:\n",
    "    file.write(model_response.content)\n",
    "    \n",
    "with open(model_vae_save_path, \"wb\") as file:\n",
    "    file.write(model_vae_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = f\"./tmp/variational-autoencoder-and-faces-generation/zip-store/{datasets_name}\"\n",
    "extract_path = \"./tmp/variational-autoencoder-and-faces-generation\"\n",
    "\n",
    "zip_ref = zipfile.ZipFile(zip_file_path, 'r')\n",
    "zip_ref.extractall(extract_path)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH =\"./tmp/variational-autoencoder-and-faces-generation/lfw-deepfunneled/\"\n",
    "ATTRIBUTES_PATH = \"./tmp/variational-autoencoder-and-faces-generation/lfw_attributes.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image data is collected from DATASET_PATH and a dataset is created in which the person information for each image is extracted and used for subsequent data analysis or processing. Finally, the filter() function is used to limit the size of the dataset by retaining only information about people who appear less than 25 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for path in glob.iglob(os.path.join(DATASET_PATH, \"**\", \"*.jpg\")):\n",
    "    person = path.split(\"/\")[-2]\n",
    "    dataset.append({\"person\":person, \"path\": path})\n",
    "    \n",
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "dataset = dataset.groupby(\"person\").filter(lambda x: len(x) < 25 )\n",
    "dataset.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's group the dataset and look at the amount of photo data for the first 200 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby(\"person\").count()[:200].plot(kind='bar', figsize=(20,5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take a random look at the photos of people in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i in range(20):\n",
    "    idx = random.randint(0, len(dataset))\n",
    "    img = plt.imread(dataset.path.iloc[idx])\n",
    "    plt.subplot(4, 5, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(dataset.person.iloc[idx])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Reads the attribute data from the txt file at the specified path and stores it in a DataFrame object called df_attrs. The txt file is tab delimited, skipping the first line.\n",
    "> - Empty list photo_ids for storing photo IDs, paths and person information. Use os.walk() to iterate through the files and folders under the specified path. For each file, if the filename ends in .jpg, extract its path, person information and photo number and add them to the photo_ids list.\n",
    "> - Then convert the photo_ids list into a DataFrame object and merge it with df_attrs, based on common columns (\"person\" and \"imagenum\"). The result is stored in a DataFrame object called df.\n",
    "> - Use the assertion statement to check that the length of the merged DataFrame object is the same as the length of the original attribute data to ensure that no data is missing.\n",
    "> - Based on the paths of the photos in the merged DataFrame, the image data for each photo is read using the imageio.imread() function. Each image is then subjected to a series of processes, including cropping, resizing, etc., and the processed image data is stored in a NumPy array called all_photos.\n",
    "> - Remove the photo path, person information and photo number from the merged DataFrame and store the result in a DataFrame object called all_attrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_dataset(dx=80,dy=80, dimx=45,dimy=45):\n",
    "    \n",
    "    df_attrs = pd.read_csv(ATTRIBUTES_PATH, sep='\\t', skiprows=1,) \n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "    \n",
    "    photo_ids = []\n",
    "    for dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n",
    "        for fname in filenames:\n",
    "            if fname.endswith(\".jpg\"):\n",
    "                fpath = os.path.join(dirpath,fname)\n",
    "                photo_id = fname[:-4].replace('_',' ').split()\n",
    "                person_id = ' '.join(photo_id[:-1])\n",
    "                photo_number = int(photo_id[-1])\n",
    "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
    "    \n",
    "    all_photos = df['photo_path'].apply(imageio.imread)\\\n",
    "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
    "                                .apply(lambda img: np.array(Image.fromarray(img).resize([dimx,dimy])) )\n",
    "\n",
    "    all_photos = np.stack(all_photos.values).astype('uint8')\n",
    "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
    "    \n",
    "    return all_photos,all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, attrs = fetch_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the height and width of the image from data and define the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#45,45\n",
    "IMAGE_H = data.shape[1]\n",
    "IMAGE_W = data.shape[2]\n",
    "\n",
    "N_CHANNELS = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data and divide the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data / 255, dtype='float32')\n",
    "X_train, X_val = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_z=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(45*45*3,1500),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1500,1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, dim_z),\n",
    "            nn.BatchNorm1d(dim_z),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_z,1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(),\n",
    "            #nn.Linear(500,1000),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(1000,1500),\n",
    "            nn.BatchNorm1d(1500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1500,45*45*3)\n",
    "        )\n",
    "      \n",
    "    def encode(self,x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self,z):\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x) \n",
    "        decoded = self.decode(encoded)     \n",
    "\n",
    "        \n",
    "        return encoded, decoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will not be used in the following codes, but you can use it to instead of Autoencoder() to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=5, stride=2),\n",
    "            #nn.ReLU(),\n",
    "            #nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=2),\n",
    "            #nn.ReLU(),\n",
    "            #nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=5, stride=2)\n",
    "        )\n",
    "        \n",
    "    def decode(self,z):\n",
    "        return self.decoder(z)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,3,1,2)\n",
    "        encoded = self.encoder(x)  \n",
    "        decoded = self.decode(encoded)     \n",
    "\n",
    "        \n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auto = Autoencoder().to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - get_batch: It uses the Generator method to generate batches of a specified size by iterating over them. The amount of data generated is batch_size each time, until the entire data set is traversed.\n",
    "> - plot_gallery: This function is used to visualise an image.\n",
    "> - fit_epoch: This function is used to perform the training process of the model. The function traverses the training data set, feeds the data into the model for forward propagation, calculates losses, back propagation and parameter updates. Finally the training loss is returned.\n",
    "> - eval_epoch: This function is used to perform the validation process of the model. The function traverses the validation data set, feeds the data into the model for forward propagation and calculates the loss. Finally the validation loss is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size=64):\n",
    "    total_len = data.shape[0]\n",
    "    for i in range(0, total_len, batch_size):\n",
    "        yield data[i:min(i+batch_size,total_len)]\n",
    "\n",
    "def plot_gallery(images, h, w, n_row=3, n_col=6, with_title=False, titles=[]):\n",
    "    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n",
    "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
    "    for i in range(n_row * n_col):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        try:\n",
    "            plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n",
    "            if with_title:\n",
    "                plt.title(titles[i])\n",
    "            plt.xticks(())\n",
    "            plt.yticks(())\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "def fit_epoch(model, train_x, criterion, optimizer, batch_size, is_cnn=False):\n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs in get_batch(train_x,batch_size):\n",
    "        \n",
    "        if not is_cnn:\n",
    "            inputs = inputs.view(-1, 45*45*3)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        encoder, decoder = model(inputs)\n",
    "        \n",
    "        #print('decoder shape: ', decoder.shape)\n",
    "        \n",
    "        if not is_cnn:\n",
    "            outputs = decoder.view(-1, 45*45*3)\n",
    "        else:\n",
    "            outputs = decoder.permute(0,2,3,1)\n",
    "        \n",
    "        loss = criterion(outputs,inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.shape[0]\n",
    "        processed_data += inputs.shape[0]\n",
    "    \n",
    "    train_loss = running_loss / processed_data    \n",
    "    return train_loss\n",
    "\n",
    "def eval_epoch(model, x_val, criterion, is_cnn=False):\n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs in get_batch(x_val):\n",
    "        if not is_cnn:\n",
    "            inputs = inputs.view(-1, 45*45*3)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            encoder, decoder = model(inputs)\n",
    "            \n",
    "            if not is_cnn:\n",
    "                outputs = decoder.view(-1, 45*45*3)\n",
    "            else:\n",
    "                outputs = decoder.permute(0,2,3,1)\n",
    "                \n",
    "            loss = criterion(outputs,inputs)\n",
    "            running_loss += loss.item() * inputs.shape[0]\n",
    "            processed_data += inputs.shape[0]\n",
    "    \n",
    "    val_loss = running_loss / processed_data\n",
    "    \n",
    "    #draw\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pic = x_val[3]\n",
    "        \n",
    "        if not is_cnn:            \n",
    "            pic_input = pic.view(-1, 45*45*3)\n",
    "        else:\n",
    "            pic_input = torch.FloatTensor(pic.unsqueeze(0))\n",
    "            \n",
    "        pic_input = pic_input.to(DEVICE)        \n",
    "        encoder, decoder = model(pic_input)\n",
    "        \n",
    "        if not is_cnn:\n",
    "            pic_output = decoder.view(-1, 45*45*3).squeeze()\n",
    "        else:\n",
    "            pic_output = decoder.permute(0,2,3,1)\n",
    "            \n",
    "        pic_output = pic_output.to(\"cpu\")        \n",
    "        pic_input = pic_input.to(\"cpu\")\n",
    "        plot_gallery([pic_input, pic_output],45,45,1,2)\n",
    "    \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_x, val_x, model, epochs=10, batch_size=32, is_cnn=False):     \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)        \n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n",
    "    \n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        for epoch in range(epochs):            \n",
    "            train_loss = fit_epoch(model,train_x,criterion,optimizer,batch_size,is_cnn)\n",
    "            val_loss = eval_epoch(model,val_x,criterion, is_cnn)\n",
    "            print(\"loss: \", train_loss)\n",
    "\n",
    "            history.append((train_loss,val_loss))\n",
    "\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = train(X_train, X_val, model_auto, epochs=50, batch_size=64)\n",
    "# torch.save(history,\"variational-autoencoder-and-faces-generation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = torch.load(\"./tmp/variational-autoencoder-and-faces-generation/variational-autoencoder-and-faces-generation.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = zip(*history)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(val_loss, label='Val loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some samples from random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.random.randn(25, dim_z)\n",
    "print(z.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = torch.FloatTensor(z)    \n",
    "    inputs = inputs.to(DEVICE)\n",
    "    model_auto.eval()\n",
    "    output = model_auto.decode(inputs)\n",
    "    plot_gallery(output.data.cpu().numpy(), IMAGE_H, IMAGE_W, n_row=5, n_col=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding smile and glasses\n",
    "\n",
    "Let's find some attributes like smiles or glasses on the photo and try to add it to the photos which don't have it. We will use the second dataset for it. It contains a bunch of such attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_ids = attrs['Smiling'].sort_values(ascending=False).iloc[100:125].index.values\n",
    "smile_data = data[smile_ids]\n",
    "\n",
    "no_smile_ids = attrs['Smiling'].sort_values(ascending=True).head(25).index.values\n",
    "no_smile_data = data[no_smile_ids]\n",
    "\n",
    "eyeglasses_ids = attrs['Eyeglasses'].sort_values(ascending=False).head(25).index.values\n",
    "eyeglasses_data = data[eyeglasses_ids]\n",
    "\n",
    "sunglasses_ids = attrs['Sunglasses'].sort_values(ascending=False).head(25).index.values\n",
    "sunglasses_data = data[sunglasses_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(smile_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=smile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(no_smile_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=no_smile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(eyeglasses_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=eyeglasses_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(sunglasses_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=sunglasses_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating latent space vector for the selected images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_latent(pic):\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.FloatTensor(pic.reshape(-1, 45*45*3))\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        model_auto.eval()\n",
    "        output = model_auto.encode(inputs)        \n",
    "        return output\n",
    "\n",
    "def from_latent(vec):\n",
    "    with torch.no_grad():\n",
    "        inputs = vec.to(DEVICE)\n",
    "        model_auto.eval()\n",
    "        output = model_auto.decode(inputs)        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_latent = to_latent(smile_data).mean(axis=0)\n",
    "no_smile_latent = to_latent(no_smile_data).mean(axis=0)\n",
    "sunglasses_latent = to_latent(sunglasses_data).mean(axis=0)\n",
    "\n",
    "smile_vec = smile_latent-no_smile_latent\n",
    "sunglasses_vec = sunglasses_latent - smile_latent\n",
    "\n",
    "def make_me_smile(ids):\n",
    "    for id in ids:\n",
    "        pic = data[id:id+1]\n",
    "        latent_vec = to_latent(pic)\n",
    "        latent_vec[0] += smile_vec\n",
    "        pic_output = from_latent(latent_vec)\n",
    "        pic_output = pic_output.view(-1,45,45,3).cpu()\n",
    "        plot_gallery([pic,pic_output], IMAGE_H, IMAGE_W, n_row=1, n_col=2)\n",
    "        \n",
    "def give_me_sunglasses(ids):\n",
    "    for id in ids:\n",
    "        pic = data[id:id+1]\n",
    "        latent_vec = to_latent(pic)\n",
    "        latent_vec[0] += sunglasses_vec\n",
    "        pic_output = from_latent(latent_vec)\n",
    "        pic_output = pic_output.view(-1,45,45,3).cpu()\n",
    "        plot_gallery([pic,pic_output], IMAGE_H, IMAGE_W, n_row=1, n_col=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_me_smile(no_smile_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "give_me_sunglasses(smile_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the concept is pretty straightforward the simple autoencoder have some disadvantages. Let's explore them and try to do better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have trained our encoder to reconstruct the very same image that we've transfered to latent space. That means that when we're trying to **generate** new image from the point decoder never met we're getting _the best image it can produce_, but the quelity is not good enough. \n",
    "\n",
    "> **In other words the encoded vectors may not be continuous in the latent space.**\n",
    "\n",
    "In other hand Variational Autoencoders makes not only one encoded vector but **two**:\n",
    "- vector of means, μ;\n",
    "- vector of standard deviations, σ.\n",
    "\n",
    "![VAE_architecture.png](https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/VAE_architecture.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're going to try to use VAE to create better new data. The difference in thinking between VAE and AE is that VAE does not map images as \"numerical codes\", but as \"distributions\". \n",
    "\n",
    "It will be easier to understand with another simpler example, if we map the 'new moon' image to a normal distribution with µ=1, then it is equivalent to adding noise around 1, where not only does 1 represent the 'new moon', but the values around 1 also represent The \"new moon\" is not only represented by 1, but also by the values around 1, except that 1 is most like the \"new moon\". Mapping the \"full moon\" to a normal distribution with µ=10, all values around 10 also represent the \"full moon\". Then with code=5, it has the characteristics of both a 'new moon' and a 'full moon', so the probability of decoding is a 'half moon'. This is the idea behind VAE.\n",
    "\n",
    "![VAE_analogy.jpg](https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/variational-autoencoder-and-faces-generation/VAE_analogy.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall structure of VAE is similar to that of AE, except that the Encoder of AE outputs the code directly, whereas the Encoder of VAE outputs a number of normally distributed means (μ1, μ2...μn) and standard deviations (σ1,σ2...σn), and then from each normal distribution N(μ1,σ21),N(μ2,σ22)...N(μn,σ2n) Sampling gives the code (Z1,Z2...Zn) and the code is fed to the Decoder for decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_z = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(45*45*3, 1500)\n",
    "        self.fc21 = nn.Linear(1500, dim_z)\n",
    "        self.fc22 = nn.Linear(1500, dim_z)\n",
    "        self.fc3 = nn.Linear(dim_z, 1500)\n",
    "        self.fc4 = nn.Linear(1500, 45*45*3)        \n",
    "        self.relu = nn.LeakyReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc21(x), self.fc22(x)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 *logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = self.relu(self.fc3(z)) #1500\n",
    "        return torch.sigmoid(self.fc4(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "def loss_vae_fn(x, recon_x, mu, logvar):    \n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VAE().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch_vae(model, train_x, optimizer, batch_size, is_cnn=False):\n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    \n",
    "    for inputs in get_batch(train_x,batch_size):\n",
    "        inputs = inputs.view(-1, 45*45*3)\n",
    "        inputs = inputs.to(DEVICE)        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoded,mu,logvar, = model(inputs)\n",
    "        outputs = decoded.view(-1, 45*45*3)\n",
    "        outputs = outputs.to(DEVICE)\n",
    "        \n",
    "        loss = loss_vae_fn(inputs,outputs,mu,logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.shape[0]\n",
    "        processed_data += inputs.shape[0]\n",
    "    \n",
    "    train_loss = running_loss / processed_data    \n",
    "    return train_loss\n",
    "\n",
    "def eval_epoch_vae(model, x_val, batch_size):\n",
    "    running_loss = 0.0\n",
    "    processed_data = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for inputs in get_batch(x_val,batch_size=batch_size):\n",
    "        inputs = inputs.view(-1, 45*45*3)\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            decoded,mu,logvar = model(inputs)\n",
    "            outputs = decoded.view(-1, 45*45*3)        \n",
    "            loss = loss_vae_fn(inputs,outputs,mu,logvar)\n",
    "            running_loss += loss.item() * inputs.shape[0]\n",
    "            processed_data += inputs.shape[0]\n",
    "    \n",
    "    val_loss = running_loss / processed_data\n",
    "    \n",
    "    #draw\n",
    "    with torch.set_grad_enabled(False):\n",
    "        pic = x_val[3]         \n",
    "        pic_input = pic.view(-1, 45*45*3)            \n",
    "        pic_input = pic_input.to(DEVICE)        \n",
    "        decoded,mu,logvar = model(inputs)        \n",
    "        pic_output = decoded[0].view(-1, 45*45*3).squeeze()\n",
    "        pic_output = pic_output.to(\"cpu\") \n",
    "        pic_input = pic_input.to(\"cpu\")\n",
    "        plot_gallery([pic_input, pic_output],45,45,1,2)\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "def train_vae(train_x, val_x, model, epochs=10, batch_size=32, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)        \n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n",
    "    \n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        for epoch in range(epochs):            \n",
    "            train_loss = fit_epoch_vae(model,train_x,optimizer,batch_size)\n",
    "            val_loss = eval_epoch_vae(model,val_x,batch_size)\n",
    "            print(\"loss: \", train_loss)\n",
    "\n",
    "            history.append((train_loss,val_loss))\n",
    "\n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_vae = train_vae(X_train, X_val, model_vae, epochs=50, batch_size=128, lr=0.001)\n",
    "# torch.save(history_vae,\"vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vae = torch.load(\"./tmp/variational-autoencoder-and-faces-generation/vae_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss = zip(*history_vae)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(val_loss, label='Val loss')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders are cool. Although models in this particular notebook are simple they let us design complex generative models of data, and fit them to large datasets. They can generate images of fictional celebrity faces and high-resolution digital artwork.\n",
    "These models also yield state-of-the-art machine learning results in image generation and reinforcement learning. Variational autoencoders (VAEs) were defined in 2013 by Kingma et al. and Rezende et al."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to SERGEI AVERKIEV for creating the Kaggle open-source project [Variational Autoencoder and Faces Generation](https://www.kaggle.com/code/averkij/variational-autoencoder-and-faces-generation). It inspires the majority of the content in this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
