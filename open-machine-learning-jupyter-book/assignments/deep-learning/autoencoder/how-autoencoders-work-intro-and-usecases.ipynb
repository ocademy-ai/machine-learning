{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"90570d8e9316b2604fc5e47a72959724b049a7fb"},"source":["# How Autoencoders work - Understanding the math and implementation\n","\n","## Introduction\n","### What are Autoencoders \n","\n","Autoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n","\n","A typical autoencoder architecture comprises of three main components: \n","\n","- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n","- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n","- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n","\n","![](https://i.imgur.com/Rrmaise.png)\n","\n","A highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n","\n","- Dimentionality Reduction   \n","- Image Compression   \n","- Image Denoising   \n","- Image Generation    \n","- Feature Extraction  \n","\n","\n","\n","### How Autoencoders work \n","\n","Lets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  Consider a data repersentation space (N dimentional space which is used to repersent the data) and consider the data points repersented by two variables : x1 and x2. Data Manifold is the space inside the data repersentation space in which the true data resides. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import and generate random data"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"5f12cca7edf15c8be04043c5e97723afee974963","collapsed":true,"trusted":true},"outputs":[],"source":["from plotly.offline import init_notebook_mode, iplot\n","import plotly.graph_objs as go\n","import numpy as np\n","init_notebook_mode(connected=True)\n","\n","\n","N = 50\n","random_x = np.linspace(2, 10, N)\n","random_y1 = np.linspace(2, 10, N)\n","random_y2 = np.linspace(2, 10, N)\n","\n","trace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\", name=\"Actual Data\")\n","trace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\", name=\"Model\")\n","layout = go.Layout(title=\"2D Data Repersentation Space\", xaxis=dict(title=\"x2\", range=(0,12)), \n","                   yaxis=dict(title=\"x1\", range=(0,12)), height=400, \n","                   annotations=[dict(x=5, y=5, xref='x', yref='y', text='This 1D line is the Data Manifold (where data resides)',\n","                   showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n","                   ax=-120, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8)])\n","figure = go.Figure(data = [trace1], layout = layout)\n","iplot(figure)"]},{"cell_type":"markdown","metadata":{"_uuid":"02f52eb97e8351c3ab85ade2fa9cd25d3121ca72"},"source":["To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n","\n","- Reference Point on the line : A  \n","- Angle L with a horizontal axis  \n","\n","then any other point, say B, on line A can be repersented in terms of Distance \"d\" from A and angle L.  "]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"013677ecc0bfb7fcfc9785e91014b8b27ad48ce8","collapsed":true,"trusted":true},"outputs":[],"source":["random_y3 = [2 for i in range(100)]\n","random_y4 = random_y2 + 1\n","trace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\n","trace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\n","trace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\n","trace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\n","layout = go.Layout(xaxis=dict(title=\"x1\", range=(0,12)), yaxis=dict(title=\"x2\", range=(0,12)), height=400,\n","                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n","                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n","                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n","                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n","                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n","                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"2D Data Repersentation Space\", showlegend=False)\n","data = [trace1, trace2, trace3, trace4]\n","figure = go.Figure(data = data, layout = layout)\n","iplot(figure)\n","\n","\n","\n","#################\n","\n","random_y3 = [2 for i in range(100)]\n","random_y4 = random_y2 + 1\n","trace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\n","trace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\n","trace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\n","trace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\n","layout = go.Layout(xaxis=dict(title=\"u1\", range=(1.5,12)), yaxis=dict(title=\"u2\", range=(1.5,12)), height=400,\n","                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n","                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n","                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n","                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n","                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n","                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"Latent Distance View Space\", showlegend=False)\n","data = [trace1, trace2, trace3, trace4]\n","figure = go.Figure(data = data, layout = layout)\n","iplot(figure)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"9be8008f126f74f3e9e68356ead6c8d2f5565460"},"source":["But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule / equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n","\n","Consider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n","\n","**Step1 : Repersent the points in Latent View Space**   \n","\n","If the coordinates of point A and B in the data representation space are: \n","\n","- Point A : (x1A, x2A)  \n","- Point B : (x1B, x2B)   \n","\n","then their coordinates in the latent view space will be:   \n","\n","(x1A, x2A) ---> (0, 0)  \n","(x1B, x2B) ---> (u1B, u2B)  \n","\n","- Point A : (0, 0)  \n","- Point B : (u1B, u2B)   \n","\n","Where u1B and u2B can be represented in the form of distance between the point and the reference point  \n","\n","u1B = x1B - x1A  \n","u2B = x2B - x2A\n","\n","**Step2 : Represent the points with distance d and angle L**    \n","\n","Now, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n","\n","**=> (d, L)**     \n","**=> (d, 0)**   (after rotation)   \n","\n","This is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n","\n","**=> (d, 0) = W. (u1B, u2B)**    \n","==> (encoding)    \n","\n","where W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n","\n","**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n","==> (decoding)  \n","\n","The reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules / Learning function / encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n","\n","\n","### Different Rules for Different data\n","\n","Same rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["fs is sample rate, f is the frequency of the signal, x is the points on the x axis for plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"903b44cdeba77b14287e5fc99899626084b033b6","collapsed":true,"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt \n","import numpy as np\n","fs = 100 \n","f = 2 \n","x = np.arange(fs) \n","y = [ np.sin(2*np.pi*f * (i/fs)) for i in x]\n","\n","% matplotlib inline\n","plt.figure(figsize=(15,4))\n","plt.stem(x,y, 'r', );\n","plt.plot(x,y);"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"3a23da8f403f2637a6f032a996bbdae07d54c5db"},"source":["In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n","\n","So how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n","\n","The following image describes this property: \n","\n","![](https://i.imgur.com/gKCOdiL.png)\n","\n","Lets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n","\n","## Implementation\n","\n","### UseCase 1 : Image Reconstruction\n","\n","#### Load the required libraries\n"]},{"cell_type":"code","execution_count":9,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m EarlyStopping\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"]}],"source":["from keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import EarlyStopping\n","from keras.utils import to_categorical\n","from numpy import argmax, array_equal\n","import matplotlib.pyplot as plt\n","from keras.models import Model\n","from imgaug import augmenters\n","from random import randint\n","import pandas as pd\n","import numpy as np\n","import os\n","import requests"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"59096882cae137d26c5a39189860907f20c43390"},"source":["#### Dataset Prepration\n","\n","Load the dataset, separate predictors and target, normalize the inputs."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/how-autoencoders-work-intro-and-usecases/fashion-mnist_train.csv.zip\"\n","test_url = \"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/autoencoder/how-autoencoders-work-intro-and-usecases/fashion-mnist_test.csv.zip\"\n","\n","notebook_path = os.getcwd()\n","\n","tmp_folder_path = os.path.join(notebook_path, \"tmp\")\n","\n","if not os.path.exists(tmp_folder_path):\n","    os.makedirs(tmp_folder_path)\n","    \n","tmp_zip_path = os.path.join(tmp_folder_path,\"zip-store\")\n","\n","if not os.path.exists(tmp_zip_path):\n","    os.makedirs(tmp_zip_path)\n","    \n","train_response = requests.get(train_url)\n","test_response = requests.get(test_url)\n","\n","train_name = os.path.basename(train_url)\n","test_name = os.path.basename(test_url)\n","\n","train_save_path = os.path.join(tmp_zip_path, train_name)\n","test_save_path = os.path.join(tmp_zip_path, test_name)\n","\n","with open(train_save_path, \"wb\") as file:\n","    file.write(train_response.content)\n","\n","with open(test_save_path, \"wb\") as file:\n","    file.write(test_response.content)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import zipfile\n","\n","zip_train_path = f\"./tmp/zip-store/{train_name}\"\n","zip_test_path = f\"./tmp/zip-store/{test_name}\"\n","extract_path = \"./tmp/\"\n","\n","zip_train_ref = zipfile.ZipFile(zip_train_path, 'r')\n","zip_train_ref.extractall(extract_path)\n","zip_train_ref.close()\n","\n","zip_test_ref = zipfile.ZipFile(zip_test_path, 'r')\n","zip_test_ref.extractall(extract_path)\n","zip_test_ref.close()"]},{"cell_type":"code","execution_count":8,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./tmp/fashion-mnist_train.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_x \u001b[39m=\u001b[39m train[\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mcolumns)[\u001b[39m1\u001b[39m:]]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m train_y \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["train = pd.read_csv(\"./tmp/fashion-mnist_train.csv\")\n","train_x = train[list(train.columns)[1:]].values\n","train_y = train['label'].values\n","\n"," \n","train_x = train_x / 255\n","\n","\n","train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n","\n","\n","train_x = train_x.reshape(-1, 784)\n","val_x = val_x.reshape(-1, 784)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"40518d4112c844d4d85c384bdc23bdd778051575"},"source":["#### Create Autoencoder architecture\n","\n","In this section, lets create an autoencoder architecture. The encoding part comprises of three layers with 2000, 1200, and 500 nodes. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500, 1200, and 2000 nodes. The final layer comprises of exact number of nodes as the input layer."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d0bcdde23f8ffd7dc8ee86c55b0554c6ca2a6ede","trusted":true},"outputs":[],"source":["input_layer = Input(shape=(784,))\n","\n","encode_layer1 = Dense(1500, activation='relu')(input_layer)\n","encode_layer2 = Dense(1000, activation='relu')(encode_layer1)\n","encode_layer3 = Dense(500, activation='relu')(encode_layer2)\n","\n","latent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n","\n","decode_layer1 = Dense(500, activation='relu')(latent_view)\n","decode_layer2 = Dense(1000, activation='relu')(decode_layer1)\n","decode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n","\n","output_layer  = Dense(784)(decode_layer3)\n","\n","model = Model(input_layer, output_layer)"]},{"cell_type":"markdown","metadata":{"_uuid":"a9e2102050adb5180a523354290d3202100ec363"},"source":["Here is the summary of our autoencoder architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"9275352fb6d82d47ea3526ce496cd99a2b1de71a","trusted":true},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"8dadb3a777626145000b6f2f4c371e08f1662c3b"},"source":["Next, we will train the model with early stopping callback."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"918290ec9683d33c7e4c2eb2108c36aaa1a4e39c","trusted":true},"outputs":[],"source":["model.compile(optimizer='adam', loss='mse')\n","early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n","model.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])"]},{"cell_type":"markdown","metadata":{"_uuid":"83408009eab07c833b9e1dda03af5f89e373a62f"},"source":["Generate the predictions on validation data. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"7c3502c132ee4b9d302cfdad03269e3fd8d28faf","trusted":true},"outputs":[],"source":["preds = model.predict(val_x)"]},{"cell_type":"markdown","metadata":{"_uuid":"0424c3abb39dc0b2bcc61bf671ea4dd3e729421c"},"source":["Lets plot the original and predicted image\n","\n","**Inputs: Actual Images**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"4a04a2c0d3c158745ee9a0e3c0459b6aae590229","trusted":true},"outputs":[],"source":["from PIL import Image \n","f, ax = plt.subplots(1,5)\n","f.set_size_inches(80, 40)\n","for i in range(5):\n","    ax[i].imshow(val_x[i].reshape(28, 28))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"71bd4ac665e27c62c2b0a9b2411d09e0e8204d7e"},"source":["**Predicted : Autoencoder Output**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"d4acfae56b53be5f8f28828066a02a0ba4b3632c","trusted":true},"outputs":[],"source":["f, ax = plt.subplots(1,5)\n","f.set_size_inches(80, 40)\n","for i in range(5):\n","    ax[i].imshow(preds[i].reshape(28, 28))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"30c028a0ad5b8b8aa18be73ec7a12621803f4b48","trusted":true},"source":["So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n","\n","### UseCase 2 - Image Denoising\n","\n","Autoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n","\n","![](https://www.learnopencv.com/wp-content/uploads/2017/11/denoising-autoencoder-600x299.jpg)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's recreate the train_x array and val_x array, normalize and reshape them ."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"970b155eea8555ee5dc1d8d8ca8797a32ef86926","trusted":true},"outputs":[],"source":["train_x = train[list(train.columns)[1:]].values\n","train_x, val_x = train_test_split(train_x, test_size=0.2)\n","\n","train_x = train_x/255.\n","val_x = val_x/255."]},{"cell_type":"markdown","metadata":{"_uuid":"367c58c48e7fd4ed34e57ec7d2f8add1fc07e090"},"source":["In this autoencoder network, we will add convolutional layers because convolutional networks works really well with the image inputs. To apply convolutions on image data, we will reshape our inputs in the form of 28 * 28 matrix. For more information related to CNN,  refer to my previous [kernel](https://www.kaggle.com/shivamb/a-very-comprehensive-tutorial-nn-cnn).  "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5b5ee148e0d59c190ccc5dda2f30e14a21f4a4bf","trusted":true},"outputs":[],"source":["train_x = train_x.reshape(-1, 28, 28, 1)\n","val_x = val_x.reshape(-1, 28, 28, 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"e42345b98f022ab2dd5bc389f52697d6fbb1d925"},"source":["#### Noisy Images \n","\n","We can intentionally introduce the noise in an image. I am using imaug package which can be used to augment the images with different variations. One such variation can be introduction of noise. Different types of noises can be added to the images. For example: \n","\n","- Salt and Pepper Noise  \n","- Gaussian Noise  \n","- Periodic Noise  \n","- Speckle Noise  \n","\n","Lets introduce salt and pepper noise to our data which is also known as impulse noise. This noise introduces sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels. \n","\n","Thanks to @ColinMorris for suggesting the correction in salt and pepper noise. Let's add sample noise - Salt and Pepper"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"db4006f8bbf597d993162a35b116aeaed01b703c","trusted":true},"outputs":[],"source":["noise = augmenters.SaltAndPepper(0.1)\n","seq_object = augmenters.Sequential([noise])\n","\n","train_x_n = seq_object.augment_images(train_x * 255) / 255\n","val_x_n = seq_object.augment_images(val_x * 255) / 255"]},{"cell_type":"markdown","metadata":{"_uuid":"42ff022216bce713240ae039bbac9bd0ec4a0ed5"},"source":["Before adding noise"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"da6a5e4c0b6be40d10dc05c48dbe9574553579de","collapsed":true,"trusted":true},"outputs":[],"source":["f, ax = plt.subplots(1,5)\n","f.set_size_inches(80, 40)\n","for i in range(5,10):\n","    ax[i-5].imshow(train_x[i].reshape(28, 28))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"c919f79e7f48c09e5679447c908bff72f82bd57b"},"source":["After adding noise"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_uuid":"3b83f1d4ecd0cec34d4acc65ea850e05c3eb3e09","collapsed":true,"trusted":true},"outputs":[],"source":["f, ax = plt.subplots(1,5)\n","f.set_size_inches(80, 40)\n","for i in range(5,10):\n","    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"123730780bb7e1a1c38bb184173ac7744cce0921"},"source":["Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n","\n","**Encoding Architecture:**   \n","\n","The encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as \"same\". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n","\n","**Decoding Architecture:**   \n","\n","Similarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution / dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a67b7fd1b9751d543801df27db6c1221f8f8a38f","collapsed":true,"trusted":true},"outputs":[],"source":["input_layer = Input(shape=(28, 28, 1))\n","\n","encoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n","encoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\n","encoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\n","encoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\n","encoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\n","latent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n","\n","decoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\n","decoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\n","decoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\n","decoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\n","decoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\n","decoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\n","output_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n","\n","model_2 = Model(input_layer, output_layer)\n","model_2.compile(optimizer='adam', loss='mse')"]},{"cell_type":"markdown","metadata":{"_uuid":"c945e74a7b0753008aeadf1577e354560f0e48f5"},"source":["Here is the model summary"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_uuid":"1d2a5b425fd14017e5510b696566e17516a7108a","collapsed":true,"trusted":true},"outputs":[],"source":["model_2.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"df3ef4d6d93b8ecb712f43e498cf8ddf9f786401"},"source":["Train the model with early stopping callback. Increase the number of epochs to a higher number for better results. "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e1e5c7075c83bc2b9df348a20f8c35e3ddbb25b1","collapsed":true,"trusted":true},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\n","history = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])"]},{"cell_type":"markdown","metadata":{"_uuid":"263b0c562f96b94272d37658ac4d454eae3b019e"},"source":["Lets obtain the predictions of the model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4d6af9e047aebe116246539367172567705dabae","collapsed":true,"trusted":true},"outputs":[],"source":["preds = model_2.predict(val_x_n[:10])\n","f, ax = plt.subplots(1,5)\n","f.set_size_inches(80, 40)\n","for i in range(5,10):\n","    ax[i-5].imshow(preds[i].reshape(28, 28))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"d6496e25f1b8235fd9d65d123445a841ef002396"},"source":["In this implementation, I have not traiened this network for longer epoochs, but for better predictions, you can train the network for larger number of epoochs say somewhere in the range of 500 - 1000. \n","\n","### UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n","\n","\n","Next use case is sequence to sequence prediction. In the previous example we input an image which was a basicaly a 2 dimentional data, in this example we will input a sequence data as the input which will be 1 dimentional. Example of sequence data are time series data and text data. This usecase can be applied in machine translation. Unlike CNNs in image example, in this use-case we will use LSTMs. \n","\n","Most of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him. \n","- Reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n","\n","#### Autoencoder Architecture  \n","\n","The architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence, called the decoder. First lets understand the internal working of LSTMs which will be used in this architecture. \n","\n","- The Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.   \n","- Unlike other recurrent neural networks, the network’s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.   \n","- We can define the number of LSTM memory units in the LSTM layer, Each unit or cell within the layer has an internal memory / cell state, often abbreviated as “c“, and outputs a hidden state, often abbreviated as “h“.   \n","- By using Keras, we can access both output states of the LSTM layer as well as the current states of the LSTM layers.  \n","\n","Lets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. There are two components: \n","\n","- An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output  \n","- A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence\n","- We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. \n","\n","Lets first of all, generate a sequence dataset containing random sequences of fixed lengths. We will create a function to generate random sequences. \n","\n","- X1 repersents the input sequence containing random numbers  \n","- X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence  \n","- y repersents the target sequence or the actual sequence \n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"beaafc0178a0d6f334883b5422b3e67f174fc70f","trusted":true},"outputs":[],"source":["def dataset_preparation(n_in, n_out, n_unique, n_samples):\n","    X1, X2, y = [], [], []\n","    for _ in range(n_samples):\n","        # create random numbers sequence - input \n","        inp_seq = [randint(1, n_unique-1) for _ in range(n_in)]\n","        \n","        # create target sequence\n","        target = inp_seq[:n_out]\n","    \n","        # create padded sequence / seed sequence \n","        target_seq = list(reversed(target))\n","        seed_seq = [0] + target_seq[:-1]  \n","        \n","        # convert the elements to categorical using keras api\n","        X1.append(to_categorical([inp_seq], num_classes=n_unique))\n","        X2.append(to_categorical([seed_seq], num_classes=n_unique))\n","        y.append(to_categorical([target_seq], num_classes=n_unique))\n","    \n","    # remove unnecessary dimention\n","    X1 = np.squeeze(np.array(X1), axis=1) \n","    X2 = np.squeeze(np.array(X2), axis=1) \n","    y  = np.squeeze(np.array(y), axis=1) \n","    return X1, X2, y\n","\n","samples = 100000\n","features = 51\n","inp_size = 6\n","out_size = 3\n","\n","inputs, seeds, outputs = dataset_preparation(inp_size, out_size, features, samples)\n","print(\"Shapes: \", inputs.shape, seeds.shape, outputs.shape)\n","print (\"Here is first categorically encoded input sequence looks like: \", )\n","inputs[0][0]"]},{"attachments":{},"cell_type":"markdown","metadata":{"_uuid":"6b75bdd301f99080c630be22afd1bbe2b646729d"},"source":["Next, lets create the architecture of our model in Keras. \n","\n","First define the encoder architecture, input is sequence and output is encoder states.\n","\n","Then define the encoder-decoder architecture, input is a seed sequence and output is decoder states, decoded output.\n","\n","Finally define the decoder model, input is current states + encoded sequence, output is decoded sequence."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c7bad1ef7d5491dd5ab16c2e557c62056a18955a","trusted":true},"outputs":[],"source":["def define_models(n_input, n_output):\n","\n","    encoder_inputs = Input(shape=(None, n_input))\n","    encoder = LSTM(128, return_state=True)\n","    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","    encoder_states = [state_h, state_c]\n","\n","    decoder_inputs = Input(shape=(None, n_output))\n","    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n","    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","    decoder_dense = Dense(n_output, activation='softmax')\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","    \n","    encoder_model = Model(encoder_inputs, encoder_states)\n","    decoder_state_input_h = Input(shape=(128,))\n","    decoder_state_input_c = Input(shape=(128,))\n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","    return model, encoder_model, decoder_model\n","\n","autoencoder, encoder_model, decoder_model = define_models(features, features)"]},{"cell_type":"markdown","metadata":{"_uuid":"6ce01e89f66c1f02486bb052786f2660902dc64f"},"source":["Lets look at the model summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"33b154e46afba9fb54fdeca1745dc89a388782d1","trusted":true},"outputs":[],"source":["encoder_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3bcac5ea1a5085bd93d4320596aa671915522496","trusted":true},"outputs":[],"source":["decoder_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"729eb6757c0152cf070fddd5e51ec626c361f0f7","trusted":true},"outputs":[],"source":["autoencoder.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"59ca6693b0ca22b36458a9de79120a868a1fb91d"},"source":["Now, lets train the autoencoder model using Adam optimizer and Categorical Cross Entropy loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6c30b736055303c7bd07c083cfacdc86c74c880b","trusted":true},"outputs":[],"source":["autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n","autoencoder.fit([inputs, seeds], outputs, epochs=1)"]},{"cell_type":"markdown","metadata":{"_uuid":"edca43647c99f20de5de05b5973b307a95ec726f"},"source":["Lets write a function to predict the sequence based on input sequence "]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b480d7362901fff79c1b4c356dcd88934828fc6a","trusted":true},"outputs":[],"source":["def reverse_onehot(encoded_seq):\n","    return [argmax(vector) for vector in encoded_seq]\n","\n","def predict_sequence(encoder, decoder, sequence):\n","    output = []\n","    target_seq = np.array([0.0 for _ in range(features)])\n","    target_seq = target_seq.reshape(1, 1, features)\n","\n","    current_state = encoder.predict(sequence)\n","    for t in range(out_size):\n","        pred, h, c = decoder.predict([target_seq] + current_state)\n","        output.append(pred[0, 0, :])\n","        current_state = [h, c]\n","        target_seq = pred\n","    return np.array(output)"]},{"cell_type":"markdown","metadata":{"_uuid":"79f059adb62e16ac9bf566825057493a6cf5bdf9"},"source":["Generate some predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"345c75e2df0839e3b7ec35455937d1a62926c179","collapsed":true,"trusted":true},"outputs":[],"source":["for k in range(5):\n","    X1, X2, y = dataset_preparation(inp_size, out_size, features, 1)\n","    target = predict_sequence(encoder_model, decoder_model, X1)\n","    print('\\nInput Sequence=%s SeedSequence=%s, PredictedSequence=%s' \n","          % (reverse_onehot(X1[0]), reverse_onehot(y[0]), reverse_onehot(target)))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Acknowledgments\n","\n","Thanks to Shivam Bansal for creating the Kaggle open-source project [How Autoencoders Work: Intro and UseCases](https://www.kaggle.com/code/shivamb/how-autoencoders-work-intro-and-usecases). It inspires the majority of the content in this chapter."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":1}
