{"cells":[{"metadata":{"_uuid":"90570d8e9316b2604fc5e47a72959724b049a7fb"},"cell_type":"markdown","source":"## How Autoencoders work - Understanding the math and implementation\n\n### Contents \n\n<ul>\n<li>1. Introduction</li>\n<ul>\n    <li>1.1 What are Autoencoders ? </li>\n    <li>1.2 How Autoencoders Work ? </li>\n</ul>\n<li>2. Implementation and UseCases</li>\n<ul>\n    <li>2.1 UseCase 1: Image Reconstruction </li>\n    <li>2.2 UseCase 2: Noise Removal </li>\n    <li>2.3 UseCase 3: Sequence to Sequence Prediction </li>\n</ul>\n</ul>\n\n<br>\n\n## 1. Introduction\n## 1.1 What are Autoencoders \n\nAutoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n\nA typical autoencoder architecture comprises of three main components: \n\n- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n\n![](https://i.imgur.com/Rrmaise.png)\n\nA highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n\n- Dimentionality Reduction   \n- Image Compression   \n- Image Denoising   \n- Image Generation    \n- Feature Extraction  \n\n\n\n## 1.2 How Autoencoders work \n\nLets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  Consider a data repersentation space (N dimentional space which is used to repersent the data) and consider the data points repersented by two variables : x1 and x2. Data Manifold is the space inside the data repersentation space in which the true data resides. "},{"metadata":{"trusted":true,"_uuid":"5f12cca7edf15c8be04043c5e97723afee974963","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport numpy as np\ninit_notebook_mode(connected=True)\n\n## generate random data\nN = 50\nrandom_x = np.linspace(2, 10, N)\nrandom_y1 = np.linspace(2, 10, N)\nrandom_y2 = np.linspace(2, 10, N)\n\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\", name=\"Actual Data\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\", name=\"Model\")\nlayout = go.Layout(title=\"2D Data Repersentation Space\", xaxis=dict(title=\"x2\", range=(0,12)), \n                   yaxis=dict(title=\"x1\", range=(0,12)), height=400, \n                   annotations=[dict(x=5, y=5, xref='x', yref='y', text='This 1D line is the Data Manifold (where data resides)',\n                   showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                   ax=-120, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8)])\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02f52eb97e8351c3ab85ade2fa9cd25d3121ca72"},"cell_type":"markdown","source":"To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n\n- Reference Point on the line : A  \n- Angle L with a horizontal axis  \n\nthen any other point, say B, on line A can be repersented in terms of Distance \"d\" from A and angle L.  "},{"metadata":{"trusted":true,"_uuid":"013677ecc0bfb7fcfc9785e91014b8b27ad48ce8","_kg_hide-input":true,"collapsed":true},"cell_type":"code","source":"random_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"x1\", range=(0,12)), yaxis=dict(title=\"x2\", range=(0,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"2D Data Repersentation Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)\n\n\n\n#################\n\nrandom_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"u1\", range=(1.5,12)), yaxis=dict(title=\"u2\", range=(1.5,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"Latent Distance View Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9be8008f126f74f3e9e68356ead6c8d2f5565460"},"cell_type":"markdown","source":"But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule / equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n\nConsider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n\n![](https://i.imgur.com/lfq4eEy.png)\n\n<br>\n**Step1 : Repersent the points in Latent View Space**   \n\nIf the coordinates of point A and B in the data representation space are: \n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\nthen their coordinates in the latent view space will be:   \n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\nWhere u1B and u2B can be represented in the form of distance between the point and the reference point  \n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\nNow, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\nThis is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\nwhere W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\nThe reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules / Learning function / encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n\n\n## Different Rules for Different data\n\nSame rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"903b44cdeba77b14287e5fc99899626084b033b6","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport numpy as np\nfs = 100 # sample rate \nf = 2 # the frequency of the signal\nx = np.arange(fs) # the points on the x axis for plotting\ny = [ np.sin(2*np.pi*f * (i/fs)) for i in x]\n\n% matplotlib inline\nplt.figure(figsize=(15,4))\nplt.stem(x,y, 'r', );\nplt.plot(x,y);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3a23da8f403f2637a6f032a996bbdae07d54c5db"},"cell_type":"markdown","source":"In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n\nSo how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n\nThe following image describes this property: \n\n![](https://i.imgur.com/gKCOdiL.png)\n\nLets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n\n## 2. Implementation\n\n## 2.1 UseCase 1 : Image Reconstruction\n\n1. Load the required libraries\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## load the libraries \nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters\nfrom random import randint\nimport pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59096882cae137d26c5a39189860907f20c43390"},"cell_type":"markdown","source":"### 2. Dataset Prepration \n\nLoad the dataset, separate predictors and target, normalize the inputs."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"### read dataset \ntrain = pd.read_csv(\"../input/fashion-mnist_train.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\n\n## normalize and reshape the predictors  \ntrain_x = train_x / 255\n\n## create train and validation datasets\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 784)\nval_x = val_x.reshape(-1, 784)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40518d4112c844d4d85c384bdc23bdd778051575"},"cell_type":"markdown","source":"### 3. Create Autoencoder architecture\n\nIn this section, lets create an autoencoder architecture. The encoding part comprises of three layers with 2000, 1200, and 500 nodes. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500, 1200, and 2000 nodes. The final layer comprises of exact number of nodes as the input layer."},{"metadata":{"trusted":true,"_uuid":"d0bcdde23f8ffd7dc8ee86c55b0554c6ca2a6ede"},"cell_type":"code","source":"## input layer\ninput_layer = Input(shape=(784,))\n\n## encoding architecture\nencode_layer1 = Dense(1500, activation='relu')(input_layer)\nencode_layer2 = Dense(1000, activation='relu')(encode_layer1)\nencode_layer3 = Dense(500, activation='relu')(encode_layer2)\n\n## latent view\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n## decoding architecture\ndecode_layer1 = Dense(500, activation='relu')(latent_view)\ndecode_layer2 = Dense(1000, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n\n## output layer\noutput_layer  = Dense(784)(decode_layer3)\n\nmodel = Model(input_layer, output_layer)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9e2102050adb5180a523354290d3202100ec363"},"cell_type":"markdown","source":"Here is the summary of our autoencoder architecture."},{"metadata":{"trusted":true,"_uuid":"9275352fb6d82d47ea3526ce496cd99a2b1de71a"},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dadb3a777626145000b6f2f4c371e08f1662c3b"},"cell_type":"markdown","source":"Next, we will train the model with early stopping callback."},{"metadata":{"trusted":true,"_uuid":"918290ec9683d33c7e4c2eb2108c36aaa1a4e39c"},"cell_type":"code","source":"model.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nmodel.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83408009eab07c833b9e1dda03af5f89e373a62f"},"cell_type":"markdown","source":"Generate the predictions on validation data. "},{"metadata":{"trusted":true,"_uuid":"7c3502c132ee4b9d302cfdad03269e3fd8d28faf"},"cell_type":"code","source":"preds = model.predict(val_x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0424c3abb39dc0b2bcc61bf671ea4dd3e729421c"},"cell_type":"markdown","source":"Lets plot the original and predicted image\n\n**Inputs: Actual Images**"},{"metadata":{"trusted":true,"_uuid":"4a04a2c0d3c158745ee9a0e3c0459b6aae590229","_kg_hide-input":true},"cell_type":"code","source":"from PIL import Image \nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x[i].reshape(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71bd4ac665e27c62c2b0a9b2411d09e0e8204d7e"},"cell_type":"markdown","source":"**Predicted : Autoencoder Output**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d4acfae56b53be5f8f28828066a02a0ba4b3632c"},"cell_type":"code","source":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30c028a0ad5b8b8aa18be73ec7a12621803f4b48"},"cell_type":"markdown","source":"So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n\n## 2.2 UseCase 2 - Image Denoising\n\nAutoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n\n![](https://www.learnopencv.com/wp-content/uploads/2017/11/denoising-autoencoder-600x299.jpg)"},{"metadata":{"trusted":true,"_uuid":"970b155eea8555ee5dc1d8d8ca8797a32ef86926"},"cell_type":"code","source":"## recreate the train_x array and val_x array\ntrain_x = train[list(train.columns)[1:]].values\ntrain_x, val_x = train_test_split(train_x, test_size=0.2)\n\n## normalize and reshape\ntrain_x = train_x/255.\nval_x = val_x/255.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"367c58c48e7fd4ed34e57ec7d2f8add1fc07e090"},"cell_type":"markdown","source":"In this autoencoder network, we will add convolutional layers because convolutional networks works really well with the image inputs. To apply convolutions on image data, we will reshape our inputs in the form of 28 * 28 matrix. For more information related to CNN,  refer to my previous [kernel](https://www.kaggle.com/shivamb/a-very-comprehensive-tutorial-nn-cnn).  "},{"metadata":{"trusted":true,"_uuid":"5b5ee148e0d59c190ccc5dda2f30e14a21f4a4bf"},"cell_type":"code","source":"train_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e42345b98f022ab2dd5bc389f52697d6fbb1d925"},"cell_type":"markdown","source":"### Noisy Images \n\nWe can intentionally introduce the noise in an image. I am using imaug package which can be used to augment the images with different variations. One such variation can be introduction of noise. Different types of noises can be added to the images. For example: \n\n- Salt and Pepper Noise  \n- Gaussian Noise  \n- Periodic Noise  \n- Speckle Noise  \n\nLets introduce salt and pepper noise to our data which is also known as impulse noise. This noise introduces sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels. \n\nThanks to @ColinMorris for suggesting the correction in salt and pepper noise."},{"metadata":{"trusted":true,"_uuid":"db4006f8bbf597d993162a35b116aeaed01b703c"},"cell_type":"code","source":"# Lets add sample noise - Salt and Pepper\nnoise = augmenters.SaltAndPepper(0.1)\nseq_object = augmenters.Sequential([noise])\n\ntrain_x_n = seq_object.augment_images(train_x * 255) / 255\nval_x_n = seq_object.augment_images(val_x * 255) / 255","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42ff022216bce713240ae039bbac9bd0ec4a0ed5"},"cell_type":"markdown","source":"Before adding noise"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"da6a5e4c0b6be40d10dc05c48dbe9574553579de","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x[i].reshape(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c919f79e7f48c09e5679447c908bff72f82bd57b"},"cell_type":"markdown","source":"After adding noise"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3b83f1d4ecd0cec34d4acc65ea850e05c3eb3e09","collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"123730780bb7e1a1c38bb184173ac7744cce0921"},"cell_type":"markdown","source":"Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n\n**Encoding Architecture:**   \n\nThe encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as \"same\". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n\n**Decoding Architecture:**   \n\nSimilarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution / dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n\n"},{"metadata":{"trusted":true,"_uuid":"a67b7fd1b9751d543801df27db6c1221f8f8a38f","collapsed":true},"cell_type":"code","source":"# input layer\ninput_layer = Input(shape=(28, 28, 1))\n\n# encoding architecture\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nlatent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# decoding architecture\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# compile the model\nmodel_2 = Model(input_layer, output_layer)\nmodel_2.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c945e74a7b0753008aeadf1577e354560f0e48f5"},"cell_type":"markdown","source":"Here is the model summary"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"1d2a5b425fd14017e5510b696566e17516a7108a","collapsed":true},"cell_type":"code","source":"model_2.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df3ef4d6d93b8ecb712f43e498cf8ddf9f786401"},"cell_type":"markdown","source":"Train the model with early stopping callback. Increase the number of epochs to a higher number for better results. "},{"metadata":{"trusted":true,"_uuid":"e1e5c7075c83bc2b9df348a20f8c35e3ddbb25b1","collapsed":true},"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\nhistory = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"263b0c562f96b94272d37658ac4d454eae3b019e"},"cell_type":"markdown","source":"Lets obtain the predictions of the model"},{"metadata":{"trusted":true,"_uuid":"4d6af9e047aebe116246539367172567705dabae","collapsed":true},"cell_type":"code","source":"preds = model_2.predict(val_x_n[:10])\nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(preds[i].reshape(28, 28))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6496e25f1b8235fd9d65d123445a841ef002396"},"cell_type":"markdown","source":"In this implementation, I have not traiened this network for longer epoochs, but for better predictions, you can train the network for larger number of epoochs say somewhere in the range of 500 - 1000. \n\n## 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n\n\nNext use case is sequence to sequence prediction. In the previous example we input an image which was a basicaly a 2 dimentional data, in this example we will input a sequence data as the input which will be 1 dimentional. Example of sequence data are time series data and text data. This usecase can be applied in machine translation. Unlike CNNs in image example, in this use-case we will use LSTMs. \n\nMost of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him. \n- Reference : https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n\n#### Autoencoder Architecture  \n\nThe architecuture of this use case will contain an encoder to encode the source sequence and second to decode the encoded source sequence into the target sequence, called the decoder. First lets understand the internal working of LSTMs which will be used in this architecture. \n\n- The Long Short-Term Memory, or LSTM, is a recurrent neural network that is comprised of internal gates.   \n- Unlike other recurrent neural networks, the network’s internal gates allow the model to be trained successfully using backpropagation through time, or BPTT, and avoid the vanishing gradients problem.   \n- We can define the number of LSTM memory units in the LSTM layer, Each unit or cell within the layer has an internal memory / cell state, often abbreviated as “c“, and outputs a hidden state, often abbreviated as “h“.   \n- By using Keras, we can access both output states of the LSTM layer as well as the current states of the LSTM layers.  \n\nLets now create an autoencoder architecutre for learning and producing sequences made up of LSTM layers. There are two components: \n\n- An encoder architecture which takes a sequence as input and returns the current state of LSTM as the output  \n- A decoder architecture which takes the sequence and encoder LSTM states as input and returns the decoded output sequence\n- We are saving and accessing hidden and memory states of LSTM so that we can use them while generating predictions on unseen data. \n\nLets first of all, generate a sequence dataset containing random sequences of fixed lengths. We will create a function to generate random sequences. \n\n- X1 repersents the input sequence containing random numbers  \n- X2 repersents the padded sequence which is used as the seed to reproduce the other elements of the sequence  \n- y repersents the target sequence or the actual sequence \n"},{"metadata":{"trusted":true,"_uuid":"beaafc0178a0d6f334883b5422b3e67f174fc70f"},"cell_type":"code","source":"def dataset_preparation(n_in, n_out, n_unique, n_samples):\n    X1, X2, y = [], [], []\n    for _ in range(n_samples):\n        ## create random numbers sequence - input \n        inp_seq = [randint(1, n_unique-1) for _ in range(n_in)]\n        \n        ## create target sequence\n        target = inp_seq[:n_out]\n    \n        ## create padded sequence / seed sequence \n        target_seq = list(reversed(target))\n        seed_seq = [0] + target_seq[:-1]  \n        \n        # convert the elements to categorical using keras api\n        X1.append(to_categorical([inp_seq], num_classes=n_unique))\n        X2.append(to_categorical([seed_seq], num_classes=n_unique))\n        y.append(to_categorical([target_seq], num_classes=n_unique))\n    \n    # remove unnecessary dimention\n    X1 = np.squeeze(np.array(X1), axis=1) \n    X2 = np.squeeze(np.array(X2), axis=1) \n    y  = np.squeeze(np.array(y), axis=1) \n    return X1, X2, y\n\nsamples = 100000\nfeatures = 51\ninp_size = 6\nout_size = 3\n\ninputs, seeds, outputs = dataset_preparation(inp_size, out_size, features, samples)\nprint(\"Shapes: \", inputs.shape, seeds.shape, outputs.shape)\nprint (\"Here is first categorically encoded input sequence looks like: \", )\ninputs[0][0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b75bdd301f99080c630be22afd1bbe2b646729d"},"cell_type":"markdown","source":"Next, lets create the architecture of our model in Keras. "},{"metadata":{"trusted":true,"_uuid":"c7bad1ef7d5491dd5ab16c2e557c62056a18955a"},"cell_type":"code","source":"def define_models(n_input, n_output):\n    ## define the encoder architecture \n    ## input : sequence \n    ## output : encoder states \n    encoder_inputs = Input(shape=(None, n_input))\n    encoder = LSTM(128, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_states = [state_h, state_c]\n\n    ## define the encoder-decoder architecture \n    ## input : a seed sequence \n    ## output : decoder states, decoded output \n    decoder_inputs = Input(shape=(None, n_output))\n    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n    decoder_dense = Dense(n_output, activation='softmax')\n    decoder_outputs = decoder_dense(decoder_outputs)\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    \n    ## define the decoder model\n    ## input : current states + encoded sequence\n    ## output : decoded sequence\n    encoder_model = Model(encoder_inputs, encoder_states)\n    decoder_state_input_h = Input(shape=(128,))\n    decoder_state_input_c = Input(shape=(128,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n\n    return model, encoder_model, decoder_model\n\nautoencoder, encoder_model, decoder_model = define_models(features, features)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ce01e89f66c1f02486bb052786f2660902dc64f"},"cell_type":"markdown","source":"Lets look at the model summaries"},{"metadata":{"trusted":true,"_uuid":"33b154e46afba9fb54fdeca1745dc89a388782d1"},"cell_type":"code","source":"encoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3bcac5ea1a5085bd93d4320596aa671915522496"},"cell_type":"code","source":"decoder_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"729eb6757c0152cf070fddd5e51ec626c361f0f7"},"cell_type":"code","source":"autoencoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59ca6693b0ca22b36458a9de79120a868a1fb91d"},"cell_type":"markdown","source":"Now, lets train the autoencoder model using Adam optimizer and Categorical Cross Entropy loss function"},{"metadata":{"trusted":true,"_uuid":"6c30b736055303c7bd07c083cfacdc86c74c880b"},"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nautoencoder.fit([inputs, seeds], outputs, epochs=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edca43647c99f20de5de05b5973b307a95ec726f"},"cell_type":"markdown","source":"Lets write a function to predict the sequence based on input sequence "},{"metadata":{"trusted":true,"_uuid":"b480d7362901fff79c1b4c356dcd88934828fc6a"},"cell_type":"code","source":"def reverse_onehot(encoded_seq):\n    return [argmax(vector) for vector in encoded_seq]\n\ndef predict_sequence(encoder, decoder, sequence):\n    output = []\n    target_seq = np.array([0.0 for _ in range(features)])\n    target_seq = target_seq.reshape(1, 1, features)\n\n    current_state = encoder.predict(sequence)\n    for t in range(out_size):\n        pred, h, c = decoder.predict([target_seq] + current_state)\n        output.append(pred[0, 0, :])\n        current_state = [h, c]\n        target_seq = pred\n    return np.array(output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79f059adb62e16ac9bf566825057493a6cf5bdf9"},"cell_type":"markdown","source":"Generate some predictions"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"345c75e2df0839e3b7ec35455937d1a62926c179"},"cell_type":"code","source":"for k in range(5):\n    X1, X2, y = dataset_preparation(inp_size, out_size, features, 1)\n    target = predict_sequence(encoder_model, decoder_model, X1)\n    print('\\nInput Sequence=%s SeedSequence=%s, PredictedSequence=%s' \n          % (reverse_onehot(X1[0]), reverse_onehot(y[0]), reverse_onehot(target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f0bc237501b2b743ce1560cf585ed9db41cb595"},"cell_type":"markdown","source":"\n### Excellent References\n\n1. https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/\n2. https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798\n3. https://blog.keras.io/building-autoencoders-in-keras.html\n4. https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.html  \n5. https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n\n\nThanks for viewing the kernel, **please upvote** if you liked it. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}