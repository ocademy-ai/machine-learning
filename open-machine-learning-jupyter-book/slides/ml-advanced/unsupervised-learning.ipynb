{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- The customized css for the slides -->\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/python-programming-introduction.css\"/>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../styles/basic.css\"/>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../../assets/styles/basic.css\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MRC0e0KhQ0S",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "\n",
    "* What is unsupervised learning\n",
    "* Dimension reduction\n",
    "    * Principal Component Analysis (PCA)\n",
    "* Clustering\n",
    "    * K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWd1UlMnhT2s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is unsupervised learning\n",
    "\n",
    "- Input data are unlabeled (i.e. no labels or classes given)\n",
    "- The algorithm learns the structure of the data without any assistance\n",
    "- It allows us to process large amounts of data because the data does not need to be manually labeled\n",
    "- It is difficult to evaluate the quality of an unsupervised algorithm due to the absence of an explicit goodness metric as used in supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimension reduction\n",
    "\n",
    "- Can help with data visualization\n",
    "- Can help deal with the multicollinearity of your data and prepare the data for a supervised learning method\n",
    "- PCA, t-SNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "- One of the easiest, most intuitive, and most frequently used methods for dimensionality reduction\n",
    "- Projecting data onto its orthogonal feature subspace\n",
    "- All observations can be considered as an ellipsoid in a subspace of an initial feature space, and the new basis set in this subspace is aligned with the ellipsoid axes\n",
    "- This assumption lets us remove highly correlated features since basis set vectors are orthogonal\n",
    "- We accomplish this in a 'greedy' fashion, sequentially selecting each of the ellipsoid axes by identifying where the dispersion is maximal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Quotes from AI giants\n",
    "\n",
    "> \"To deal with hyper-planes in a 14 dimensional space, visualize a 3D space and say 'fourteen' very loudly. Everyone does it.\" - Geoffrey Hinton\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## PCA with iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YvGPUQaHhXfL",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set(style='white')\n",
    "from sklearn import decomposition\n",
    "from sklearn import datasets\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a beautiful 3d-plot\n",
    "fig = plt.figure(1, figsize=(6, 5))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "              X[y == label, 1].mean() + 1.5,\n",
    "              X[y == label, 2].mean(), name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Change the order of labels, so that they match\n",
    "y_clr = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_clr, \n",
    "           cmap=plt.cm.nipy_spectral)\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's see how PCA will improve the results of a simple model that is not able to correctly fit all of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Decision trees with depth = 2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's try this again, but, this time, let's reduce the dimensionality to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Using PCA from sklearn PCA\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_centered = X - X.mean(axis=0)\n",
    "pca.fit(X_centered)\n",
    "X_pca = pca.transform(X_centered)\n",
    "\n",
    "# Plotting the results of PCA\n",
    "plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')\n",
    "plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')\n",
    "plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Test-train split and apply PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print('Accuracy: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The accuracy did not increase significantly in this case, but, with other datasets with a high number of dimensions, PCA can drastically improve the accuracy of decision trees and other ensemble methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's check out the percent of variance that can be explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i, component in enumerate(pca.components_):\n",
    "    print(\"{} component: {}% of initial variance\".format(i + 1, \n",
    "          round(100 * pca.explained_variance_ratio_[i], 2)))\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name)\n",
    "                     for value, name in zip(component,\n",
    "                                            iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA with mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start by visualizing our data. Fetch the first 10 numbers. The numbers are represented by 8 x 8 matrixes with the color intensity for each pixel. Every matrix is flattened into a vector of 64 numbers, so we get the feature version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i,:].reshape([8,8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our data has 64 dimensions, but we are going to reduce it to only 2 and see that, even with just 2 dimensions, we can clearly see that digits separate into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Indeed, with t-SNE, the picture looks better since PCA has a linear constraint while t-SNE does not. However, even with such a small dataset, the t-SNE algorithm takes significantly more time to complete than PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=17)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. t-SNE projection');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In practice, we would choose the number of principal components such that we can explain 90% of the initial data dispersion (via the `explained_variance_ratio`). Here, that means retaining 21 principal components; therefore, we reduce the dimensionality from 64 features to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA().fit(X)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Total explained variance')\n",
    "plt.xlim(0, 63)\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "plt.axvline(21, c='b')\n",
    "plt.axhline(0.9, c='r')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "- Basically, \"I have these points here, and I can see that they organize into groups. It would be nice to describe these things more concretely, and, when a new point comes in, assign it to the correct group.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    "K-means algorithm is the most popular and yet simplest of all the clustering algorithms. Here is how it works:\n",
    "1. Select the number of clusters $k$ that you think is the optimal number.\n",
    "2. Initialize $k$ points as \"centroids\" randomly within the space of our data.\n",
    "3. Attribute each observation to its closest centroid.\n",
    "4. Update the centroids to the center of all the attributed set of observations. \n",
    "5. Repeat steps 3 and 4 a fixed number of times or until all of the centroids are stable (i.e. no longer change in step 4).\n",
    "\n",
    "This algorithm is easy to describe and visualize. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's begin by allocation 3 cluster's points\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Scipy has function that takes 2 tuples and return\n",
    "# calculated distance between them\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Randomly allocate the 3 centroids \n",
    "np.random.seed(seed=42)\n",
    "centroids = np.random.normal(loc=0.0, scale=1., size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Calculating the distance from a point to a centroid\n",
    "    distances = cdist(X, centroids)\n",
    "    # Checking what's the closest centroid for the point\n",
    "    labels = distances.argmin(axis=1)\n",
    "    \n",
    "    # Labeling the point according the point's distance\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "    \n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot K-means\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "    \n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], 'bo', label='cluster #1')\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], 'co', label='cluster #2')\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], 'mo', label='cluster #3')\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], 'rX')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Step {:}'.format(i + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here, we used Euclidean distance, but the algorithm will converge with any other metric. You can not only vary the number of steps or the convergence criteria but also the distance measure between the points and cluster centroids.\n",
    "\n",
    "Another \"feature\" of this algorithm is its sensitivity to the initial positions of the cluster centroids. You can run the algorithm several times and then average all the centroid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-means with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 8):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(X)\n",
    "    inertia.append(np.sqrt(kmeans.inertia_))\n",
    "\n",
    "plt.plot(range(1, 8), inertia, marker='s');\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('$J(C_k)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Acknowledgments\n",
    "\n",
    "Thanks to YURY KASHNITSKY for creating the open-source [Kaggle jupyter notebook](https://www.kaggle.com/code/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering), licensed under Apache 2.0. It inspires the majority of the content of this slides."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "authorship_tag": "ABX9TyOsvB/iqEjYj3VN6C/JbvkE",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "logistic_regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
