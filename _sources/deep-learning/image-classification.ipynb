{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fe3e3f",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "## What is image classification?\n",
    "\n",
    "In this chapter we will introduce the image classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications.\n",
    "\n",
    "Let me give you an example. In the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as “cat”.\n",
    "\n",
    ":::{figure-md} 01_example_of_cls\n",
    "<img src=\"../../images/deep-learning/imgcls/01_classify_eg.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Example of the image classification task\n",
    ":::\n",
    "\n",
    "The task in image classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values:\n",
    "- Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).\n",
    "- Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways.\n",
    "- Occlusion. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\n",
    "- Illumination conditions. The effects of illumination are drastic on the pixel level.\n",
    "- Background clutter. The objects of interest may blend into their environment, making them hard to identify.\n",
    "- Intra-class variation. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.\n",
    "\n",
    "A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n",
    "\n",
    "## Pipeline of image classification\n",
    "\n",
    "We’ve seen that the task in image classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows:\n",
    "- Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set,\n",
    "- Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model,\n",
    "- Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers (which we call the ground truth).\n",
    "\n",
    "## History & classic models\n",
    "\n",
    "Since image classification is a classic task for computer vision, there are several models that are well-performed in the past. We can list them as follows: LeNet, AlexNet, VGGNet, GoogleNet, ResNet, DenseNet, SENet, MobileNet, ShuffleNet and ViT. In this part, we will introduce some of them.\n",
    "\n",
    "### VGGNet\n",
    "\n",
    "The VGG (Visual Geometry Group) multilayer network model has 19 more layers than AlexNet, verifying that increasing the depth in the network structure can directly affect the model performance. The design idea of VGG is to increase the depth of the network and use a small size convolutional kernel instead. As shown in the figure below, three 3×3 convolutional kernels are used to replace the 7×7 convolutional kernels in AlexNet, and two 3×3 convolutional kernels are used to replace the 5×5 convolutional kernels, which can increase the depth of the network and improve the model effect while ensuring the same perceptual field. The number of model parameters and operations can be reduced by using smaller 3×3 Filters, and the image feature information can be better retained. The specific advantages of the improvement are summarized as follows. \n",
    "\n",
    "- Using small 3×3 filters to replace large convolutional kernels.\n",
    "- After replacing the convolution kernel, the convolution layers have the same perceptual field. \n",
    "- Each layer is trained by Re LU activation function and batch gradient descent after convolution operation.\n",
    "- It is verified that increasing the network depth can improve the model performance Although, VGG has achieved good results in image classification and localization problems in 2014 due to its deeper network structure and low computational complexity, it uses 140 million parameters and is computationally intensive, which is its shortcoming.\n",
    "\n",
    ":::{figure-md} 02_VGG_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/02_VGG.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of VGGNet {cite}`VGG_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ce182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def conv_bn(out_channels, kernel_size, strides, padding, groups=1):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.ZeroPadding2D(padding=padding),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=\"valid\",\n",
    "                groups=groups,\n",
    "                use_bias=False,\n",
    "                name=\"conv\",\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(name=\"bn\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "class RepVGGBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        strides=1,\n",
    "        padding=1,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        deploy=False,\n",
    "    ):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        self.nonlinearity = tf.keras.layers.ReLU()\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.ZeroPadding2D(padding=padding),\n",
    "                    tf.keras.layers.Conv2D(\n",
    "                        filters=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=strides,\n",
    "                        padding=\"valid\",\n",
    "                        dilation_rate=dilation,\n",
    "                        groups=groups,\n",
    "                        use_bias=True,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.rbr_identity = (\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "                if out_channels == in_channels and strides == 1\n",
    "                else None\n",
    "            )\n",
    "            self.rbr_dense = conv_bn(\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "            )\n",
    "            self.rbr_1x1 = conv_bn(\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                strides=strides,\n",
    "                padding=padding_11,\n",
    "                groups=groups,\n",
    "            )\n",
    "            print(\"RepVGG Block, identity = \", self.rbr_identity)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if hasattr(self, \"rbr_reparam\"):\n",
    "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
    "\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "\n",
    "        return self.nonlinearity(\n",
    "            self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out\n",
    "        )\n",
    "\n",
    "    # This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
    "    # You can get the equivalent kernel and bias at any time and do whatever you want,\n",
    "    #     for example, apply some penalties or constraints during training, just like you do to the other models.\n",
    "    # May be useful for quantization or pruning.\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return (\n",
    "            kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid,\n",
    "            bias3x3 + bias1x1 + biasid,\n",
    "        )\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return tf.pad(\n",
    "                kernel1x1, tf.constant([[1, 1], [1, 1], [0, 0], [0, 0]])\n",
    "            )\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, tf.keras.Sequential):\n",
    "            kernel = branch.get_layer(\"conv\").weights[0]\n",
    "            running_mean = branch.get_layer(\"bn\").moving_mean\n",
    "            running_var = branch.get_layer(\"bn\").moving_variance\n",
    "            gamma = branch.get_layer(\"bn\").gamma\n",
    "            beta = branch.get_layer(\"bn\").beta\n",
    "            eps = branch.get_layer(\"bn\").epsilon\n",
    "        else:\n",
    "            assert isinstance(branch, tf.keras.layers.BatchNormalization)\n",
    "            if not hasattr(self, \"id_tensor\"):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros(\n",
    "                    (3, 3, input_dim, self.in_channels), dtype=np.float32\n",
    "                )\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[1, 1, i % input_dim, i] = 1\n",
    "                self.id_tensor = tf.convert_to_tensor(\n",
    "                    kernel_value, dtype=np.float32\n",
    "                )\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.moving_mean\n",
    "            running_var = branch.moving_variance\n",
    "            gamma = branch.gamma\n",
    "            beta = branch.beta\n",
    "            eps = branch.epsilon\n",
    "        std = tf.sqrt(running_var + eps)\n",
    "        t = gamma / std\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "    def repvgg_convert(self):\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        return kernel, bias\n",
    "\n",
    "\n",
    "class RepVGG(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        num_classes=1000,\n",
    "        width_multiplier=None,\n",
    "        override_groups_map=None,\n",
    "        deploy=False,\n",
    "    ):\n",
    "        super(RepVGG, self).__init__()\n",
    "\n",
    "        assert len(width_multiplier) == 4\n",
    "\n",
    "        self.deploy = deploy\n",
    "        self.override_groups_map = override_groups_map or dict()\n",
    "\n",
    "        assert 0 not in self.override_groups_map\n",
    "\n",
    "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
    "\n",
    "        self.stage0 = RepVGGBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=self.in_planes,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding=1,\n",
    "            deploy=self.deploy,\n",
    "        )\n",
    "        self.cur_layer_idx = 1\n",
    "        self.stage1 = self._make_stage(\n",
    "            int(64 * width_multiplier[0]), num_blocks[0], stride=2\n",
    "        )\n",
    "        self.stage2 = self._make_stage(\n",
    "            int(128 * width_multiplier[1]), num_blocks[1], stride=2\n",
    "        )\n",
    "        self.stage3 = self._make_stage(\n",
    "            int(256 * width_multiplier[2]), num_blocks[2], stride=2\n",
    "        )\n",
    "        self.stage4 = self._make_stage(\n",
    "            int(512 * width_multiplier[3]), num_blocks[3], stride=2\n",
    "        )\n",
    "        self.gap = tfa.layers.AdaptiveAveragePooling2D(output_size=1)\n",
    "        self.linear = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def _make_stage(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
    "            blocks.append(\n",
    "                RepVGGBlock(\n",
    "                    in_channels=self.in_planes,\n",
    "                    out_channels=planes,\n",
    "                    kernel_size=3,\n",
    "                    strides=stride,\n",
    "                    padding=1,\n",
    "                    groups=cur_groups,\n",
    "                    deploy=self.deploy,\n",
    "                )\n",
    "            )\n",
    "            self.in_planes = planes\n",
    "            self.cur_layer_idx += 1\n",
    "        return tf.keras.Sequential(blocks)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.stage0(x)\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.stage4(out)\n",
    "        out = self.gap(out)\n",
    "        out = tf.keras.layers.Flatten()(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c89c6",
   "metadata": {},
   "source": [
    "### Resnet\n",
    "\n",
    "ResNet (Residual Network) was proposed by Kaiming He and won the 2015 ILSVRC Grand Prix with an error rate of 3.57%. In the previous network, when the model is not deep enough, its network recognition is not strong, but when the network stack (Plain Network) is very deep, the network gradient disappearance and gradient dispersion are obvious, resulting in the model's computational effectiveness but not up but down. Therefore, in view of the degradation problem of this deep network, ResNet is designed as an ultra-deep network without the gradient vanishing problem.ResNet has various types depending on the number of layers, from 18 to 1202 layers. As an example, Res Net50 consists of 49 convolutional layers and 1 fully connected layer, as shown in the figure below. This simple addition does not add additional parameters and computation to the network, but can greatly increase the training speed and improve the training effect, and this simple structure can well solve the degradation problem when the model deepens the number of layers. In this way, the network will always be in the optimal state and the performance of the network will not decrease with increasing depth.\n",
    "\n",
    "The most important part of ResNet should be the residual block, and here is the structure.\n",
    "\n",
    ":::{figure-md} 03_residual_block\n",
    "<img src=\"../../images/deep-learning/imgcls/03_resblock.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of residual block {cite}`resblock_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 04_ResNet_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/04_ResNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of residual network {cite}`resnet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976370e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib as tf_contrib\n",
    "\n",
    "weight_init = tf_contrib.layers.variance_scaling_initializer()\n",
    "weight_regularizer = tf_contrib.layers.l2_regularizer(0.0001)\n",
    "\n",
    "# Layer\n",
    "def conv(x, channels, kernel=4, stride=2, padding='SAME', use_bias=True, scope='conv_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = tf.layers.conv2d(inputs=x, filters=channels,\n",
    "                             kernel_size=kernel, kernel_initializer=weight_init,\n",
    "                             kernel_regularizer=weight_regularizer,\n",
    "                             strides=stride, use_bias=use_bias, padding=padding)\n",
    "\n",
    "        return x\n",
    "\n",
    "def fully_conneted(x, units, use_bias=True, scope='fully_0'):\n",
    "    with tf.variable_scope(scope):\n",
    "        x = flatten(x)\n",
    "        x = tf.layers.dense(x, units=units, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, use_bias=use_bias)\n",
    "\n",
    "        return x\n",
    "\n",
    "def resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='resblock') :\n",
    "    with tf.variable_scope(scope) :\n",
    "\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_0')\n",
    "        x = relu(x)\n",
    "\n",
    "\n",
    "        if downsample :\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            x_init = conv(x_init, channels, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        else :\n",
    "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1')\n",
    "        x = relu(x)\n",
    "        x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_1')\n",
    "\n",
    "\n",
    "\n",
    "        return x + x_init\n",
    "\n",
    "def bottle_resblock(x_init, channels, is_training=True, use_bias=True, downsample=False, scope='bottle_resblock') :\n",
    "    with tf.variable_scope(scope) :\n",
    "        x = batch_norm(x_init, is_training, scope='batch_norm_1x1_front')\n",
    "        shortcut = relu(x)\n",
    "\n",
    "        x = conv(shortcut, channels, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_front')\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_3x3')\n",
    "        x = relu(x)\n",
    "\n",
    "        if downsample :\n",
    "            x = conv(x, channels, kernel=3, stride=2, use_bias=use_bias, scope='conv_0')\n",
    "            shortcut = conv(shortcut, channels*4, kernel=1, stride=2, use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        else :\n",
    "            x = conv(x, channels, kernel=3, stride=1, use_bias=use_bias, scope='conv_0')\n",
    "            shortcut = conv(shortcut, channels * 4, kernel=1, stride=1, use_bias=use_bias, scope='conv_init')\n",
    "\n",
    "        x = batch_norm(x, is_training, scope='batch_norm_1x1_back')\n",
    "        x = relu(x)\n",
    "        x = conv(x, channels*4, kernel=1, stride=1, use_bias=use_bias, scope='conv_1x1_back')\n",
    "\n",
    "        return x + shortcut\n",
    "\n",
    "\n",
    "\n",
    "def get_residual_layer(res_n) :\n",
    "    x = []\n",
    "\n",
    "    if res_n == 18 :\n",
    "        x = [2, 2, 2, 2]\n",
    "\n",
    "    if res_n == 34 :\n",
    "        x = [3, 4, 6, 3]\n",
    "\n",
    "    if res_n == 50 :\n",
    "        x = [3, 4, 6, 3]\n",
    "\n",
    "    if res_n == 101 :\n",
    "        x = [3, 4, 23, 3]\n",
    "\n",
    "    if res_n == 152 :\n",
    "        x = [3, 8, 36, 3]\n",
    "\n",
    "    return x\n",
    "\n",
    "# Sampling\n",
    "def flatten(x) :\n",
    "    return tf.layers.flatten(x)\n",
    "\n",
    "def global_avg_pooling(x):\n",
    "    gap = tf.reduce_mean(x, axis=[1, 2], keepdims=True)\n",
    "    return gap\n",
    "\n",
    "def avg_pooling(x) :\n",
    "    return tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='SAME')\n",
    "\n",
    "# Activation function\n",
    "def relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "# Normalization function\n",
    "def batch_norm(x, is_training=True, scope='batch_norm'):\n",
    "    return tf_contrib.layers.batch_norm(x,\n",
    "                                        decay=0.9, epsilon=1e-05,\n",
    "                                        center=True, scale=True, updates_collections=None,\n",
    "                                        is_training=is_training, scope=scope)\n",
    "    \n",
    "class ResNet(object):\n",
    "    def __init__(self, sess, args):\n",
    "        self.model_name = 'ResNet'\n",
    "        self.sess = sess\n",
    "        self.dataset_name = args.dataset\n",
    "\n",
    "        if self.dataset_name == 'cifar10' :\n",
    "            self.train_x, self.train_y, self.test_x, self.test_y = load_cifar10()\n",
    "            self.img_size = 32\n",
    "            self.c_dim = 3\n",
    "            self.label_dim = 10\n",
    "\n",
    "        self.checkpoint_dir = args.checkpoint_dir\n",
    "        self.log_dir = args.log_dir\n",
    "\n",
    "        self.res_n = args.res_n\n",
    "\n",
    "        self.epoch = args.epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.iteration = len(self.train_x) // self.batch_size\n",
    "\n",
    "        self.init_lr = args.lr\n",
    "\n",
    "    # Generator\n",
    "    def network(self, x, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(\"network\", reuse=reuse):\n",
    "\n",
    "            if self.res_n < 50 :\n",
    "                residual_block = resblock\n",
    "            else :\n",
    "                residual_block = bottle_resblock\n",
    "\n",
    "            residual_list = get_residual_layer(self.res_n)\n",
    "\n",
    "            ch = 32 # paper is 64\n",
    "            x = conv(x, channels=ch, kernel=3, stride=1, scope='conv')\n",
    "\n",
    "            for i in range(residual_list[0]) :\n",
    "                x = residual_block(x, channels=ch, is_training=is_training, downsample=False, scope='resblock0_' + str(i))\n",
    "\n",
    "            x = residual_block(x, channels=ch*2, is_training=is_training, downsample=True, scope='resblock1_0')\n",
    "\n",
    "            for i in range(1, residual_list[1]) :\n",
    "                x = residual_block(x, channels=ch*2, is_training=is_training, downsample=False, scope='resblock1_' + str(i))\n",
    "\n",
    "            x = residual_block(x, channels=ch*4, is_training=is_training, downsample=True, scope='resblock2_0')\n",
    "\n",
    "            for i in range(1, residual_list[2]) :\n",
    "                x = residual_block(x, channels=ch*4, is_training=is_training, downsample=False, scope='resblock2_' + str(i))\n",
    "\n",
    "            x = residual_block(x, channels=ch*8, is_training=is_training, downsample=True, scope='resblock_3_0')\n",
    "\n",
    "            for i in range(1, residual_list[3]) :\n",
    "                x = residual_block(x, channels=ch*8, is_training=is_training, downsample=False, scope='resblock_3_' + str(i))\n",
    "\n",
    "            x = batch_norm(x, is_training, scope='batch_norm')\n",
    "            x = relu(x)\n",
    "\n",
    "            x = global_avg_pooling(x)\n",
    "            x = fully_conneted(x, units=self.label_dim, scope='logit')\n",
    "\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623456a",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "\n",
    "DenseNet proposes a more radical dense connection mechanism than ResNet: i.e., connecting all layers to each other, specifically each layer accepts all the layers before it as its additional input. resNet short-circuits each layer with some previous layer (usually 2-3 layers), and the connection is made by element-level summation. In DenseNet, each layer is connected (concat) with all the preceding layers in the channel dimension and used as input to the next layer, which is a dense connection. Moreover, DenseNet is directly concat feature maps from different layers, which enables feature reuse and improves efficiency, and this feature is the most important difference between DenseNet and ResNet.\n",
    "\n",
    ":::{figure-md} 05_dense_block\n",
    "<img src=\"../../images/deep-learning/imgcls/05_denseblock.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of dense block {cite}`denseblock_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 06_DenseNet_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/06_DenseNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of dense network {cite}`densenet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tflearn.layers.conv import global_avg_pool\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib.layers import batch_norm, flatten\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter\n",
    "nb_block = 2\n",
    "dropout_rate = 0.2\n",
    "\n",
    "def conv_layer(input, filter, kernel, stride=1, layer_name=\"conv\"):\n",
    "    with tf.name_scope(layer_name):\n",
    "        network = tf.layers.conv2d(inputs=input, filters=filter, kernel_size=kernel, strides=stride, padding='SAME')\n",
    "        return network\n",
    "\n",
    "def Global_Average_Pooling(x, stride=1):\n",
    "    \"\"\"\n",
    "    width = np.shape(x)[1]\n",
    "    height = np.shape(x)[2]\n",
    "    pool_size = [width, height]\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride) # The stride value does not matter\n",
    "    It is global average pooling without tflearn\n",
    "    \"\"\"\n",
    "\n",
    "    return global_avg_pool(x, name='Global_avg_pooling')\n",
    "    # But maybe you need to install h5py and curses or not\n",
    "\n",
    "\n",
    "def Batch_Normalization(x, training, scope):\n",
    "    with arg_scope([batch_norm],\n",
    "                   scope=scope,\n",
    "                   updates_collections=None,\n",
    "                   decay=0.9,\n",
    "                   center=True,\n",
    "                   scale=True,\n",
    "                   zero_debias_moving_mean=True) :\n",
    "        return tf.cond(training,\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=None),\n",
    "                       lambda : batch_norm(inputs=x, is_training=training, reuse=True))\n",
    "\n",
    "def Drop_out(x, rate, training) :\n",
    "    return tf.layers.dropout(inputs=x, rate=rate, training=training)\n",
    "\n",
    "def Relu(x):\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def Average_pooling(x, pool_size=[2,2], stride=2, padding='VALID'):\n",
    "    return tf.layers.average_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "\n",
    "def Max_Pooling(x, pool_size=[3,3], stride=2, padding='VALID'):\n",
    "    return tf.layers.max_pooling2d(inputs=x, pool_size=pool_size, strides=stride, padding=padding)\n",
    "\n",
    "def Concatenation(layers) :\n",
    "    return tf.concat(layers, axis=3)\n",
    "\n",
    "def Linear(x) :\n",
    "    return tf.layers.dense(inputs=x, units=class_num, name='linear')\n",
    "\n",
    "class DenseNet():\n",
    "    def __init__(self, x, nb_blocks, filters, training):\n",
    "        self.nb_blocks = nb_blocks\n",
    "        self.filters = filters\n",
    "        self.training = training\n",
    "        self.model = self.Dense_net(x)\n",
    "\n",
    "    def bottleneck_layer(self, x, scope):\n",
    "        # print(x)\n",
    "        with tf.name_scope(scope):\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=4 * self.filters, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch2')\n",
    "            x = Relu(x)\n",
    "            x = conv_layer(x, filter=self.filters, kernel=[3,3], layer_name=scope+'_conv2')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "\n",
    "            # print(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def transition_layer(self, x, scope):\n",
    "        with tf.name_scope(scope):\n",
    "            x = Batch_Normalization(x, training=self.training, scope=scope+'_batch1')\n",
    "            x = Relu(x)\n",
    "            # x = conv_layer(x, filter=self.filters, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            \n",
    "            # https://github.com/taki0112/Densenet-Tensorflow/issues/10\n",
    "            \n",
    "            in_channel = x.shape[-1]\n",
    "            x = conv_layer(x, filter=in_channel*0.5, kernel=[1,1], layer_name=scope+'_conv1')\n",
    "            x = Drop_out(x, rate=dropout_rate, training=self.training)\n",
    "            x = Average_pooling(x, pool_size=[2,2], stride=2)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def dense_block(self, input_x, nb_layers, layer_name):\n",
    "        with tf.name_scope(layer_name):\n",
    "            layers_concat = list()\n",
    "            layers_concat.append(input_x)\n",
    "\n",
    "            x = self.bottleneck_layer(input_x, scope=layer_name + '_bottleN_' + str(0))\n",
    "\n",
    "            layers_concat.append(x)\n",
    "\n",
    "            for i in range(nb_layers - 1):\n",
    "                x = Concatenation(layers_concat)\n",
    "                x = self.bottleneck_layer(x, scope=layer_name + '_bottleN_' + str(i + 1))\n",
    "                layers_concat.append(x)\n",
    "\n",
    "            x = Concatenation(layers_concat)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def Dense_net(self, input_x):\n",
    "        x = conv_layer(input_x, filter=2 * self.filters, kernel=[7,7], stride=2, layer_name='conv0')\n",
    "        x = Max_Pooling(x, pool_size=[3,3], stride=2)\n",
    "\n",
    "        for i in range(self.nb_blocks) :\n",
    "            # 6 -> 12 -> 48\n",
    "            x = self.dense_block(input_x=x, nb_layers=4, layer_name='dense_'+str(i))\n",
    "            x = self.transition_layer(x, scope='trans_'+str(i))\n",
    "\n",
    "        \"\"\"\n",
    "        x = self.dense_block(input_x=x, nb_layers=6, layer_name='dense_1')\n",
    "        x = self.transition_layer(x, scope='trans_1')\n",
    "        x = self.dense_block(input_x=x, nb_layers=12, layer_name='dense_2')\n",
    "        x = self.transition_layer(x, scope='trans_2')\n",
    "        x = self.dense_block(input_x=x, nb_layers=48, layer_name='dense_3')\n",
    "        x = self.transition_layer(x, scope='trans_3')\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.dense_block(input_x=x, nb_layers=32, layer_name='dense_final')\n",
    "\n",
    "        # 100 Layer\n",
    "        x = Batch_Normalization(x, training=self.training, scope='linear_batch')\n",
    "        x = Relu(x)\n",
    "        x = Global_Average_Pooling(x)\n",
    "        x = flatten(x)\n",
    "        x = Linear(x)\n",
    "\n",
    "        # x = tf.reshape(x, [-1, 10])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155dbf3",
   "metadata": {},
   "source": [
    "### MobileNet\n",
    "\n",
    "MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem.\n",
    "\n",
    "Besides, the standard convolutional filters for normal CNNs are replaced by two layers: depthwise convolution and pointwise convolution to build a depthwise separable filter.\n",
    "\n",
    ":::{figure-md} 07_MobileNet_convolution_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/07_mobileconv.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Replacement of standard convolution filter {cite}`mobileconv_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 07_MobileNet_body_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/08_MobileNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Body structure of MobileNet {cite}`mobilenet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "\n",
    "def mobilenet_v2_arg_scope(weight_decay, is_training=True, depth_multiplier=1.0, regularize_depthwise=False,\n",
    "                           dropout_keep_prob=1.0):\n",
    "\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(weight_decay)\n",
    "    if regularize_depthwise:\n",
    "        depthwise_regularizer = regularizer\n",
    "    else:\n",
    "        depthwise_regularizer = None\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],\n",
    "                        activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params={'is_training': is_training, 'center': True, 'scale': True }):\n",
    "\n",
    "        with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):\n",
    "\n",
    "            with slim.arg_scope([slim.separable_conv2d],\n",
    "                                weights_regularizer=depthwise_regularizer, depth_multiplier=depth_multiplier):\n",
    "\n",
    "                with slim.arg_scope([slim.dropout], is_training=is_training, keep_prob=dropout_keep_prob) as sc:\n",
    "\n",
    "                    return sc\n",
    "\n",
    "\n",
    "def block(net, input_filters, output_filters, expansion, stride):\n",
    "    res_block = net\n",
    "    res_block = slim.conv2d(inputs=res_block, num_outputs=input_filters * expansion, kernel_size=[1, 1])\n",
    "    res_block = slim.separable_conv2d(inputs=res_block, num_outputs=None, kernel_size=[3, 3], stride=stride)\n",
    "    res_block = slim.conv2d(inputs=res_block, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n",
    "    if stride == 2:\n",
    "        return res_block\n",
    "    else:\n",
    "        if input_filters != output_filters:\n",
    "            net = slim.conv2d(inputs=net, num_outputs=output_filters, kernel_size=[1, 1], activation_fn=None)\n",
    "        return tf.add(res_block, net)\n",
    "\n",
    "\n",
    "def blocks(net, expansion, output_filters, repeat, stride):\n",
    "    input_filters = net.shape[3].value\n",
    "\n",
    "    # first layer should take stride into account\n",
    "    net = block(net, input_filters, output_filters, expansion, stride)\n",
    "\n",
    "    for _ in range(1, repeat):\n",
    "        net = block(net, input_filters, output_filters, expansion, 1)\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def mobilenet_v2(inputs,\n",
    "                 num_classes=1000,\n",
    "                 dropout_keep_prob=0.999,\n",
    "                 is_training=True,\n",
    "                 depth_multiplier=1.0,\n",
    "                 prediction_fn=tf.contrib.layers.softmax,\n",
    "                 spatial_squeeze=True,\n",
    "                 scope='MobilenetV2'):\n",
    "\n",
    "    endpoints = dict()\n",
    "\n",
    "    expansion = 6\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "\n",
    "        with slim.arg_scope(mobilenet_v2_arg_scope(0.0004, is_training=is_training, depth_multiplier=depth_multiplier,\n",
    "                                                   dropout_keep_prob=dropout_keep_prob)):\n",
    "            net = tf.identity(inputs)\n",
    "\n",
    "            net = slim.conv2d(net, 32, [3, 3], scope='conv11', stride=2)\n",
    "\n",
    "            net = blocks(net=net, expansion=1, output_filters=16, repeat=1, stride=1)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=24, repeat=2, stride=2)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=32, repeat=3, stride=2)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=64, repeat=4, stride=2)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=96, repeat=3, stride=1)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=160, repeat=3, stride=2)\n",
    "\n",
    "            net = blocks(net=net, expansion=expansion, output_filters=320, repeat=1, stride=1)\n",
    "\n",
    "            net = slim.conv2d(net, 1280, [1, 1], scope='last_bottleneck')\n",
    "\n",
    "            net = slim.avg_pool2d(net, [7, 7])\n",
    "\n",
    "            logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='features')\n",
    "\n",
    "            if spatial_squeeze:\n",
    "                logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "\n",
    "            endpoints['Logits'] = logits\n",
    "\n",
    "            if prediction_fn:\n",
    "                endpoints['Predictions'] = prediction_fn(logits, scope='Predictions')\n",
    "\n",
    "    return logits, endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4bdaa7",
   "metadata": {},
   "source": [
    "### ViT\n",
    "\n",
    "Different from the previous models, ViT (Vision Transformer) uses the concept of Transformer. Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, they split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. They train the model on image classification in supervised fashion.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6623b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.layers as nn\n",
    "\n",
    "from tensorflow import einsum\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.tensorflow import Rearrange\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class PreNorm(Layer):\n",
    "    def __init__(self, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "\n",
    "        self.norm = nn.LayerNormalization()\n",
    "        self.fn = fn\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.fn(self.norm(x), training=training)\n",
    "\n",
    "class MLP(Layer):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        def GELU():\n",
    "            def gelu(x, approximate=False):\n",
    "                if approximate:\n",
    "                    coeff = tf.cast(0.044715, x.dtype)\n",
    "                    return 0.5 * x * (1.0 + tf.tanh(0.7978845608028654 * (x + coeff * tf.pow(x, 3))))\n",
    "                else:\n",
    "                    return 0.5 * x * (1.0 + tf.math.erf(x / tf.cast(1.4142135623730951, x.dtype)))\n",
    "\n",
    "            return nn.Activation(gelu)\n",
    "\n",
    "        self.net = Sequential([\n",
    "            nn.Dense(units=hidden_dim),\n",
    "            GELU(),\n",
    "            nn.Dropout(rate=dropout),\n",
    "            nn.Dense(units=dim),\n",
    "            nn.Dropout(rate=dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        return self.net(x, training=training)\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
    "        super(Attention, self).__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax()\n",
    "        self.to_qkv = nn.Dense(units=inner_dim * 3, use_bias=False)\n",
    "\n",
    "        if project_out:\n",
    "            self.to_out = [\n",
    "                nn.Dense(units=dim),\n",
    "                nn.Dropout(rate=dropout)\n",
    "            ]\n",
    "        else:\n",
    "            self.to_out = []\n",
    "\n",
    "        self.to_out = Sequential(self.to_out)\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        qkv = self.to_qkv(x)\n",
    "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        # dots = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) * self.scale\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        # x = tf.matmul(attn, v)\n",
    "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
    "        x = self.to_out(x, training=training)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Transformer(Layer):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append([\n",
    "                PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(MLP(dim, mlp_dim, dropout=dropout))\n",
    "            ])\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        for attn, mlp in self.layers:\n",
    "            x = attn(x, training=training) + x\n",
    "            x = mlp(x, training=training) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "class ViT(Model):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,\n",
    "                 pool='cls', dim_head=64, dropout=0.0, emb_dropout=0.0):\n",
    "        \"\"\"\n",
    "            image_size: int.\n",
    "            -> Image size. If you have rectangular images, make sure your image size is the maximum of the width and height\n",
    "            patch_size: int.\n",
    "            -> Number of patches. image_size must be divisible by patch_size.\n",
    "            -> The number of patches is: n = (image_size // patch_size) ** 2 and n must be greater than 16.\n",
    "            num_classes: int.\n",
    "            -> Number of classes to classify.\n",
    "            dim: int.\n",
    "            -> Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n",
    "            depth: int.\n",
    "            -> Number of Transformer blocks.\n",
    "            heads: int.\n",
    "            -> Number of heads in Multi-head Attention layer.\n",
    "            mlp_dim: int.\n",
    "            -> Dimension of the MLP (FeedForward) layer.\n",
    "            dropout: float between [0, 1], default 0..\n",
    "            -> Dropout rate.\n",
    "            emb_dropout: float between [0, 1], default 0.\n",
    "            -> Embedding dropout rate.\n",
    "            pool: string, either cls token pooling or mean pooling\n",
    "        \"\"\"\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.patch_embedding = Sequential([\n",
    "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            nn.Dense(units=dim)\n",
    "        ], name='patch_embedding')\n",
    "\n",
    "        self.pos_embedding = tf.Variable(initial_value=tf.random.normal([1, num_patches + 1, dim]))\n",
    "        self.cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, dim]))\n",
    "        self.dropout = nn.Dropout(rate=emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "\n",
    "        self.mlp_head = Sequential([\n",
    "            nn.LayerNormalization(),\n",
    "            nn.Dense(units=num_classes)\n",
    "        ], name='mlp_head')\n",
    "\n",
    "    def call(self, img, training=True, **kwargs):\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, d = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
    "        x = tf.concat([cls_tokens, x], axis=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        x = self.transformer(x, training=training)\n",
    "\n",
    "        if self.pool == 'mean':\n",
    "            x = tf.reduce_mean(x, axis=1)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d6650",
   "metadata": {},
   "source": [
    "## Classic datasets\n",
    "\n",
    "As we said before, image classification task is mainly trained by datasets, so the importance of dataset is obvious. Here, we will introduce some widely-used datasets.\n",
    "\n",
    "### CIFAR-10/100\n",
    "\n",
    "The [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "The CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "\n",
    "```{note}\n",
    "Download for Linux\n",
    "wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "wget http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "\n",
    "Download for Win/Mac\n",
    "Download from the offical website\n",
    "```\n",
    "\n",
    "### ImageNet-1000\n",
    "\n",
    "[ImageNet](https://image-net.org/) is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet; the majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly labeled and sorted images for most of the concepts in the WordNet hierarchy.\n",
    "\n",
    "```{note}\n",
    "If you want to download this dataset, please visit [kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description) for more information.\n",
    "```\n",
    "\n",
    "## Standards\n",
    "\n",
    "### Top-1 accuracy\n",
    "\n",
    "If your predicted label takes the largest one inside the final probability vector as the prediction result, and if the one with the highest probability in your prediction result is correctly classified, then the prediction is correct. Otherwise, the prediction is wrong.\n",
    "\n",
    "### Top-5 accuracy\n",
    "\n",
    "Among the 50 classification probabilities of the test image, take the first 5 maximum classification probabilities, whether the correct label (classification) is in it or not, that is, whether it is one of these first 5, if it is, it is a successful classification.\n",
    "\n",
    "## Your turn! 🚀\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Stanford](https://www.stanford.edu) for creating the open-source course [CS231n: Deep Learning for Computer Vision](https://cs231n.github.io/classification/), [Duc Thang HOANG](https://github.com/hoangthang1607) for creating the open-source project [RepVGG-Tensorflow-2](https://github.com/hoangthang1607/RepVGG-Tensorflow-2), [Junho Kim](https://github.com/taki0112) for creating the open-source project [ResNet-Tensorflow](https://github.com/taki0112/ResNet-Tensorflow), [Densenet-Tensorflow](https://github.com/taki0112/Densenet-Tensorflow) and [vit-tensorflow](https://github.com/taki0112/vit-tensorflow) and [ohadlights](https://github.com/ohadlights) for creating the open-source project [mobilenetv2](https://github.com/ohadlights/mobilenetv2). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   73,
   306,
   328,
   505,
   525,
   672,
   694,
   797,
   805,
   983
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}