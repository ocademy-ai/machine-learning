{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee33fb34",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "\n",
    "## What is image classification?\n",
    "\n",
    "In this chapter we will introduce the image classification problem, which is the task of assigning an input image one label from a fixed set of categories. This is one of the core problems in Computer Vision that, despite its simplicity, has a large variety of practical applications.\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<iframe src=\"../assets/html/cls-demo/index.html\" width=\"105%\" height=\"700px;\" style=\"border:none;\"></iframe>\n",
    "A demo of image classification. <a href=\"http://vision.stanford.edu/teaching/cs231n/\">[source]</a>\n",
    "</p>\n",
    "\n",
    "Let me give you an example. In the image below an image classification model takes a single image and assigns probabilities to 4 labels, {cat, dog, hat, mug}. As shown in the image, keep in mind that to a computer an image is represented as one large 3-dimensional array of numbers. In this example, the cat image is 248 pixels wide, 400 pixels tall, and has three color channels Red,Green,Blue (or RGB for short). Therefore, the image consists of 248 x 400 x 3 numbers, or a total of 297,600 numbers. Each number is an integer that ranges from 0 (black) to 255 (white). Our task is to turn this quarter of a million numbers into a single label, such as ‚Äúcat‚Äù.\n",
    "\n",
    ":::{figure-md} 01_example_of_cls\n",
    "<img src=\"../../images/deep-learning/imgcls/01_classify_eg.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Example of the image classification task\n",
    ":::\n",
    "\n",
    "The task in image classification is to predict a single label (or a distribution over labels as shown here to indicate our confidence) for a given image. Images are 3-dimensional arrays of integers from 0 to 255, of size Width x Height x 3. The 3 represents the three color channels Red, Green, Blue.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Since this task of recognizing a visual concept (e.g. cat) is relatively trivial for a human to perform, it is worth considering the challenges involved from the perspective of a Computer Vision algorithm. As we present (an inexhaustive) list of challenges below, keep in mind the raw representation of images as a 3-D array of brightness values:\n",
    "- Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera.\n",
    "- Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).\n",
    "- Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways.\n",
    "- Occlusion. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.\n",
    "- Illumination conditions. The effects of illumination are drastic on the pixel level.\n",
    "- Background clutter. The objects of interest may blend into their environment, making them hard to identify.\n",
    "- Intra-class variation. The classes of interest can often be relatively broad, such as chair. There are many different types of these objects, each with their own appearance.\n",
    "\n",
    "A good image classification model must be invariant to the cross product of all these variations, while simultaneously retaining sensitivity to the inter-class variations.\n",
    "\n",
    "## Pipeline of image classification\n",
    "\n",
    "We‚Äôve seen that the task in image classification is to take an array of pixels that represents a single image and assign a label to it. Our complete pipeline can be formalized as follows:\n",
    "- Input: Our input consists of a set of N images, each labeled with one of K different classes. We refer to this data as the training set,\n",
    "- Learning: Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as training a classifier, or learning a model,\n",
    "- Evaluation: In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we‚Äôre hoping that a lot of the predictions match up with the true answers (which we call the ground truth).\n",
    "\n",
    "## History & classic models\n",
    "\n",
    "Since image classification is a classic task for computer vision, there are several models that are well-performed in the past. We can list them as follows: LeNet, AlexNet, VGGNet, GoogleNet, ResNet, DenseNet, SENet, MobileNet, ShuffleNet and ViT. In this part, we will introduce some of them.\n",
    "\n",
    "### VGGNet\n",
    "\n",
    "The VGG (Visual Geometry Group) multilayer network model has 19 more layers than AlexNet, verifying that increasing the depth in the network structure can directly affect the model performance. The design idea of VGG is to increase the depth of the network and use a small size convolutional kernel instead. As shown in the figure below, three 3√ó3 convolutional kernels are used to replace the 7√ó7 convolutional kernels in AlexNet, and two 3√ó3 convolutional kernels are used to replace the 5√ó5 convolutional kernels, which can increase the depth of the network and improve the model effect while ensuring the same perceptual field. The number of model parameters and operations can be reduced by using smaller 3√ó3 Filters, and the image feature information can be better retained. The specific advantages of the improvement are summarized as follows. \n",
    "\n",
    "- Using small 3√ó3 filters to replace large convolutional kernels.\n",
    "- After replacing the convolution kernel, the convolution layers have the same perceptual field. \n",
    "- Each layer is trained by Re LU activation function and batch gradient descent after convolution operation.\n",
    "- It is verified that increasing the network depth can improve the model performance Although, VGG has achieved good results in image classification and localization problems in 2014 due to its deeper network structure and low computational complexity, it uses 140 million parameters and is computationally intensive, which is its shortcoming.\n",
    "\n",
    ":::{figure-md} 02_VGG_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/02_VGG.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of VGGNet {cite}`VGG_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9885b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "#This code defines a function `conv_bn` that returns a sequential model consisting of a zero-padding layer, \n",
    "# a convolution layer, and a batch normalization layer.\n",
    "\n",
    "def conv_bn(out_channels, kernel_size, strides, padding, groups=1):\n",
    "    return tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.ZeroPadding2D(padding=padding),\n",
    "            tf.keras.layers.Conv2D(\n",
    "                filters=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=\"valid\",\n",
    "                groups=groups,\n",
    "                use_bias=False,\n",
    "                name=\"conv\",\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(name=\"bn\"),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1a5cf",
   "metadata": {},
   "source": [
    "This function is useful for constructing convolutional neural network architectures, as it provides a simple way to combine convolution and batch normalization layers.\n",
    "\n",
    "- `out_channels` specifies the number of output channels for the convolution layer. \n",
    "- `kernel_size` specifies the size of the convolution kernel. \n",
    "- `strides` specifies the stride size for the convolution operation. \n",
    "- `padding` specifies the padding size for the zero-padding layer. \n",
    "- `groups` specifies the number of groups for the group convolution operation.\n",
    "\n",
    "The function returns a sequential model that consists of three layers: \n",
    "a zero-padding layer, a convolution layer, and a batch normalization layer. \n",
    "\n",
    "The convolution layer has `out_channels` filters, a kernel size of `kernel_size`, and a stride size of `strides`. The zero-padding layer has a padding size of `padding`. The batch normalization layer normalizes the activations of the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepVGGBlock(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        strides=1,\n",
    "        padding=1,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        deploy=False,\n",
    "    ):\n",
    "        super(RepVGGBlock, self).__init__()\n",
    "        self.deploy = deploy\n",
    "        self.groups = groups\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        assert kernel_size == 3\n",
    "        assert padding == 1\n",
    "\n",
    "        padding_11 = padding - kernel_size // 2\n",
    "\n",
    "        self.nonlinearity = tf.keras.layers.ReLU()\n",
    "\n",
    "        if deploy:\n",
    "            self.rbr_reparam = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.ZeroPadding2D(padding=padding),\n",
    "                    tf.keras.layers.Conv2D(\n",
    "                        filters=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        strides=strides,\n",
    "                        padding=\"valid\",\n",
    "                        dilation_rate=dilation,\n",
    "                        groups=groups,\n",
    "                        use_bias=True,\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.rbr_identity = (\n",
    "                tf.keras.layers.BatchNormalization()\n",
    "                if out_channels == in_channels and strides == 1\n",
    "                else None\n",
    "            )\n",
    "            self.rbr_dense = conv_bn(\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                strides=strides,\n",
    "                padding=padding,\n",
    "                groups=groups,\n",
    "            )\n",
    "            self.rbr_1x1 = conv_bn(\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                strides=strides,\n",
    "                padding=padding_11,\n",
    "                groups=groups,\n",
    "            )\n",
    "            print(\"RepVGG Block, identity = \", self.rbr_identity)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if hasattr(self, \"rbr_reparam\"):\n",
    "            return self.nonlinearity(self.rbr_reparam(inputs))\n",
    "\n",
    "        if self.rbr_identity is None:\n",
    "            id_out = 0\n",
    "        else:\n",
    "            id_out = self.rbr_identity(inputs)\n",
    "\n",
    "        return self.nonlinearity(\n",
    "            self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out\n",
    "        )\n",
    "\n",
    "    # This func derives the equivalent kernel and bias in a DIFFERENTIABLE way.\n",
    "    # You can get the equivalent kernel and bias at any time and do whatever you want,\n",
    "    #     for example, apply some penalties or constraints during training, just like you do to the other models.\n",
    "    # May be useful for quantization or pruning.\n",
    "    def get_equivalent_kernel_bias(self):\n",
    "        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)\n",
    "        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)\n",
    "        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)\n",
    "        return (\n",
    "            kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid,\n",
    "            bias3x3 + bias1x1 + biasid,\n",
    "        )\n",
    "\n",
    "    def _pad_1x1_to_3x3_tensor(self, kernel1x1):\n",
    "        if kernel1x1 is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return tf.pad(\n",
    "                kernel1x1, tf.constant([[1, 1], [1, 1], [0, 0], [0, 0]])\n",
    "            )\n",
    "\n",
    "    def _fuse_bn_tensor(self, branch):\n",
    "        if branch is None:\n",
    "            return 0, 0\n",
    "        if isinstance(branch, tf.keras.Sequential):\n",
    "            kernel = branch.get_layer(\"conv\").weights[0]\n",
    "            running_mean = branch.get_layer(\"bn\").moving_mean\n",
    "            running_var = branch.get_layer(\"bn\").moving_variance\n",
    "            gamma = branch.get_layer(\"bn\").gamma\n",
    "            beta = branch.get_layer(\"bn\").beta\n",
    "            eps = branch.get_layer(\"bn\").epsilon\n",
    "        else:\n",
    "            assert isinstance(branch, tf.keras.layers.BatchNormalization)\n",
    "            if not hasattr(self, \"id_tensor\"):\n",
    "                input_dim = self.in_channels // self.groups\n",
    "                kernel_value = np.zeros(\n",
    "                    (3, 3, input_dim, self.in_channels), dtype=np.float32\n",
    "                )\n",
    "                for i in range(self.in_channels):\n",
    "                    kernel_value[1, 1, i % input_dim, i] = 1\n",
    "                self.id_tensor = tf.convert_to_tensor(\n",
    "                    kernel_value, dtype=np.float32\n",
    "                )\n",
    "            kernel = self.id_tensor\n",
    "            running_mean = branch.moving_mean\n",
    "            running_var = branch.moving_variance\n",
    "            gamma = branch.gamma\n",
    "            beta = branch.beta\n",
    "            eps = branch.epsilon\n",
    "        std = tf.sqrt(running_var + eps)\n",
    "        t = gamma / std\n",
    "        return kernel * t, beta - running_mean * gamma / std\n",
    "\n",
    "    def repvgg_convert(self):\n",
    "        kernel, bias = self.get_equivalent_kernel_bias()\n",
    "        return kernel, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af638ba9",
   "metadata": {},
   "source": [
    "This code defines a RepVGGBlock layer in TensorFlow, which is a convolutional block used in the RepVGG model. The RepVGG model is a neural network architecture that achieves high accuracy while having a simple structure. The RepVGGBlock layer is designed to be more efficient and easier to train compared to other convolutional layers.\n",
    "\n",
    "The RepVGGBlock layer has several parameters such as `in_channels`, `out_channels`, `kernel_size`, `strides`, `padding`, `dilation`, and `groups`. `in_channels` and `out_channels` determine the number of input and output channels, respectively. \n",
    "- `kernel_size` specifies the size of the convolution kernel. \n",
    "- `strides` determines the step size of the convolution operation. \n",
    "- `padding` controls the amount of padding to be added to the input image.\n",
    "- `dilation` specifies the dilation rate of the convolution operation. \n",
    "- `groups` determines the number of groups to be used in the convolution operation.\n",
    "\n",
    "The layer contains several convolutional operations, including a 3x3 convolution, a 1x1 convolution, and a residual identity. These convolutional operations are fused with batch normalization to improve training efficiency. The layer also includes a rectified linear unit (ReLU) activation function.\n",
    "\n",
    "- The `call()` function of the RepVGGBlock layer applies the convolutional operations and the ReLU activation function to the input tensor. \n",
    "- The `get_equivalent_kernel_bias()` function derives the equivalent kernel and bias of the layer in a differentiable way, which can be useful for applying penalties or constraints during training, such as in quantization or pruning. \n",
    "- The `repvgg_convert()` function converts the RepVGGBlock layer to a standard convolutional layer by fusing batch normalization with convolutional operations.\n",
    "\n",
    "```\n",
    "\n",
    "class RepVGG(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_blocks,\n",
    "        num_classes=1000,\n",
    "        width_multiplier=None,\n",
    "        override_groups_map=None,\n",
    "        deploy=False,\n",
    "    ):\n",
    "        super(RepVGG, self).__init__()\n",
    "\n",
    "        assert len(width_multiplier) == 4\n",
    "\n",
    "        self.deploy = deploy\n",
    "        self.override_groups_map = override_groups_map or dict()\n",
    "\n",
    "        assert 0 not in self.override_groups_map\n",
    "\n",
    "        self.in_planes = min(64, int(64 * width_multiplier[0]))\n",
    "\n",
    "        self.stage0 = RepVGGBlock(\n",
    "            in_channels=3,\n",
    "            out_channels=self.in_planes,\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding=1,\n",
    "            deploy=self.deploy,\n",
    "        )\n",
    "        self.cur_layer_idx = 1\n",
    "        self.stage1 = self._make_stage(\n",
    "            int(64 * width_multiplier[0]), num_blocks[0], stride=2\n",
    "        )\n",
    "        self.stage2 = self._make_stage(\n",
    "            int(128 * width_multiplier[1]), num_blocks[1], stride=2\n",
    "        )\n",
    "        self.stage3 = self._make_stage(\n",
    "            int(256 * width_multiplier[2]), num_blocks[2], stride=2\n",
    "        )\n",
    "        self.stage4 = self._make_stage(\n",
    "            int(512 * width_multiplier[3]), num_blocks[3], stride=2\n",
    "        )\n",
    "        self.gap = tfa.layers.AdaptiveAveragePooling2D(output_size=1)\n",
    "        self.linear = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def _make_stage(self, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        blocks = []\n",
    "        for stride in strides:\n",
    "            cur_groups = self.override_groups_map.get(self.cur_layer_idx, 1)\n",
    "            blocks.append(\n",
    "                RepVGGBlock(\n",
    "                    in_channels=self.in_planes,\n",
    "                    out_channels=planes,\n",
    "                    kernel_size=3,\n",
    "                    strides=stride,\n",
    "                    padding=1,\n",
    "                    groups=cur_groups,\n",
    "                    deploy=self.deploy,\n",
    "                )\n",
    "            )\n",
    "            self.in_planes = planes\n",
    "            self.cur_layer_idx += 1\n",
    "        return tf.keras.Sequential(blocks)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.stage0(x)\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.stage4(out)\n",
    "        out = self.gap(out)\n",
    "        out = tf.keras.layers.Flatten()(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "This is a TensorFlow implementation of the RepVGG model, a type of convolutional neural network designed for efficient inference on mobile and embedded devices. \n",
    "- The RepVGG model uses a series of RepVGG blocks to transform the input image into a feature representation that is then fed into a fully connected layer to make the final prediction. \n",
    "- The RepVGG blocks are based on the VGG-style architecture, but instead of using traditional convolutional layers, they use a combination of 1x1 and 3x3 depthwise separable convolutions to reduce the number of parameters while maintaining performance. \n",
    "- The width of the network can be adjusted by specifying a width multiplier for each stage of the network. \n",
    "- The model also includes an adaptive average pooling layer to reduce the spatial dimensions of the feature map before the fully connected layer.\n",
    "\n",
    "### Resnet\n",
    "\n",
    "ResNet (Residual Network) was proposed by Kaiming He and won the 2015 ILSVRC Grand Prix with an error rate of 3.57%. In the previous network, when the model is not deep enough, its network recognition is not strong, but when the network stack (Plain Network) is very deep, the network gradient disappearance and gradient dispersion are obvious, resulting in the model's computational effectiveness but not up but down. Therefore, in view of the degradation problem of this deep network, ResNet is designed as an ultra-deep network without the gradient vanishing problem.ResNet has various types depending on the number of layers, from 18 to 1202 layers. As an example, Res Net50 consists of 49 convolutional layers and 1 fully connected layer, as shown in the figure below. This simple addition does not add additional parameters and computation to the network, but can greatly increase the training speed and improve the training effect, and this simple structure can well solve the degradation problem when the model deepens the number of layers. In this way, the network will always be in the optimal state and the performance of the network will not decrease with increasing depth.\n",
    "\n",
    "The most important part of ResNet should be the residual block, and here is the structure.\n",
    "\n",
    ":::{figure-md} 03_residual_block\n",
    "<img src=\"../../images/deep-learning/imgcls/03_resblock.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of residual block {cite}`resblock_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 04_ResNet_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/04_ResNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of residual network {cite}`resnet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41714609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_bn_relu(x, filters, kernel_size, strides=1):\n",
    "    x = tf.layers.conv2d(x, filters, kernel_size, strides=strides, padding='same', use_bias=False)\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(x, filters, strides=1):\n",
    "    shortcut = x\n",
    "    x = conv_bn_relu(x, filters, kernel_size=3, strides=strides)\n",
    "    x = conv_bn_relu(x, filters, kernel_size=3)\n",
    "    x = x + shortcut\n",
    "    return x\n",
    "\n",
    "def resnet(input_shape, num_classes, num_filters=16, num_blocks=[3,3,3]):\n",
    "    inputs = tf.placeholder(tf.float32, shape=[None, *input_shape])\n",
    "    x = conv_bn_relu(inputs, num_filters, kernel_size=3)\n",
    "\n",
    "    # Stacking residual blocks\n",
    "    for i, num_blocks in enumerate(num_blocks):\n",
    "        for j in range(num_blocks):\n",
    "            strides = 1\n",
    "            if j == 0 and i != 0:\n",
    "                strides = 2\n",
    "            x = residual_block(x, num_filters*(2**i), strides=strides)\n",
    "\n",
    "    # Global average pooling and fully-connected layer for classification\n",
    "    x = tf.reduce_mean(x, axis=[1,2])\n",
    "    logits = tf.layers.dense(x, num_classes)\n",
    "\n",
    "    return inputs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0b288",
   "metadata": {},
   "source": [
    "This implementation uses the conv_bn_relu function to perform a convolution operation followed by batch normalization and ReLU activation. \n",
    "- The residual_block function defines a residual block that consists of two convolutional layers with batch normalization and ReLU activation, followed by an addition of the input to the output of the second convolutional layer. \n",
    "- The resnet function stacks multiple residual blocks and ends with a global average pooling layer and a fully-connected layer for classification.\n",
    "\n",
    "### DenseNet\n",
    "\n",
    "DenseNet proposes a more radical dense connection mechanism than ResNet: i.e., connecting all layers to each other, specifically each layer accepts all the layers before it as its additional input. resNet short-circuits each layer with some previous layer (usually 2-3 layers), and the connection is made by element-level summation. In DenseNet, each layer is connected (concat) with all the preceding layers in the channel dimension and used as input to the next layer, which is a dense connection. Moreover, DenseNet is directly concat feature maps from different layers, which enables feature reuse and improves efficiency, and this feature is the most important difference between DenseNet and ResNet.\n",
    "\n",
    ":::{figure-md} 05_dense_block\n",
    "<img src=\"../../images/deep-learning/imgcls/05_denseblock.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of dense block {cite}`denseblock_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 06_DenseNet_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/06_DenseNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Structure of dense network {cite}`densenet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3553d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def conv_block(input, filters, dropout_rate):\n",
    "    x = tf.layers.batch_normalization(input)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x, filters, kernel_size=3, padding='same', activation=None)\n",
    "    x = tf.layers.dropout(x, rate=dropout_rate)\n",
    "    return x\n",
    "\n",
    "def dense_block(input, n_layers, growth_rate, dropout_rate):\n",
    "    for i in range(n_layers):\n",
    "        conv = conv_block(input, growth_rate, dropout_rate)\n",
    "        input = tf.concat([input, conv], axis=-1)\n",
    "    return input\n",
    "\n",
    "def transition_block(input, compression):\n",
    "    n_filters = input.get_shape().as_list()[-1]\n",
    "    n_filters = int(n_filters * compression)\n",
    "    x = tf.layers.batch_normalization(input)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.conv2d(x, n_filters, kernel_size=1, padding='same', activation=None)\n",
    "    x = tf.layers.average_pooling2d(x, pool_size=2, strides=2, padding='valid')\n",
    "    return x\n",
    "\n",
    "def densenet(input, n_classes, n_dense_blocks=3, n_layers_per_block=4, growth_rate=12, compression=0.5, dropout_rate=0.2):\n",
    "    # Initial convolution layer\n",
    "    x = tf.layers.conv2d(input, filters=2*growth_rate, kernel_size=7, strides=2, padding='same', activation=None)\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=3, strides=2, padding='same')\n",
    "\n",
    "    # Dense blocks\n",
    "    for i in range(n_dense_blocks):\n",
    "        x = dense_block(x, n_layers_per_block, growth_rate, dropout_rate)\n",
    "        if i != n_dense_blocks-1:\n",
    "            x = transition_block(x, compression)\n",
    "\n",
    "    # Global average pooling and classification layer\n",
    "    x = tf.layers.batch_normalization(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    x = tf.layers.average_pooling2d(x, pool_size=x.get_shape().as_list()[1:3], strides=1)\n",
    "    x = tf.layers.flatten(x)\n",
    "    output = tf.layers.dense(x, units=n_classes, activation=None)\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6509560",
   "metadata": {},
   "source": [
    "This implementation includes functions for building the building blocks of DenseNet: conv_block, dense_block, and transition_block. These building blocks are then used to construct the densenet function, which takes an input tensor and returns the output logits for a classification task.\n",
    "\n",
    "The densenet function takes several hyperparameters as inputs, such as the number of dense blocks, the number of layers per block, the growth rate of each block, the compression factor for the transition blocks, and the dropout rate. These hyperparameters can be adjusted to optimize the performance of the model for a specific task.\n",
    "\n",
    "### MobileNet\n",
    "\n",
    "MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem.\n",
    "\n",
    "Besides, the standard convolutional filters for normal CNNs are replaced by two layers: depthwise convolution and pointwise convolution to build a depthwise separable filter.\n",
    "\n",
    ":::{figure-md} 07_MobileNet_convolution_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/07_mobileconv.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Replacement of standard convolution filter {cite}`mobileconv_structure`\n",
    ":::\n",
    "\n",
    ":::{figure-md} 07_MobileNet_body_structure\n",
    "<img src=\"../../images/deep-learning/imgcls/08_MobileNet.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "Body structure of MobileNet {cite}`mobilenet_structure`\n",
    ":::\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d5a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def depthwise_separable_conv(inputs, num_filters, width_multiplier, downsample=False):\n",
    "    \"\"\"Depthwise Separable Convolution\"\"\"\n",
    "    # Depthwise Convolution\n",
    "    net = tf.keras.layers.DepthwiseConv2D(kernel_size=(3, 3), strides=(2 if downsample else 1, 2 if downsample else 1),\n",
    "                                          depth_multiplier=width_multiplier, padding='same')(inputs)\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.ReLU()(net)\n",
    "\n",
    "    # Pointwise Convolution\n",
    "    net = tf.keras.layers.Conv2D(num_filters, kernel_size=(1, 1), strides=(1, 1), padding='same')(net)\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.ReLU()(net)\n",
    "\n",
    "    return net\n",
    "\n",
    "def mobilenet(input_shape, num_classes, width_multiplier=1):\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial Convolution\n",
    "    net = tf.keras.layers.Conv2D(int(32 * width_multiplier), kernel_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.ReLU()(net)\n",
    "\n",
    "    # Depthwise Separable Convolution x 13\n",
    "    net = depthwise_separable_conv(net, int(64 * width_multiplier), width_multiplier)\n",
    "    net = depthwise_separable_conv(net, int(128 * width_multiplier), width_multiplier, downsample=True)\n",
    "    net = depthwise_separable_conv(net, int(128 * width_multiplier), width_multiplier)\n",
    "    net = depthwise_separable_conv(net, int(256 * width_multiplier), width_multiplier, downsample=True)\n",
    "    net = depthwise_separable_conv(net, int(256 * width_multiplier), width_multiplier)\n",
    "    net = depthwise_separable_conv(net, int(512 * width_multiplier), width_multiplier, downsample=True)\n",
    "    for i in range(5):\n",
    "        net = depthwise_separable_conv(net, int(512 * width_multiplier), width_multiplier)\n",
    "    net = depthwise_separable_conv(net, int(1024 * width_multiplier), width_multiplier, downsample=True)\n",
    "    net = depthwise_separable_conv(net, int(1024 * width_multiplier), width_multiplier)\n",
    "    net = tf.keras.layers.GlobalAveragePooling2D()(net)\n",
    "\n",
    "    # Output\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(net)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=inputs, outputs=outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebcf45",
   "metadata": {},
   "source": [
    "This implementation defines two functions: depthwise_separable_conv and mobilenet. The former implements the depthwise separable convolution operation used by the MobileNet architecture, while the latter constructs the entire MobileNet model.\n",
    "\n",
    "The mobilenet function takes three arguments: input_shape (a tuple specifying the input shape of the model), num_classes (the number of output classes), and width_multiplier (a scaling factor for the number of filters in each layer of the model, defaulting to 1).\n",
    "\n",
    "### ViT\n",
    "\n",
    "Different from the previous models, ViT (Vision Transformer) uses the concept of Transformer. Inspired by the Transformer scaling successes in NLP, they experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, they split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. They train the model on image classification in supervised fashion.\n",
    "\n",
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9fb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class VisionTransformer(tf.keras.Model):\n",
    "    def __init__(self, image_size, patch_size, num_layers, num_heads, ff_dim, num_classes, rate=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_dim = 3 * patch_size ** 2\n",
    "        self.reshape = Resizing(image_size, interpolation='bilinear')\n",
    "        self.patchify = tf.keras.layers.experimental.preprocessing.Patching(num_patches)\n",
    "        self.patch_projection = Dense(units=self.patch_dim, activation='linear')\n",
    "        self.position_embedding = self.add_weight('position_embedding', shape=(1, num_patches + 1, self.patch_dim),\n",
    "                                                   initializer=tf.keras.initializers.RandomNormal(),\n",
    "                                                   trainable=True)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim=self.patch_dim, num_heads=num_heads, ff_dim=ff_dim, rate=rate)\n",
    "                                   for _ in range(num_layers)]\n",
    "        self.layernorm = LayerNormalization(epsilon=1e-6)\n",
    "        self.classifier = Dense(units=num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.reshape(inputs)\n",
    "        x = self.patchify(x)\n",
    "        x = self.patch_projection(x)\n",
    "        x = tf.concat([tf.zeros((tf.shape(x)[0], 1, self.patch_dim)), x], axis=1)\n",
    "        x += self.position_embedding\n",
    "        x = self.dropout(x, training=training)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training)\n",
    "        x = self.layernorm(x)\n",
    "        x = x[:, 0]\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8618450",
   "metadata": {},
   "source": [
    "The above code defines a Vision Transformer (ViT) model in TensorFlow, which is a state-of-the-art architecture for image classification tasks that combines the transformer architecture with a patch-based approach for image processing.\n",
    "\n",
    "The TransformerBlock class defines a single transformer block with multi-head attention and a feedforward neural network. The constructor takes in the following arguments:\n",
    "\n",
    "- embed_dim: the dimensionality of the embedding layer\n",
    "- num_heads: the number of attention heads\n",
    "- ff_dim: the dimensionality of the feedforward layer\n",
    "- rate: the dropout rate\n",
    "\n",
    "The call method of the TransformerBlock class takes in the input tensor and a boolean flag training indicating whether the layer should behave in training or inference mode. The input tensor is passed through the multi-head attention layer, followed by a dropout layer, and then added to the input tensor using residual connections. The resulting tensor is passed through a feedforward neural network, followed by another dropout layer, and then added to the residual tensor using another residual connection.\n",
    "\n",
    "The VisionTransformer class defines the ViT model, which consists of multiple transformer blocks and a final dense layer for classification. The constructor takes in the following arguments:\n",
    "\n",
    "- image_size: the size of the input image\n",
    "- patch_size: the size of the patches to be extracted from the image\n",
    "- num_layers: the number of transformer blocks in the model\n",
    "- num_heads: the number of attention heads in each transformer block\n",
    "- ff_dim: the dimensionality of the feedforward layer in each transformer block\n",
    "- num_classes: the number of output classes\n",
    "- rate: the dropout rate\n",
    "\n",
    "The call method of the VisionTransformer class takes in the input tensor and a boolean flag training indicating whether the layer should behave in training or inference mode. The input tensor is first resized to the specified image_size and then passed through a patch extraction layer that divides the image into patches of size patch_size. Each patch is projected to a patch_dim-dimensional embedding space using a dense layer. A learnable position embedding is added to the patches and the resulting tensor is passed through a dropout layer. The resulting tensor is then passed through a stack of num_layers transformer blocks, each followed by a dropout layer. The output of the final transformer block is passed through a layer normalization layer and the first element of the resulting tensor is passed through a dense layer with num_classes output units and a softmax activation function.\n",
    "\n",
    "## Classic datasets\n",
    "\n",
    "As we said before, image classification task is mainly trained by datasets, so the importance of dataset is obvious. Here, we will introduce some widely-used datasets.\n",
    "\n",
    "### CIFAR-10/100\n",
    "\n",
    "The [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html) consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "The CIFAR-100 dataset is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a \"fine\" label (the class to which it belongs) and a \"coarse\" label (the superclass to which it belongs).\n",
    "\n",
    "```{note}\n",
    "Download for Linux\n",
    "wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "wget http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "\n",
    "Download for Win/Mac\n",
    "Download from the offical website\n",
    "```\n",
    "\n",
    "### ImageNet-1000\n",
    "\n",
    "[ImageNet](https://image-net.org/) is an image dataset organized according to the WordNet hierarchy. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a \"synonym set\" or \"synset\". There are more than 100,000 synsets in WordNet; the majority of them are nouns (80,000+). In ImageNet, we aim to provide on average 1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated. In its completion, we hope ImageNet will offer tens of millions of cleanly labeled and sorted images for most of the concepts in the WordNet hierarchy.\n",
    "\n",
    "```{note}\n",
    "If you want to download this dataset, please visit [kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description) for more information.\n",
    "```\n",
    "\n",
    "## Standards\n",
    "\n",
    "### Top-1 accuracy\n",
    "\n",
    "If your predicted label takes the largest one inside the final probability vector as the prediction result, and if the one with the highest probability in your prediction result is correctly classified, then the prediction is correct. Otherwise, the prediction is wrong.\n",
    "\n",
    "### Top-5 accuracy\n",
    "\n",
    "Among the 50 classification probabilities of the test image, take the first 5 maximum classification probabilities, whether the correct label (classification) is in it or not, that is, whether it is one of these first 5, if it is, it is a successful classification.\n",
    "\n",
    "## Your turn! üöÄ\n",
    "\n",
    "TBD.\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Stanford](https://www.stanford.edu) for creating the open-source course [CS231n: Deep Learning for Computer Vision](https://cs231n.github.io/classification/), [Duc Thang HOANG](https://github.com/hoangthang1607) for creating the open-source project [RepVGG-Tensorflow-2](https://github.com/hoangthang1607/RepVGG-Tensorflow-2), [Junho Kim](https://github.com/taki0112) for creating the open-source project [ResNet-Tensorflow](https://github.com/taki0112/ResNet-Tensorflow), [Densenet-Tensorflow](https://github.com/taki0112/Densenet-Tensorflow) and [vit-tensorflow](https://github.com/taki0112/vit-tensorflow) and [ohadlights](https://github.com/ohadlights) for creating the open-source project [mobilenetv2](https://github.com/ohadlights/mobilenetv2). They inspire the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   78,
   106,
   120,
   252,
   373,
   406,
   429,
   477,
   502,
   546,
   557,
   616
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}