{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d8b3fe",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "```{epigraph}\n",
    "The nervous system contains many circular paths, whose activity so regenerates the excitation of any participant neuron that reference to time past becomes indefinite, although it still implies that afferent activity has realized one of a certain class of configurations over time. Precise specification of these implications by means of recursive functions, and determination of those that can be embodied in the activity of nervous nets, completes the theory.\n",
    "\n",
    "-- Warren McCulloch and Walter Pitts, 1943\n",
    "```\n",
    "\n",
    "A recurrent neural network (RNN) is a type of artificial neural network that uses sequential data or time series data and it is mainly used for Natural Language Processing. Now let us see what it looks like.\n",
    "\n",
    "Sequential data is not [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables).\n",
    "\n",
    ":::{figure-md} sequential data\n",
    "<img src=\"../../images/deep-learning/RNN/sequential_data.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "sequential data\n",
    ":::\n",
    "\n",
    "And the RNNs use recurrent edge to update.\n",
    "\n",
    ":::{figure-md} rnn1\n",
    "<img src=\"../../images/deep-learning/RNN/rnn1.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "RNN1\n",
    ":::\n",
    "\n",
    "If unroll over a sequence $(x_0,x_1,x_2)$.\n",
    "\n",
    ":::{figure-md} rnn2\n",
    "<img src=\"../../images/deep-learning/RNN/rnn2.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "RNN2\n",
    ":::\n",
    "\n",
    "Then, the input (w0,w1,...,wt) sequence of words ( 1-hot encoded ) and the output (w1,w2,...,wt+1) shifted sequence of words ( 1-hot encoded ) have the following relation.\n",
    "\n",
    ":::{figure-md} rnn3\n",
    "<img src=\"../../images/deep-learning/RNN/rnn3.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "RNN3\n",
    ":::\n",
    "\n",
    "The input projection is $x_t = Emb(\\omega_t) = E\\omega_t$, the recurrent connection is $h_t = g(W^h h_t + x_t + b^h)$, and the output projection should be $y = softmax(W^o h_t + b^o)$.\n",
    "The backpropagation of RNN is in this way:\n",
    "\n",
    ":::{figure-md} rnn4\n",
    "<img src=\"../../images/deep-learning/RNN/rnn4.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "RNN4\n",
    ":::\n",
    "\n",
    "Let's make the backpropagation process more clearly.\n",
    "\n",
    "First, we unfold a single-hidden layer RNN, and we can see the weight matrices $W_h$ in it.\n",
    "\n",
    ":::{figure-md} backpropagation\n",
    "<img src=\"../../images/deep-learning/RNN/bp_rnn1.png\" width=\"90%\" class=\"bg-white mb-1\">\n",
    "\n",
    "backpropagation for RNN\n",
    ":::\n",
    "\n",
    "Through the image, we can get the output:\n",
    "\n",
    "- Net input: $z_h^{<t>} = W_hx x^{<t>} + W_hh h^{<t-1>} + b_h$\n",
    "- Activation: $h^{<t>} = \\sigma (z_h^{<t>})$\n",
    "- Output: $z_y^<t> = W_yh h^{<t>} + b_y$, $y^{<t>} = \\sigma(z_y^{<t>})$\n",
    "\n",
    "After that, the loss is computed as the sum over all time steps: $L = \\sum_{t=1}^T L^{<t>}$\n",
    "\n",
    "```{note}\n",
    "There are some key points:\n",
    "\n",
    "- Similar as training very deep networks with tied parameters.\n",
    "- Example between $x_0$ and $y_2$: Wh is used twice.\n",
    "- Usually truncate the backprop after $T$ timesteps.\n",
    "- Difficulties to train long-term dependencies.\n",
    "```\n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A text classifier implemented in TensorFlow to classify SMS spam messages.\n",
    "\n",
    "# Code first downloads and processes the SMS Spam Collection dataset from the UCI Machine Learning Repository and then builds a basic Recurrent neural network (RNN) for text classification using TensorFlow.\n",
    "\n",
    "# The code first cleans and preprocesses the text, then splits it into training and test sets, followed by tokenizing and padding the training set. Next, the code uses an embedding layer to convert the tokenized text into a vector representation, which is then fed into a recurrent neural network and finally classified using a Softmax loss function.\n",
    "\n",
    "#The output of the # code is the accuracy of the classifier along with some statistics\n",
    "\n",
    "# We implement an RNN in TensorFlow to predict spam/ham from texts\n",
    "\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.python.framework import ops\n",
    "tf.disable_v2_behavior()\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Start a graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set RNN parameters\n",
    "epochs = 20\n",
    "batch_size = 250\n",
    "max_sequence_length =25 \n",
    "rnn_size = 10\n",
    "embedding_size = 50\n",
    "min_word_frequency = 10\n",
    "learning_rate = 0.0005\n",
    "dropout_keep_prob = tf.placeholder(tf.float32,name='dropout_keep_prob')\n",
    "\n",
    "\n",
    "# Download or open data\n",
    "data_dir = 'tmp'\n",
    "data_file = 'text_data.txt'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # Format Data\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii', errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "\n",
    "    # Save data to text file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as file_conn:\n",
    "        for text in text_data:\n",
    "            file_conn.write(\"{}\\n\".format(text))\n",
    "else:\n",
    "    # Open data from text file\n",
    "    text_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        for row in file_conn:\n",
    "            text_data.append(row)\n",
    "    text_data = text_data[:-1]\n",
    "\n",
    "text_data = [x.split('\\t') for x in text_data if len(x) >= 1]\n",
    "[text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]\n",
    "\n",
    "\n",
    "# Create a text cleaning function\n",
    "def clean_text(text_string):\n",
    "    text_string = re.sub(r'([^\\s\\w]|_|[0-9])+', '', text_string)\n",
    "    text_string = \" \".join(text_string.split())\n",
    "    text_string = text_string.lower()\n",
    "    return text_string\n",
    "\n",
    "# Clean texts\n",
    "text_data_train = [clean_text(x) for x in text_data_train]\n",
    "#print(text_data[:5])\n",
    "print(text_data_train[:5])\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "vocab_processor = tf.keras.preprocessing.text.Tokenizer()\n",
    "vocab_processor.fit_on_texts(text_data_train)\n",
    "text_processed = vocab_processor.texts_to_sequences(text_data_train)\n",
    "max_document_length = max([len(x) for x in text_processed])\n",
    "#pads the text data to ensure all sequences have the same length (max_sequence_length).\n",
    "text_processed = tf.keras.preprocessing.sequence.pad_sequences(text_processed, maxlen=max_sequence_length, padding='post')\n",
    "print(text_processed.shape)\n",
    "# Shuffle and split data\n",
    "text_processed = np.array(text_processed)\n",
    "text_data_target = np.array([1 if x == 'ham' else 0 for x in text_data_target])\n",
    "shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))\n",
    "x_shuffled = text_processed[shuffled_ix]\n",
    "y_shuffled = text_data_target[shuffled_ix]\n",
    "\n",
    "# Split train/test set\n",
    "ix_cutoff = int(len(y_shuffled)*0.80)\n",
    "x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]\n",
    "print(x_train)\n",
    "y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]\n",
    "vocab_size = len(vocab_processor.word_counts)\n",
    "print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "print(\"80-20 Train Test split: {:d} -- {:d}\".format(len(y_train), len(y_test)))\n",
    "\n",
    "# Create placeholders\n",
    "x_data = tf.placeholder(tf.int32, [None, max_sequence_length])\n",
    "y_output = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Create embedding\n",
    "embedding_mat = tf.Variable(tf.random_uniform([vocab_size+1, embedding_size], -1.0, 1.0))\n",
    "embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)\n",
    "\n",
    "# Define the RNN cell\n",
    "# tensorflow change >= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)\n",
    "\n",
    "output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)\n",
    "output = tf.nn.dropout(output, dropout_keep_prob)\n",
    "\n",
    "# Get output of RNN sequence\n",
    "output = tf.transpose(output, [1, 0, 2])\n",
    "last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "logits_out = tf.matmul(last, weight) + bias\n",
    "\n",
    "\n",
    "# Loss function\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)\n",
    "loss = tf.reduce_mean(losses)\n",
    "print(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))\n",
    "print(accuracy)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "# Start training\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    shuffled_ix = np.random.permutation(np.arange(len(x_train)))\n",
    "    x_train = x_train[shuffled_ix]\n",
    "    y_train = y_train[shuffled_ix]\n",
    "    num_batches = int(len(x_train)/batch_size) + 1\n",
    "    # TO DO CALCULATE GENERATIONS ExACTLY\n",
    "    for i in range(num_batches):\n",
    "        # Select train data\n",
    "        min_ix = i * batch_size\n",
    "        max_ix = np.min([len(x_train), ((i+1) * batch_size)])\n",
    "        x_train_batch = x_train[min_ix:max_ix]\n",
    "        y_train_batch = y_train[min_ix:max_ix]\n",
    "        max_len = max([len(x) for x in x_train_batch])\n",
    "        x_train_batch = np.array([np.pad(x, (0, max_len - len(x)), 'constant') for x in x_train_batch])\n",
    "        # Run train step\n",
    "        train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}\n",
    "        sess.run(train_step, feed_dict=train_dict)\n",
    "    # Run loss and accuracy for training\n",
    "    train_dict = {x_data: x_train, y_output: y_train, dropout_keep_prob:1.0}\n",
    "    temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)\n",
    "    train_loss.append(temp_train_loss)\n",
    "    train_accuracy.append(temp_train_acc)\n",
    "    # Run Eval Step\n",
    "    test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}\n",
    "    temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)\n",
    "    test_loss.append(temp_test_loss)\n",
    "    test_accuracy.append(temp_test_acc)\n",
    "    print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))\n",
    "    \n",
    "# Plot loss over time\n",
    "epoch_seq = np.arange(1, epochs+1)\n",
    "plt.plot(epoch_seq, train_loss, 'k--', label='Train Set')\n",
    "plt.plot(epoch_seq, test_loss, 'r-', label='Test Set')\n",
    "plt.title('Softmax Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Softmax Loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy over time\n",
    "plt.plot(epoch_seq, train_accuracy, 'k--', label='Train Set')\n",
    "plt.plot(epoch_seq, test_accuracy, 'r-', label='Test Set')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4b116",
   "metadata": {},
   "source": [
    "## Your turn! ðŸš€\n",
    "\n",
    "Practice the Recurrent Neural Networks by following this TBD.\n",
    "\n",
    "## Self study\n",
    "\n",
    "TBD\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "Thanks to [Nick](https://github.com/nfmcclure) for creating the open-source course [tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook) and [Sebastian Raschka](https://github.com/rasbt) for creating the open-sourse [stat453-deep-learning-ss20](https://github.com/rasbt/stat453-deep-learning-ss20). It inspires the majority of the content in this chapter.\n",
    "\n",
    "---\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   96,
   289
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}